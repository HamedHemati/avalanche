<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>avalanche.evaluation.metrics &mdash; Avalanche 0.1 documentation</title>
      <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/css/mystyle.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/graphviz.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex/" />
    <link rel="search" title="Search" href="../../../../search/" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../">
            <img src="../../../../_static/avalanche_logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search/" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Avalanche API:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../../">API Reference</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../../"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche</span></code></a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="../../#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="../../benchmarks/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.benchmarks</span></code></a></li>
<li class="toctree-l4 current"><a class="reference internal" href="../"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../logging/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.logging</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../models/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.models</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../training/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.training</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../#submodules">Submodules</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../core/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.core</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../#package-contents">Package Contents</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../#avalanche.__version__">__version__</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../">Avalanche</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../" class="icon icon-home"></a> &raquo;</li>
      <li><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics</span></code></li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../_sources/autoapi/avalanche/evaluation/metrics/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="module-avalanche.evaluation.metrics">
<span id="avalanche-evaluation-metrics"></span><h1><a class="reference internal" href="#module-avalanche.evaluation.metrics" title="avalanche.evaluation.metrics"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics</span></code></a><a class="headerlink" href="#module-avalanche.evaluation.metrics" title="Permalink to this headline"></a></h1>
<p>The <code class="xref py py-mod docutils literal notranslate"><span class="pre">metrics</span></code> module provides a set of already
implemented metrics, ready to be used both standalone
and together with the <cite>EvaluationPlugin</cite>.
To use a standalone metric, please use the class which
inherits from <cite>Metric</cite> and manually call the appropriate
<cite>update</cite>, <cite>reset</cite> and ‘result` method.
To automatically monitor metrics during training and evaluation
flows, specific classes which inherit from <cite>PluginMetric</cite> or
<cite>GenericPluginMetric</cite> are provided. Most of these metrics should
be instantiated by the user by leveraging
the related helper function. Such functions create an instance of
a specific metric (e.g. accuracy) and monitors it on multiple callbacks
(after each epoch, minibatch experience or stream).
For example, to print accuracy metrics at the
end of each training epoch and at the end of each evaluation experience,
it is only required to call <cite>accuracy_metrics(epoch=True, experience=True)</cite>
when creating the <cite>EvaluationPlugin</cite>.</p>
<p>When available, please always use helper functions to specify
the metrics to be monitored.</p>
<p>The following table describes all the metrics available in Avalanche.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 33%" />
<col style="width: 67%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Metric Name</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Top1_Acc</p></td>
<td><p>The accuracy metric for single-label classification</p></td>
</tr>
<tr class="row-odd"><td><p>Loss</p></td>
<td><p>The specific loss is provided by
the user when creating the strategy.</p></td>
</tr>
<tr class="row-even"><td><p>Forgetting</p></td>
<td><p>The difference between
the training performance and the evaluation
performance after training on future experiences.</p></td>
</tr>
<tr class="row-odd"><td><p>Backward Transfer</p></td>
<td><p>The negative forgetting. That is, the difference
between the last evaluation performance and the
first training performance.</p></td>
</tr>
<tr class="row-even"><td><p>Confusion Matrix</p></td>
<td><p>A representation of
false/true positive/negatives for classification</p></td>
</tr>
<tr class="row-odd"><td><p>Multiply and Accumulate</p></td>
<td><p>a.k.a. MAC. Estimates the computational cost
of the model forward pass on a single pattern.
Cost is estimated in terms of multiplications
operations.</p></td>
</tr>
<tr class="row-even"><td><p>Timing</p></td>
<td><p>Time elapsed between different moments of the
execution</p></td>
</tr>
<tr class="row-odd"><td><p>CPU Usage</p></td>
<td><p>The average CPU consumption between different
moments of the execution.</p></td>
</tr>
<tr class="row-even"><td><p>RAM Usage</p></td>
<td><p>The maximum RAM occupancy, as retrieved by
sampling its value at fixed intervals during
execution.</p></td>
</tr>
<tr class="row-odd"><td><p>GPU Usage</p></td>
<td><p>The maximum GPU occuapncy, as retrieved by
sampling its value at fixed intervals during
execution</p></td>
</tr>
<tr class="row-even"><td><p>Disk Usage</p></td>
<td><p>The size in KB of the disk occupancy for a set
of file system paths.</p></td>
</tr>
</tbody>
</table>
<p>The following table provides a brief description of when each metric can be
computed.
<cite>Stream</cite> specifies on which stream (training or evaluation) that metric
is computed.
Please, refer to the helper function of each metric to check which levels
are available for that metric.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 22%" />
<col style="width: 67%" />
<col style="width: 11%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Level</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Stream</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>MB (minibatch)</p></td>
<td><p>Metric emitted at the end of each training
iteration. Its value is averaged across all
patterns in that minibatch. Metric is reset at the
beginning of each training iteration.</p></td>
<td><p>Train</p></td>
</tr>
<tr class="row-odd"><td><p>Epoch</p></td>
<td><p>Metric emitted at the end of each training epoch.
Its value is averaged across all patterns in
that epoch. Metric is reset at the beginning of
each training epoch.</p></td>
<td><p>Train</p></td>
</tr>
<tr class="row-even"><td><p>RunningEpoch</p></td>
<td><p>Metric emitted at the end of each training
iteration. Its value is the average across all
patterns seen since the beginning of the epoch.
Metric is reset at the beginning of each
training epoch.</p></td>
<td><p>Train</p></td>
</tr>
<tr class="row-odd"><td><p>Experience</p></td>
<td><p>Metric emitted at the end of each evaluation
experience. Its value is averaged across all
patterns in that experience.
Metric is reset at the beginning of each
evaluation experience.</p></td>
<td><p>Eval</p></td>
</tr>
<tr class="row-even"><td><p>Stream</p></td>
<td><p>Metric emitted at the end of each evaluation
stream. Its value is averaged across all patterns
in that stream. Metric is reset at the beginning
of each evaluation stream.</p></td>
<td><p>Eval</p></td>
</tr>
</tbody>
</table>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline"></a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="accuracy/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.accuracy</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpoint/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.checkpoint</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="confusion_matrix/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.confusion_matrix</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="cpu_usage/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.cpu_usage</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="disk_usage/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.disk_usage</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="forgetting_bwt/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.forgetting_bwt</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="forward_transfer/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.forward_transfer</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu_usage/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.gpu_usage</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="images_samples/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.images_samples</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="labels_repartition/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.labels_repartition</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="loss/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.loss</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="mac/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.mac</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="mean/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.mean</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="mean_scores/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.mean_scores</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="ram_usage/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.ram_usage</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="timing/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.timing</span></code></a></li>
</ul>
</div>
</section>
<section id="package-contents">
<h2>Package Contents<a class="headerlink" href="#package-contents" title="Permalink to this headline"></a></h2>
<section id="classes">
<h3>Classes<a class="headerlink" href="#classes" title="Permalink to this headline"></a></h3>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.Mean" title="avalanche.evaluation.metrics.Mean"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Mean</span></code></a></p></td>
<td><p>The standalone mean metric.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.Sum" title="avalanche.evaluation.metrics.Sum"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Sum</span></code></a></p></td>
<td><p>The standalone sum metric.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.Accuracy" title="avalanche.evaluation.metrics.Accuracy"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Accuracy</span></code></a></p></td>
<td><p>The Accuracy metric. This is a standalone metric.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.MinibatchAccuracy" title="avalanche.evaluation.metrics.MinibatchAccuracy"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MinibatchAccuracy</span></code></a></p></td>
<td><p>The minibatch plugin accuracy metric.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.EpochAccuracy" title="avalanche.evaluation.metrics.EpochAccuracy"><code class="xref py py-obj docutils literal notranslate"><span class="pre">EpochAccuracy</span></code></a></p></td>
<td><p>The average accuracy over a single training epoch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.RunningEpochAccuracy" title="avalanche.evaluation.metrics.RunningEpochAccuracy"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RunningEpochAccuracy</span></code></a></p></td>
<td><p>The average accuracy across all minibatches up to the current</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.ExperienceAccuracy" title="avalanche.evaluation.metrics.ExperienceAccuracy"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ExperienceAccuracy</span></code></a></p></td>
<td><p>At the end of each experience, this plugin metric reports</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.StreamAccuracy" title="avalanche.evaluation.metrics.StreamAccuracy"><code class="xref py py-obj docutils literal notranslate"><span class="pre">StreamAccuracy</span></code></a></p></td>
<td><p>At the end of the entire stream of experiences, this plugin metric</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.TrainedExperienceAccuracy" title="avalanche.evaluation.metrics.TrainedExperienceAccuracy"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TrainedExperienceAccuracy</span></code></a></p></td>
<td><p>At the end of each experience, this plugin metric reports the average</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.WeightCheckpoint" title="avalanche.evaluation.metrics.WeightCheckpoint"><code class="xref py py-obj docutils literal notranslate"><span class="pre">WeightCheckpoint</span></code></a></p></td>
<td><p>The WeightCheckpoint Metric. This is a standalone metric.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.ConfusionMatrix" title="avalanche.evaluation.metrics.ConfusionMatrix"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConfusionMatrix</span></code></a></p></td>
<td><p>The standalone confusion matrix metric.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.StreamConfusionMatrix" title="avalanche.evaluation.metrics.StreamConfusionMatrix"><code class="xref py py-obj docutils literal notranslate"><span class="pre">StreamConfusionMatrix</span></code></a></p></td>
<td><p>The Stream Confusion Matrix metric.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.WandBStreamConfusionMatrix" title="avalanche.evaluation.metrics.WandBStreamConfusionMatrix"><code class="xref py py-obj docutils literal notranslate"><span class="pre">WandBStreamConfusionMatrix</span></code></a></p></td>
<td><p>Confusion Matrix metric compatible with Weights and Biases logger.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.CPUUsage" title="avalanche.evaluation.metrics.CPUUsage"><code class="xref py py-obj docutils literal notranslate"><span class="pre">CPUUsage</span></code></a></p></td>
<td><p>The standalone CPU usage metric.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.MinibatchCPUUsage" title="avalanche.evaluation.metrics.MinibatchCPUUsage"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MinibatchCPUUsage</span></code></a></p></td>
<td><p>The minibatch CPU usage metric.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.EpochCPUUsage" title="avalanche.evaluation.metrics.EpochCPUUsage"><code class="xref py py-obj docutils literal notranslate"><span class="pre">EpochCPUUsage</span></code></a></p></td>
<td><p>The Epoch CPU usage metric.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.RunningEpochCPUUsage" title="avalanche.evaluation.metrics.RunningEpochCPUUsage"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RunningEpochCPUUsage</span></code></a></p></td>
<td><p>The running epoch CPU usage metric.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.ExperienceCPUUsage" title="avalanche.evaluation.metrics.ExperienceCPUUsage"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ExperienceCPUUsage</span></code></a></p></td>
<td><p>The average experience CPU usage metric.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.StreamCPUUsage" title="avalanche.evaluation.metrics.StreamCPUUsage"><code class="xref py py-obj docutils literal notranslate"><span class="pre">StreamCPUUsage</span></code></a></p></td>
<td><p>The average stream CPU usage metric.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.DiskUsage" title="avalanche.evaluation.metrics.DiskUsage"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DiskUsage</span></code></a></p></td>
<td><p>The standalone disk usage metric.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.MinibatchDiskUsage" title="avalanche.evaluation.metrics.MinibatchDiskUsage"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MinibatchDiskUsage</span></code></a></p></td>
<td><p>The minibatch Disk usage metric.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.EpochDiskUsage" title="avalanche.evaluation.metrics.EpochDiskUsage"><code class="xref py py-obj docutils literal notranslate"><span class="pre">EpochDiskUsage</span></code></a></p></td>
<td><p>The Epoch Disk usage metric.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.ExperienceDiskUsage" title="avalanche.evaluation.metrics.ExperienceDiskUsage"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ExperienceDiskUsage</span></code></a></p></td>
<td><p>The average experience Disk usage metric.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.StreamDiskUsage" title="avalanche.evaluation.metrics.StreamDiskUsage"><code class="xref py py-obj docutils literal notranslate"><span class="pre">StreamDiskUsage</span></code></a></p></td>
<td><p>The average stream Disk usage metric.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.Forgetting" title="avalanche.evaluation.metrics.Forgetting"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Forgetting</span></code></a></p></td>
<td><p>The standalone Forgetting metric.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.GenericExperienceForgetting" title="avalanche.evaluation.metrics.GenericExperienceForgetting"><code class="xref py py-obj docutils literal notranslate"><span class="pre">GenericExperienceForgetting</span></code></a></p></td>
<td><p>The GenericExperienceForgetting metric, describing the change in</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.GenericStreamForgetting" title="avalanche.evaluation.metrics.GenericStreamForgetting"><code class="xref py py-obj docutils literal notranslate"><span class="pre">GenericStreamForgetting</span></code></a></p></td>
<td><p>The GenericStreamForgetting metric, describing the average evaluation</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.ExperienceForgetting" title="avalanche.evaluation.metrics.ExperienceForgetting"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ExperienceForgetting</span></code></a></p></td>
<td><p>The ExperienceForgetting metric, describing the accuracy loss</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.StreamForgetting" title="avalanche.evaluation.metrics.StreamForgetting"><code class="xref py py-obj docutils literal notranslate"><span class="pre">StreamForgetting</span></code></a></p></td>
<td><p>The StreamForgetting metric, describing the average evaluation accuracy loss</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.BWT" title="avalanche.evaluation.metrics.BWT"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BWT</span></code></a></p></td>
<td><p>The standalone Backward Transfer metric.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.ExperienceBWT" title="avalanche.evaluation.metrics.ExperienceBWT"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ExperienceBWT</span></code></a></p></td>
<td><p>The Experience Backward Transfer metric.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.StreamBWT" title="avalanche.evaluation.metrics.StreamBWT"><code class="xref py py-obj docutils literal notranslate"><span class="pre">StreamBWT</span></code></a></p></td>
<td><p>The StreamBWT metric, emitting the average BWT across all experiences</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.ForwardTransfer" title="avalanche.evaluation.metrics.ForwardTransfer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ForwardTransfer</span></code></a></p></td>
<td><p>The standalone Forward Transfer metric.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.GenericExperienceForwardTransfer" title="avalanche.evaluation.metrics.GenericExperienceForwardTransfer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">GenericExperienceForwardTransfer</span></code></a></p></td>
<td><p>The GenericExperienceForwardMetric metric, describing the forward transfer</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.ExperienceForwardTransfer" title="avalanche.evaluation.metrics.ExperienceForwardTransfer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ExperienceForwardTransfer</span></code></a></p></td>
<td><p>The Forward Transfer computed on each experience separately.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.GenericStreamForwardTransfer" title="avalanche.evaluation.metrics.GenericStreamForwardTransfer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">GenericStreamForwardTransfer</span></code></a></p></td>
<td><p>The GenericStreamForwardTransfer metric, describing the average evaluation</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.StreamForwardTransfer" title="avalanche.evaluation.metrics.StreamForwardTransfer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">StreamForwardTransfer</span></code></a></p></td>
<td><p>The Forward Transfer averaged over all the evaluation experiences.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.MaxGPU" title="avalanche.evaluation.metrics.MaxGPU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MaxGPU</span></code></a></p></td>
<td><p>The standalone GPU usage metric.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.MinibatchMaxGPU" title="avalanche.evaluation.metrics.MinibatchMaxGPU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MinibatchMaxGPU</span></code></a></p></td>
<td><p>The Minibatch Max GPU metric.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.EpochMaxGPU" title="avalanche.evaluation.metrics.EpochMaxGPU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">EpochMaxGPU</span></code></a></p></td>
<td><p>The Epoch Max GPU metric.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.ExperienceMaxGPU" title="avalanche.evaluation.metrics.ExperienceMaxGPU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ExperienceMaxGPU</span></code></a></p></td>
<td><p>The Experience Max GPU metric.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.StreamMaxGPU" title="avalanche.evaluation.metrics.StreamMaxGPU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">StreamMaxGPU</span></code></a></p></td>
<td><p>The Stream Max GPU metric.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.Loss" title="avalanche.evaluation.metrics.Loss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Loss</span></code></a></p></td>
<td><p>The standalone Loss metric. This is a general metric</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.MinibatchLoss" title="avalanche.evaluation.metrics.MinibatchLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MinibatchLoss</span></code></a></p></td>
<td><p>The minibatch loss metric.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.EpochLoss" title="avalanche.evaluation.metrics.EpochLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">EpochLoss</span></code></a></p></td>
<td><p>The average loss over a single training epoch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.RunningEpochLoss" title="avalanche.evaluation.metrics.RunningEpochLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RunningEpochLoss</span></code></a></p></td>
<td><p>The average loss across all minibatches up to the current</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.ExperienceLoss" title="avalanche.evaluation.metrics.ExperienceLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ExperienceLoss</span></code></a></p></td>
<td><p>At the end of each experience, this metric reports</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.StreamLoss" title="avalanche.evaluation.metrics.StreamLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">StreamLoss</span></code></a></p></td>
<td><p>At the end of the entire stream of experiences, this metric reports the</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.MAC" title="avalanche.evaluation.metrics.MAC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MAC</span></code></a></p></td>
<td><p>Standalone Multiply-and-accumulate metric. Provides a lower bound of the</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.MinibatchMAC" title="avalanche.evaluation.metrics.MinibatchMAC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MinibatchMAC</span></code></a></p></td>
<td><p>The minibatch MAC metric.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.EpochMAC" title="avalanche.evaluation.metrics.EpochMAC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">EpochMAC</span></code></a></p></td>
<td><p>The MAC at the end of each epoch computed on a</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.ExperienceMAC" title="avalanche.evaluation.metrics.ExperienceMAC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ExperienceMAC</span></code></a></p></td>
<td><p>At the end of each experience, this metric reports the</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.MaxRAM" title="avalanche.evaluation.metrics.MaxRAM"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MaxRAM</span></code></a></p></td>
<td><p>The standalone RAM usage metric.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.MinibatchMaxRAM" title="avalanche.evaluation.metrics.MinibatchMaxRAM"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MinibatchMaxRAM</span></code></a></p></td>
<td><p>The Minibatch Max RAM metric.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.EpochMaxRAM" title="avalanche.evaluation.metrics.EpochMaxRAM"><code class="xref py py-obj docutils literal notranslate"><span class="pre">EpochMaxRAM</span></code></a></p></td>
<td><p>The Epoch Max RAM metric.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.ExperienceMaxRAM" title="avalanche.evaluation.metrics.ExperienceMaxRAM"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ExperienceMaxRAM</span></code></a></p></td>
<td><p>The Experience Max RAM metric.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.StreamMaxRAM" title="avalanche.evaluation.metrics.StreamMaxRAM"><code class="xref py py-obj docutils literal notranslate"><span class="pre">StreamMaxRAM</span></code></a></p></td>
<td><p>The Stream Max RAM metric.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.ElapsedTime" title="avalanche.evaluation.metrics.ElapsedTime"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ElapsedTime</span></code></a></p></td>
<td><p>The standalone Elapsed Time metric.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.MinibatchTime" title="avalanche.evaluation.metrics.MinibatchTime"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MinibatchTime</span></code></a></p></td>
<td><p>The minibatch time metric.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.EpochTime" title="avalanche.evaluation.metrics.EpochTime"><code class="xref py py-obj docutils literal notranslate"><span class="pre">EpochTime</span></code></a></p></td>
<td><p>The epoch elapsed time metric.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.RunningEpochTime" title="avalanche.evaluation.metrics.RunningEpochTime"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RunningEpochTime</span></code></a></p></td>
<td><p>The running epoch time metric.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.ExperienceTime" title="avalanche.evaluation.metrics.ExperienceTime"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ExperienceTime</span></code></a></p></td>
<td><p>The experience time metric.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.StreamTime" title="avalanche.evaluation.metrics.StreamTime"><code class="xref py py-obj docutils literal notranslate"><span class="pre">StreamTime</span></code></a></p></td>
<td><p>The stream time metric.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="functions">
<h3>Functions<a class="headerlink" href="#functions" title="Permalink to this headline"></a></h3>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.accuracy_metrics" title="avalanche.evaluation.metrics.accuracy_metrics"><code class="xref py py-obj docutils literal notranslate"><span class="pre">accuracy_metrics</span></code></a>(*, minibatch=False, epoch=False, epoch_running=False, experience=False, stream=False, trained_experience=False) → List[PluginMetric]</p></td>
<td><p>Helper method that can be used to obtain the desired set of</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.confusion_matrix_metrics" title="avalanche.evaluation.metrics.confusion_matrix_metrics"><code class="xref py py-obj docutils literal notranslate"><span class="pre">confusion_matrix_metrics</span></code></a>(num_classes=None, normalize=None, save_image=True, image_creator=default_cm_image_creator, class_names=None, stream=False, wandb=False, absolute_class_order: bool = False) → List[PluginMetric]</p></td>
<td><p>Helper method that can be used to obtain the desired set of</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.cpu_usage_metrics" title="avalanche.evaluation.metrics.cpu_usage_metrics"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cpu_usage_metrics</span></code></a>(*, minibatch=False, epoch=False, epoch_running=False, experience=False, stream=False) → List[PluginMetric]</p></td>
<td><p>Helper method that can be used to obtain the desired set of</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.disk_usage_metrics" title="avalanche.evaluation.metrics.disk_usage_metrics"><code class="xref py py-obj docutils literal notranslate"><span class="pre">disk_usage_metrics</span></code></a>(*, paths_to_monitor=None, minibatch=False, epoch=False, experience=False, stream=False) → List[PluginMetric]</p></td>
<td><p>Helper method that can be used to obtain the desired set of</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.forgetting_metrics" title="avalanche.evaluation.metrics.forgetting_metrics"><code class="xref py py-obj docutils literal notranslate"><span class="pre">forgetting_metrics</span></code></a>(*, experience=False, stream=False) → List[PluginMetric]</p></td>
<td><p>Helper method that can be used to obtain the desired set of</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.bwt_metrics" title="avalanche.evaluation.metrics.bwt_metrics"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bwt_metrics</span></code></a>(*, experience=False, stream=False) → List[PluginMetric]</p></td>
<td><p>Helper method that can be used to obtain the desired set of</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.forward_transfer_metrics" title="avalanche.evaluation.metrics.forward_transfer_metrics"><code class="xref py py-obj docutils literal notranslate"><span class="pre">forward_transfer_metrics</span></code></a>(*, experience=False, stream=False)</p></td>
<td><p>Helper method that can be used to obtain the desired set of</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.gpu_usage_metrics" title="avalanche.evaluation.metrics.gpu_usage_metrics"><code class="xref py py-obj docutils literal notranslate"><span class="pre">gpu_usage_metrics</span></code></a>(gpu_id, every=0.5, minibatch=False, epoch=False, experience=False, stream=False) → List[PluginMetric]</p></td>
<td><p>Helper method that can be used to obtain the desired set of</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.loss_metrics" title="avalanche.evaluation.metrics.loss_metrics"><code class="xref py py-obj docutils literal notranslate"><span class="pre">loss_metrics</span></code></a>(*, minibatch=False, epoch=False, epoch_running=False, experience=False, stream=False) → List[PluginMetric]</p></td>
<td><p>Helper method that can be used to obtain the desired set of</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.MAC_metrics" title="avalanche.evaluation.metrics.MAC_metrics"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MAC_metrics</span></code></a>(*, minibatch=False, epoch=False, experience=False) → List[PluginMetric]</p></td>
<td><p>Helper method that can be used to obtain the desired set of</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.ram_usage_metrics" title="avalanche.evaluation.metrics.ram_usage_metrics"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ram_usage_metrics</span></code></a>(*, every=1, minibatch=False, epoch=False, experience=False, stream=False) → List[PluginMetric]</p></td>
<td><p>Helper method that can be used to obtain the desired set of</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.timing_metrics" title="avalanche.evaluation.metrics.timing_metrics"><code class="xref py py-obj docutils literal notranslate"><span class="pre">timing_metrics</span></code></a>(*, minibatch=False, epoch=False, epoch_running=False, experience=False, stream=False) → List[PluginMetric]</p></td>
<td><p>Helper method that can be used to obtain the desired set of</p></td>
</tr>
</tbody>
</table>
<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.Mean">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">Mean</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/mean/#Mean"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.Mean" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Metric[float]</span></code></p>
<p>The standalone mean metric.</p>
<p>This utility metric is a general purpose metric that can be used to keep
track of the mean of a sequence of values.</p>
<p>Creates an instance of the mean metric.</p>
<p>This metric in its initial state will return a mean value of 0.
The metric can be updated by using the <cite>update</cite> method while the mean
can be retrieved using the <cite>result</cite> method.</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.Mean.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">SupportsFloat</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">SupportsFloat</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/mean/#Mean.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.Mean.update" title="Permalink to this definition"></a></dt>
<dd><p>Update the running mean given the value.</p>
<p>The value can be weighted with a custom value, defined by the <cite>weight</cite>
parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>value</strong> – The value to be used to update the mean.</p></li>
<li><p><strong>weight</strong> – The weight of the value. Defaults to 1.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>None.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.Mean.result">
<span class="sig-name descname"><span class="pre">result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/mean/#Mean.result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.Mean.result" title="Permalink to this definition"></a></dt>
<dd><p>Retrieves the mean.</p>
<p>Calling this method will not change the internal state of the metric.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The mean, as a float.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.Mean.reset">
<span class="sig-name descname"><span class="pre">reset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/mean/#Mean.reset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.Mean.reset" title="Permalink to this definition"></a></dt>
<dd><p>Resets the metric.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.Mean.__add__">
<span class="sig-name descname"><span class="pre">__add__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">other</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference internal" href="#avalanche.evaluation.metrics.Mean" title="avalanche.evaluation.metrics.Mean"><span class="pre">Mean</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">'Mean'</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/mean/#Mean.__add__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.Mean.__add__" title="Permalink to this definition"></a></dt>
<dd><p>Return a metric representing the weighted mean of the 2 means.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> – the other mean</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The weighted mean</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.Sum">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">Sum</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/mean/#Sum"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.Sum" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Metric[float]</span></code></p>
<p>The standalone sum metric.</p>
<p>This utility metric is a general purpose metric that can be used to keep
track of the sum of a sequence of values.</p>
<p>Beware that this metric only supports summing numbers and the result is
always a float value, even when <cite>update</cite> is called by passing <a href="#id1"><span class="problematic" id="id2">`</span></a>int`s only.</p>
<p>Creates an instance of the sum metric.</p>
<p>This metric in its initial state will return a sum value of 0.
The metric can be updated by using the <cite>update</cite> method while the sum
can be retrieved using the <cite>result</cite> method.</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.Sum.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">SupportsFloat</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/mean/#Sum.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.Sum.update" title="Permalink to this definition"></a></dt>
<dd><p>Update the running sum given the value.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>value</strong> – The value to be used to update the sum.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>None.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.Sum.result">
<span class="sig-name descname"><span class="pre">result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/mean/#Sum.result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.Sum.result" title="Permalink to this definition"></a></dt>
<dd><p>Retrieves the sum.</p>
<p>Calling this method will not change the internal state of the metric.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The sum, as a float.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.Sum.reset">
<span class="sig-name descname"><span class="pre">reset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/mean/#Sum.reset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.Sum.reset" title="Permalink to this definition"></a></dt>
<dd><p>Resets the metric.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.Accuracy">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">Accuracy</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/accuracy/#Accuracy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.Accuracy" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Metric[float]</span></code></p>
<p>The Accuracy metric. This is a standalone metric.</p>
<p>The metric keeps a dictionary of &lt;task_label, accuracy value&gt; pairs.
and update the values through a running average over multiple
&lt;prediction, target&gt; pairs of Tensors, provided incrementally.
The “prediction” and “target” tensors may contain plain labels or
one-hot/logit vectors.</p>
<p>Each time <cite>result</cite> is called, this metric emits the average accuracy
across all predictions made since the last <cite>reset</cite>.</p>
<p>The reset method will bring the metric to its initial state. By default
this metric in its initial state will return an accuracy value of 0.</p>
<p>Creates an instance of the standalone Accuracy metric.</p>
<p>By default this metric in its initial state will return an accuracy
value of 0. The metric can be updated by using the <cite>update</cite> method
while the running accuracy can be retrieved using the <cite>result</cite> method.</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.Accuracy.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predicted_y</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">true_y</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">task_labels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/accuracy/#Accuracy.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.Accuracy.update" title="Permalink to this definition"></a></dt>
<dd><p>Update the running accuracy given the true and predicted labels.
Parameter <cite>task_labels</cite> is used to decide how to update the inner
dictionary: if Float, only the dictionary value related to that task
is updated. If Tensor, all the dictionary elements belonging to the
task labels will be updated.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>predicted_y</strong> – The model prediction. Both labels and logit vectors
are supported.</p></li>
<li><p><strong>true_y</strong> – The ground truth. Both labels and one-hot vectors
are supported.</p></li>
<li><p><strong>task_labels</strong> – the int task label associated to the current
experience or the task labels vector showing the task label
for each pattern.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>None.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.Accuracy.result">
<span class="sig-name descname"><span class="pre">result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">task_label</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/accuracy/#Accuracy.result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.Accuracy.result" title="Permalink to this definition"></a></dt>
<dd><p>Retrieves the running accuracy.</p>
<p>Calling this method will not change the internal state of the metric.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>task_label</strong> – if None, return the entire dictionary of accuracies
for each task. Otherwise return the dictionary
<cite>{task_label: accuracy}</cite>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A dict of running accuracies for each task label,
where each value is a float value between 0 and 1.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.Accuracy.reset">
<span class="sig-name descname"><span class="pre">reset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">task_label</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/accuracy/#Accuracy.reset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.Accuracy.reset" title="Permalink to this definition"></a></dt>
<dd><p>Resets the metric.
:param task_label: if None, reset the entire dictionary.</p>
<blockquote>
<div><p>Otherwise, reset the value associated to <cite>task_label</cite>.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MinibatchAccuracy">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">MinibatchAccuracy</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/accuracy/#MinibatchAccuracy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MinibatchAccuracy" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.accuracy.AccuracyPluginMetric</span></code></p>
<p>The minibatch plugin accuracy metric.
This metric only works at training time.</p>
<p>This metric computes the average accuracy over patterns
from a single minibatch.
It reports the result after each iteration.</p>
<p>If a more coarse-grained logging is needed, consider using
<a class="reference internal" href="#avalanche.evaluation.metrics.EpochAccuracy" title="avalanche.evaluation.metrics.EpochAccuracy"><code class="xref py py-class docutils literal notranslate"><span class="pre">EpochAccuracy</span></code></a> instead.</p>
<p>Creates an instance of the MinibatchAccuracy metric.</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MinibatchAccuracy.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/accuracy/#MinibatchAccuracy.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MinibatchAccuracy.__str__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.EpochAccuracy">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">EpochAccuracy</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/accuracy/#EpochAccuracy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.EpochAccuracy" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.accuracy.AccuracyPluginMetric</span></code></p>
<p>The average accuracy over a single training epoch.
This plugin metric only works at training time.</p>
<p>The accuracy will be logged after each training epoch by computing
the number of correctly predicted patterns during the epoch divided by
the overall number of patterns encountered in that epoch.</p>
<p>Creates an instance of the EpochAccuracy metric.</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.EpochAccuracy.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/accuracy/#EpochAccuracy.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.EpochAccuracy.__str__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.RunningEpochAccuracy">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">RunningEpochAccuracy</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/accuracy/#RunningEpochAccuracy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.RunningEpochAccuracy" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.accuracy.AccuracyPluginMetric</span></code></p>
<p>The average accuracy across all minibatches up to the current
epoch iteration.
This plugin metric only works at training time.</p>
<p>At each iteration, this metric logs the accuracy averaged over all patterns
seen so far in the current epoch.
The metric resets its state after each training epoch.</p>
<p>Creates an instance of the RunningEpochAccuracy metric.</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.RunningEpochAccuracy.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/accuracy/#RunningEpochAccuracy.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.RunningEpochAccuracy.__str__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ExperienceAccuracy">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">ExperienceAccuracy</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/accuracy/#ExperienceAccuracy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ExperienceAccuracy" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.accuracy.AccuracyPluginMetric</span></code></p>
<p>At the end of each experience, this plugin metric reports
the average accuracy over all patterns seen in that experience.
This metric only works at eval time.</p>
<p>Creates an instance of ExperienceAccuracy metric</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ExperienceAccuracy.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/accuracy/#ExperienceAccuracy.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ExperienceAccuracy.__str__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.StreamAccuracy">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">StreamAccuracy</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/accuracy/#StreamAccuracy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.StreamAccuracy" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.accuracy.AccuracyPluginMetric</span></code></p>
<p>At the end of the entire stream of experiences, this plugin metric
reports the average accuracy over all patterns seen in all experiences.
This metric only works at eval time.</p>
<p>Creates an instance of StreamAccuracy metric</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.StreamAccuracy.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/accuracy/#StreamAccuracy.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.StreamAccuracy.__str__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.TrainedExperienceAccuracy">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">TrainedExperienceAccuracy</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/accuracy/#TrainedExperienceAccuracy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.TrainedExperienceAccuracy" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.accuracy.AccuracyPluginMetric</span></code></p>
<p>At the end of each experience, this plugin metric reports the average
accuracy for only the experiences that the model has been trained on so far.</p>
<p>This metric only works at eval time.</p>
<p>Creates an instance of TrainedExperienceAccuracy metric by first
constructing AccuracyPluginMetric</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.TrainedExperienceAccuracy.after_training_exp">
<span class="sig-name descname"><span class="pre">after_training_exp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/accuracy/#TrainedExperienceAccuracy.after_training_exp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.TrainedExperienceAccuracy.after_training_exp" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.TrainedExperienceAccuracy.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/accuracy/#TrainedExperienceAccuracy.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.TrainedExperienceAccuracy.update" title="Permalink to this definition"></a></dt>
<dd><p>Only update the accuracy with results from experiences that have been
trained on</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.TrainedExperienceAccuracy.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/accuracy/#TrainedExperienceAccuracy.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.TrainedExperienceAccuracy.__str__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.accuracy_metrics">
<span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">accuracy_metrics</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">minibatch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epoch_running</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">experience</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stream</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trained_experience</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">PluginMetric</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/accuracy/#accuracy_metrics"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.accuracy_metrics" title="Permalink to this definition"></a></dt>
<dd><p>Helper method that can be used to obtain the desired set of
plugin metrics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>minibatch</strong> – If True, will return a metric able to log
the minibatch accuracy at training time.</p></li>
<li><p><strong>epoch</strong> – If True, will return a metric able to log
the epoch accuracy at training time.</p></li>
<li><p><strong>epoch_running</strong> – If True, will return a metric able to log
the running epoch accuracy at training time.</p></li>
<li><p><strong>experience</strong> – If True, will return a metric able to log
the accuracy on each evaluation experience.</p></li>
<li><p><strong>stream</strong> – If True, will return a metric able to log
the accuracy averaged over the entire evaluation stream of experiences.</p></li>
<li><p><strong>trained_experience</strong> – If True, will return a metric able to log
the average evaluation accuracy only for experiences that the
model has been trained on</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of plugin metrics.</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.WeightCheckpoint">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">WeightCheckpoint</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/checkpoint/#WeightCheckpoint"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.WeightCheckpoint" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">PluginMetric[Tensor]</span></code></p>
<p>The WeightCheckpoint Metric. This is a standalone metric.</p>
<p>Instances of this metric keeps the weight checkpoint tensor of the
model at each experience.</p>
<p>Each time <cite>result</cite> is called, this metric emits the latest experience’s
weight checkpoint tensor since the last <cite>reset</cite>.</p>
<p>The reset method will bring the metric to its initial state. By default
this metric in its initial state will return None.</p>
<p>Creates an instance of the WeightCheckpoint Metric.</p>
<p>By default this metric in its initial state will return None.
The metric can be updated by using the <cite>update</cite> method
while the current experience’s weight checkpoint tensor can be
retrieved using the <cite>result</cite> method.</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.WeightCheckpoint.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/checkpoint/#WeightCheckpoint.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.WeightCheckpoint.update" title="Permalink to this definition"></a></dt>
<dd><p>Update the weight checkpoint at the current experience.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>weights</strong> – the weight tensor at current experience</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>None.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.WeightCheckpoint.result">
<span class="sig-name descname"><span class="pre">result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/checkpoint/#WeightCheckpoint.result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.WeightCheckpoint.result" title="Permalink to this definition"></a></dt>
<dd><p>Retrieves the weight checkpoint at the current experience.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The weight checkpoint as a tensor.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.WeightCheckpoint.reset">
<span class="sig-name descname"><span class="pre">reset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/checkpoint/#WeightCheckpoint.reset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.WeightCheckpoint.reset" title="Permalink to this definition"></a></dt>
<dd><p>Resets the metric.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.WeightCheckpoint.after_eval_exp">
<span class="sig-name descname"><span class="pre">after_eval_exp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">BaseStrategy</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">'MetricResult'</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/checkpoint/#WeightCheckpoint.after_eval_exp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.WeightCheckpoint.after_eval_exp" title="Permalink to this definition"></a></dt>
<dd><p>Called after <cite>eval_exp</cite> by the <cite>BaseStrategy</cite>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.WeightCheckpoint.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/checkpoint/#WeightCheckpoint.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.WeightCheckpoint.__str__" title="Permalink to this definition"></a></dt>
<dd><p>Return str(self).</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ConfusionMatrix">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">ConfusionMatrix</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_classes</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="pre">'true'</span><span class="p"><span class="pre">,</span> </span><span class="pre">'pred'</span><span class="p"><span class="pre">,</span> </span><span class="pre">'all'</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/confusion_matrix/#ConfusionMatrix"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ConfusionMatrix" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Metric[Tensor]</span></code></p>
<p>The standalone confusion matrix metric.</p>
<p>Instances of this metric keep track of the confusion matrix by receiving a
pair of “ground truth” and “prediction” Tensors describing the labels of a
minibatch. Those two tensors can both contain plain labels or
one-hot/logit vectors.</p>
<p>The result is the unnormalized running confusion matrix.</p>
<p>Beware that by default the confusion matrix size will depend on the value of
the maximum label as detected by looking at both the ground truth and
predictions Tensors. When passing one-hot/logit vectors, this
metric will try to infer the number of classes from the vector sizes.
Otherwise, the maximum label value encountered in the truth/prediction
Tensors will be used.</p>
<p>If the user sets the <cite>num_classes</cite>, then the confusion matrix will always be
of size <cite>num_classes, num_classes</cite>. Whenever a prediction or label tensor is
provided as logits, only the first <cite>num_classes</cite> units will be considered in
the confusion matrix computation. If they are provided as numerical labels,
each of them has to be smaller than <cite>num_classes</cite>.</p>
<p>The reset method will bring the metric to its initial state. By default
this metric in its initial state will return an empty Tensor.</p>
<p>Creates an instance of the standalone confusion matrix metric.</p>
<p>By default this metric in its initial state will return an empty Tensor.
The metric can be updated by using the <cite>update</cite> method while the running
confusion matrix can be retrieved using the <cite>result</cite> method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_classes</strong> – The number of classes. Defaults to None,
which means that the number of classes will be inferred from
ground truth and prediction Tensors (see class description for more
details). If not None, the confusion matrix will always be of size
<cite>num_classes, num_classes</cite> and only the first <cite>num_classes</cite> values
of output logits or target logits will be considered in the update.
If the output or targets are provided as numerical labels,
there can be no label greater than <cite>num_classes</cite>.</p></li>
<li><p><strong>normalize</strong> – how to normalize confusion matrix.
None to not normalize</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ConfusionMatrix.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">true_y</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predicted_y</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/confusion_matrix/#ConfusionMatrix.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ConfusionMatrix.update" title="Permalink to this definition"></a></dt>
<dd><p>Update the running confusion matrix given the true and predicted labels.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>true_y</strong> – The ground truth. Both labels and one-hot vectors
are supported.</p></li>
<li><p><strong>predicted_y</strong> – The ground truth. Both labels and logit vectors
are supported.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>None.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ConfusionMatrix.result">
<span class="sig-name descname"><span class="pre">result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/confusion_matrix/#ConfusionMatrix.result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ConfusionMatrix.result" title="Permalink to this definition"></a></dt>
<dd><p>Retrieves the unnormalized confusion matrix.</p>
<p>Calling this method will not change the internal state of the metric.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The running confusion matrix, as a Tensor.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ConfusionMatrix.reset">
<span class="sig-name descname"><span class="pre">reset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/confusion_matrix/#ConfusionMatrix.reset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ConfusionMatrix.reset" title="Permalink to this definition"></a></dt>
<dd><p>Resets the metric.</p>
<p>Calling this method will <em>not</em> reset the default number of classes
optionally defined in the constructor optional parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ConfusionMatrix.nan_to_num">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">nan_to_num</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">matrix</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/confusion_matrix/#ConfusionMatrix.nan_to_num"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ConfusionMatrix.nan_to_num" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.StreamConfusionMatrix">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">StreamConfusionMatrix</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_classes</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="pre">'true'</span><span class="p"><span class="pre">,</span> </span><span class="pre">'pred'</span><span class="p"><span class="pre">,</span> </span><span class="pre">'all'</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_image</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">image_creator</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span> </span><span class="pre">Sequence</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span> </span><span class="pre">Image</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">default_cm_image_creator</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">absolute_class_order</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/confusion_matrix/#StreamConfusionMatrix"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.StreamConfusionMatrix" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">PluginMetric[Tensor]</span></code></p>
<p>The Stream Confusion Matrix metric.
This plugin metric only works on the eval phase.</p>
<p>Confusion Matrix computation can be slow if you compute it for a large
number of classes. We recommend to set <cite>save_image=False</cite> if the runtime
is too large.</p>
<p>At the end of the eval phase, this metric logs the confusion matrix
relative to all the patterns seen during eval.</p>
<p>The metric can log either a Tensor or a PIL Image representing the
confusion matrix.</p>
<p>Creates an instance of the Stream Confusion Matrix metric.</p>
<p>We recommend to set <cite>save_image=False</cite> if the runtime is too large.
In fact, a large number of classes may increase the computation time
of this metric.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_classes</strong> – The number of classes. Defaults to None,
which means that the number of classes will be inferred from
ground truth and prediction Tensors (see class description for more
details). If not None, the confusion matrix will always be of size
<cite>num_classes, num_classes</cite> and only the first <cite>num_classes</cite> values
of output logits or target logits will be considered in the update.
If the output or targets are provided as numerical labels,
there can be no label greater than <cite>num_classes</cite>.</p></li>
<li><p><strong>normalize</strong> – Normalizes confusion matrix over the true (rows),
predicted (columns) conditions or all the population. If None,
confusion matrix will not be normalized. Valid values are: ‘true’,
‘pred’ and ‘all’ or None.</p></li>
<li><p><strong>save_image</strong> – If True, a graphical representation of the confusion
matrix will be logged, too. If False, only the Tensor representation
will be logged. Defaults to True.</p></li>
<li><p><strong>image_creator</strong> – A callable that, given the tensor representation
of the confusion matrix and the corresponding labels, returns a
graphical representation of the matrix as a PIL Image. Defaults to
<cite>default_cm_image_creator</cite>.</p></li>
<li><p><strong>absolute_class_order</strong> – If true, the labels in the created image
will be sorted by id, otherwise they will be sorted by order of
encounter at training time. This parameter is ignored if
<cite>save_image</cite> is False, or the scenario is not a NCScenario.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.StreamConfusionMatrix.reset">
<span class="sig-name descname"><span class="pre">reset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/confusion_matrix/#StreamConfusionMatrix.reset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.StreamConfusionMatrix.reset" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.StreamConfusionMatrix.result">
<span class="sig-name descname"><span class="pre">result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/confusion_matrix/#StreamConfusionMatrix.result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.StreamConfusionMatrix.result" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.StreamConfusionMatrix.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">true_y</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predicted_y</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/confusion_matrix/#StreamConfusionMatrix.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.StreamConfusionMatrix.update" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.StreamConfusionMatrix.before_eval">
<span class="sig-name descname"><span class="pre">before_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/confusion_matrix/#StreamConfusionMatrix.before_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.StreamConfusionMatrix.before_eval" title="Permalink to this definition"></a></dt>
<dd><p>Called before <cite>eval</cite> by the <cite>BaseStrategy</cite>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.StreamConfusionMatrix.after_eval_iteration">
<span class="sig-name descname"><span class="pre">after_eval_iteration</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">BaseStrategy</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/confusion_matrix/#StreamConfusionMatrix.after_eval_iteration"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.StreamConfusionMatrix.after_eval_iteration" title="Permalink to this definition"></a></dt>
<dd><p>Called after the end of an iteration by the
<cite>BaseStrategy</cite>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.StreamConfusionMatrix.after_eval">
<span class="sig-name descname"><span class="pre">after_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">BaseStrategy</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">MetricResult</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/confusion_matrix/#StreamConfusionMatrix.after_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.StreamConfusionMatrix.after_eval" title="Permalink to this definition"></a></dt>
<dd><p>Called after <cite>eval</cite> by the <cite>BaseStrategy</cite>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.StreamConfusionMatrix.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/confusion_matrix/#StreamConfusionMatrix.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.StreamConfusionMatrix.__str__" title="Permalink to this definition"></a></dt>
<dd><p>Return str(self).</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.WandBStreamConfusionMatrix">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">WandBStreamConfusionMatrix</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">class_names</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/confusion_matrix/#WandBStreamConfusionMatrix"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.WandBStreamConfusionMatrix" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="../#avalanche.evaluation.PluginMetric" title="avalanche.evaluation.PluginMetric"><code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.PluginMetric</span></code></a></p>
<p>Confusion Matrix metric compatible with Weights and Biases logger.
Differently from the <cite>StreamConfusionMatrix</cite>, this metric will use W&amp;B
built-in functionalities to log the Confusion Matrix.</p>
<p>This metric may not produce meaningful outputs with other loggers.</p>
<p><a class="reference external" href="https://docs.wandb.ai/guides/track/log#custom-charts">https://docs.wandb.ai/guides/track/log#custom-charts</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>class_names</strong> – list of names for the classes.
E.g. [“cat”, “dog”] if class 0 == “cat” and class 1 == “dog”
If None, no class names will be used. Default None.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.WandBStreamConfusionMatrix.reset">
<span class="sig-name descname"><span class="pre">reset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/confusion_matrix/#WandBStreamConfusionMatrix.reset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.WandBStreamConfusionMatrix.reset" title="Permalink to this definition"></a></dt>
<dd><p>Resets the metric internal state.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.WandBStreamConfusionMatrix.before_eval">
<span class="sig-name descname"><span class="pre">before_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/confusion_matrix/#WandBStreamConfusionMatrix.before_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.WandBStreamConfusionMatrix.before_eval" title="Permalink to this definition"></a></dt>
<dd><p>Called before <cite>eval</cite> by the <cite>BaseStrategy</cite>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.WandBStreamConfusionMatrix.result">
<span class="sig-name descname"><span class="pre">result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/confusion_matrix/#WandBStreamConfusionMatrix.result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.WandBStreamConfusionMatrix.result" title="Permalink to this definition"></a></dt>
<dd><p>Obtains the value of the metric.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The value of the metric.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.WandBStreamConfusionMatrix.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/confusion_matrix/#WandBStreamConfusionMatrix.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.WandBStreamConfusionMatrix.update" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.WandBStreamConfusionMatrix.after_eval_iteration">
<span class="sig-name descname"><span class="pre">after_eval_iteration</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">BaseStrategy</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/confusion_matrix/#WandBStreamConfusionMatrix.after_eval_iteration"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.WandBStreamConfusionMatrix.after_eval_iteration" title="Permalink to this definition"></a></dt>
<dd><p>Called after the end of an iteration by the
<cite>BaseStrategy</cite>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.WandBStreamConfusionMatrix.after_eval">
<span class="sig-name descname"><span class="pre">after_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">BaseStrategy</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">MetricResult</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/confusion_matrix/#WandBStreamConfusionMatrix.after_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.WandBStreamConfusionMatrix.after_eval" title="Permalink to this definition"></a></dt>
<dd><p>Called after <cite>eval</cite> by the <cite>BaseStrategy</cite>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.WandBStreamConfusionMatrix.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/confusion_matrix/#WandBStreamConfusionMatrix.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.WandBStreamConfusionMatrix.__str__" title="Permalink to this definition"></a></dt>
<dd><p>Return str(self).</p>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.confusion_matrix_metrics">
<span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">confusion_matrix_metrics</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_image</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">image_creator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">default_cm_image_creator</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_names</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stream</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wandb</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">absolute_class_order</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">PluginMetric</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/confusion_matrix/#confusion_matrix_metrics"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.confusion_matrix_metrics" title="Permalink to this definition"></a></dt>
<dd><p>Helper method that can be used to obtain the desired set of
plugin metrics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_classes</strong> – The number of classes. Defaults to None,
which means that the number of classes will be inferred from
ground truth and prediction Tensors (see class description for more
details). If not None, the confusion matrix will always be of size
<cite>num_classes, num_classes</cite> and only the first <cite>num_classes</cite> values
of output logits or target logits will be considered in the update.
If the output or targets are provided as numerical labels,
there can be no label greater than <cite>num_classes</cite>.</p></li>
<li><p><strong>normalize</strong> – Normalizes confusion matrix over the true (rows),
predicted (columns) conditions or all the population. If None,
confusion matrix will not be normalized. Valid values are: ‘true’,
‘pred’ and ‘all’ or None.</p></li>
<li><p><strong>save_image</strong> – If True, a graphical representation of the confusion
matrix will be logged, too. If False, only the Tensor representation
will be logged. Defaults to True.</p></li>
<li><p><strong>image_creator</strong> – A callable that, given the tensor representation
of the confusion matrix, returns a graphical representation of the
matrix as a PIL Image. Defaults to <cite>default_cm_image_creator</cite>.</p></li>
<li><p><strong>class_names</strong> – W&amp;B only. List of names for the classes.
E.g. [“cat”, “dog”] if class 0 == “cat” and class 1 == “dog”
If None, no class names will be used. Default None.</p></li>
<li><p><strong>stream</strong> – If True, will return a metric able to log
the confusion matrix averaged over the entire evaluation stream
of experiences.</p></li>
<li><p><strong>wandb</strong> – if True, will return a Weights and Biases confusion matrix
together with all the other confusion matrixes requested.</p></li>
<li><p><strong>absolute_class_order</strong> – <p>Not W&amp;B. If true, the labels in the created
image will be sorted by id, otherwise they will be sorted by order of
encounter at training time. This parameter is ignored if <cite>save_image</cite> is</p>
<blockquote>
<div><p>False, or the scenario is not a NCScenario.</p>
</div></blockquote>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of plugin metrics.</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.CPUUsage">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">CPUUsage</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/cpu_usage/#CPUUsage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.CPUUsage" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Metric[float]</span></code></p>
<p>The standalone CPU usage metric.</p>
<p>Instances of this metric compute the average CPU usage as a float value.
The metric starts tracking the CPU usage when the <cite>update</cite> method is called
for the first time. That is, the tracking does not start at the time the
constructor is invoked.</p>
<p>Calling the <cite>update</cite> method more than twice will update the metric to the
average usage between the first and the last call to <cite>update</cite>.</p>
<p>The result, obtained using the <cite>result</cite> method, is the usage computed
as stated above.</p>
<p>The reset method will bring the metric to its initial state. By default
this metric in its initial state will return an usage value of 0.</p>
<p>Creates an instance of the standalone CPU usage metric.</p>
<p>By default this metric in its initial state will return a CPU usage
value of 0. The metric can be updated by using the <cite>update</cite> method
while the average CPU usage can be retrieved using the <cite>result</cite> method.</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.CPUUsage.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/cpu_usage/#CPUUsage.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.CPUUsage.update" title="Permalink to this definition"></a></dt>
<dd><p>Update the running CPU usage.</p>
<p>For more info on how to set the starting moment see the class
description.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.CPUUsage.result">
<span class="sig-name descname"><span class="pre">result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/cpu_usage/#CPUUsage.result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.CPUUsage.result" title="Permalink to this definition"></a></dt>
<dd><p>Retrieves the average CPU usage.</p>
<p>Calling this method will not change the internal state of the metric.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The average CPU usage, as a float value.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.CPUUsage.reset">
<span class="sig-name descname"><span class="pre">reset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/cpu_usage/#CPUUsage.reset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.CPUUsage.reset" title="Permalink to this definition"></a></dt>
<dd><p>Resets the metric.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MinibatchCPUUsage">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">MinibatchCPUUsage</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/cpu_usage/#MinibatchCPUUsage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MinibatchCPUUsage" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.cpu_usage.CPUPluginMetric</span></code></p>
<p>The minibatch CPU usage metric.
This plugin metric only works at training time.</p>
<p>This metric “logs” the CPU usage for each iteration.</p>
<p>If a more coarse-grained logging is needed, consider using
<a class="reference internal" href="#avalanche.evaluation.metrics.EpochCPUUsage" title="avalanche.evaluation.metrics.EpochCPUUsage"><code class="xref py py-class docutils literal notranslate"><span class="pre">EpochCPUUsage</span></code></a>.</p>
<p>Creates an instance of the minibatch CPU usage metric.</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MinibatchCPUUsage.before_training_iteration">
<span class="sig-name descname"><span class="pre">before_training_iteration</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/cpu_usage/#MinibatchCPUUsage.before_training_iteration"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MinibatchCPUUsage.before_training_iteration" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MinibatchCPUUsage.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/cpu_usage/#MinibatchCPUUsage.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MinibatchCPUUsage.__str__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.EpochCPUUsage">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">EpochCPUUsage</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/cpu_usage/#EpochCPUUsage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.EpochCPUUsage" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.cpu_usage.CPUPluginMetric</span></code></p>
<p>The Epoch CPU usage metric.
This plugin metric only works at training time.</p>
<p>The average usage will be logged after each epoch.</p>
<p>Creates an instance of the epoch CPU usage metric.</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.EpochCPUUsage.before_training_epoch">
<span class="sig-name descname"><span class="pre">before_training_epoch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/cpu_usage/#EpochCPUUsage.before_training_epoch"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.EpochCPUUsage.before_training_epoch" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.EpochCPUUsage.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/cpu_usage/#EpochCPUUsage.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.EpochCPUUsage.__str__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.RunningEpochCPUUsage">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">RunningEpochCPUUsage</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/cpu_usage/#RunningEpochCPUUsage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.RunningEpochCPUUsage" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.cpu_usage.CPUPluginMetric</span></code></p>
<p>The running epoch CPU usage metric.
This plugin metric only works at training time</p>
<p>After each iteration, the metric logs the average CPU usage up
to the current epoch iteration.</p>
<p>Creates an instance of the average epoch cpu usage metric.</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.RunningEpochCPUUsage.result">
<span class="sig-name descname"><span class="pre">result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/cpu_usage/#RunningEpochCPUUsage.result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.RunningEpochCPUUsage.result" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.RunningEpochCPUUsage.before_training_epoch">
<span class="sig-name descname"><span class="pre">before_training_epoch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/cpu_usage/#RunningEpochCPUUsage.before_training_epoch"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.RunningEpochCPUUsage.before_training_epoch" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.RunningEpochCPUUsage.before_training_iteration">
<span class="sig-name descname"><span class="pre">before_training_iteration</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/cpu_usage/#RunningEpochCPUUsage.before_training_iteration"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.RunningEpochCPUUsage.before_training_iteration" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.RunningEpochCPUUsage.after_training_iteration">
<span class="sig-name descname"><span class="pre">after_training_iteration</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/cpu_usage/#RunningEpochCPUUsage.after_training_iteration"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.RunningEpochCPUUsage.after_training_iteration" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.RunningEpochCPUUsage.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/cpu_usage/#RunningEpochCPUUsage.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.RunningEpochCPUUsage.__str__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ExperienceCPUUsage">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">ExperienceCPUUsage</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/cpu_usage/#ExperienceCPUUsage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ExperienceCPUUsage" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.cpu_usage.CPUPluginMetric</span></code></p>
<p>The average experience CPU usage metric.
This plugin metric works only at eval time.</p>
<p>After each experience, this metric emits the average CPU usage on that
experience.</p>
<p>Creates an instance of the experience CPU usage metric.</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ExperienceCPUUsage.before_eval_exp">
<span class="sig-name descname"><span class="pre">before_eval_exp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/cpu_usage/#ExperienceCPUUsage.before_eval_exp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ExperienceCPUUsage.before_eval_exp" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ExperienceCPUUsage.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/cpu_usage/#ExperienceCPUUsage.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ExperienceCPUUsage.__str__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.StreamCPUUsage">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">StreamCPUUsage</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/cpu_usage/#StreamCPUUsage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.StreamCPUUsage" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.cpu_usage.CPUPluginMetric</span></code></p>
<p>The average stream CPU usage metric.
This plugin metric works only at eval time.</p>
<p>After the entire evaluation stream, this metric emits
the average CPU usage on all experiences.</p>
<p>Creates an instance of the stream CPU usage metric.</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.StreamCPUUsage.before_eval">
<span class="sig-name descname"><span class="pre">before_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/cpu_usage/#StreamCPUUsage.before_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.StreamCPUUsage.before_eval" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.StreamCPUUsage.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/cpu_usage/#StreamCPUUsage.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.StreamCPUUsage.__str__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.cpu_usage_metrics">
<span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">cpu_usage_metrics</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">minibatch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epoch_running</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">experience</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stream</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">PluginMetric</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/cpu_usage/#cpu_usage_metrics"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.cpu_usage_metrics" title="Permalink to this definition"></a></dt>
<dd><p>Helper method that can be used to obtain the desired set of
plugin metrics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>minibatch</strong> – If True, will return a metric able to log the minibatch
CPU usage</p></li>
<li><p><strong>epoch</strong> – If True, will return a metric able to log the epoch
CPU usage</p></li>
<li><p><strong>epoch_running</strong> – If True, will return a metric able to log the running
epoch CPU usage.</p></li>
<li><p><strong>experience</strong> – If True, will return a metric able to log the experience
CPU usage.</p></li>
<li><p><strong>stream</strong> – If True, will return a metric able to log the evaluation
stream CPU usage.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of plugin metrics.</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.DiskUsage">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">DiskUsage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">paths_to_monitor</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">PathAlike</span><span class="p"><span class="pre">,</span> </span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">PathAlike</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/disk_usage/#DiskUsage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.DiskUsage" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Metric[float]</span></code></p>
<p>The standalone disk usage metric.</p>
<p>This metric can be used to monitor the size of a set of directories.
e.g. This can be useful to monitor the size of a replay buffer,</p>
<p>Creates an instance of the standalone disk usage metric.</p>
<p>The <cite>result</cite> method will return the sum of the size
of the directories specified as the first parameter in KiloBytes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>paths_to_monitor</strong> – a path or a list of paths to monitor. If None,
the current working directory is used. Defaults to None.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.DiskUsage.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/disk_usage/#DiskUsage.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.DiskUsage.update" title="Permalink to this definition"></a></dt>
<dd><p>Updates the disk usage statistics.</p>
<p>:return None.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.DiskUsage.result">
<span class="sig-name descname"><span class="pre">result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/disk_usage/#DiskUsage.result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.DiskUsage.result" title="Permalink to this definition"></a></dt>
<dd><p>Retrieves the disk usage as computed during the last call to the
<cite>update</cite> method.</p>
<p>Calling this method will not change the internal state of the metric.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The disk usage or None if <cite>update</cite> was not invoked yet.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.DiskUsage.reset">
<span class="sig-name descname"><span class="pre">reset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/disk_usage/#DiskUsage.reset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.DiskUsage.reset" title="Permalink to this definition"></a></dt>
<dd><p>Resets the metric.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.DiskUsage.get_dir_size">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">get_dir_size</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/disk_usage/#DiskUsage.get_dir_size"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.DiskUsage.get_dir_size" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MinibatchDiskUsage">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">MinibatchDiskUsage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">paths_to_monitor</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/disk_usage/#MinibatchDiskUsage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MinibatchDiskUsage" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.disk_usage.DiskPluginMetric</span></code></p>
<p>The minibatch Disk usage metric.
This plugin metric only works at training time.</p>
<p>At the end of each iteration, this metric logs the total
size (in KB) of all the monitored paths.</p>
<p>If a more coarse-grained logging is needed, consider using
<a class="reference internal" href="#avalanche.evaluation.metrics.EpochDiskUsage" title="avalanche.evaluation.metrics.EpochDiskUsage"><code class="xref py py-class docutils literal notranslate"><span class="pre">EpochDiskUsage</span></code></a>.</p>
<p>Creates an instance of the minibatch Disk usage metric.</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MinibatchDiskUsage.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/disk_usage/#MinibatchDiskUsage.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MinibatchDiskUsage.__str__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.EpochDiskUsage">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">EpochDiskUsage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">paths_to_monitor</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/disk_usage/#EpochDiskUsage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.EpochDiskUsage" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.disk_usage.DiskPluginMetric</span></code></p>
<p>The Epoch Disk usage metric.
This plugin metric only works at training time.</p>
<p>At the end of each epoch, this metric logs the total
size (in KB) of all the monitored paths.</p>
<p>Creates an instance of the epoch Disk usage metric.</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.EpochDiskUsage.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/disk_usage/#EpochDiskUsage.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.EpochDiskUsage.__str__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ExperienceDiskUsage">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">ExperienceDiskUsage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">paths_to_monitor</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/disk_usage/#ExperienceDiskUsage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ExperienceDiskUsage" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.disk_usage.DiskPluginMetric</span></code></p>
<p>The average experience Disk usage metric.
This plugin metric works only at eval time.</p>
<p>At the end of each experience, this metric logs the total
size (in KB) of all the monitored paths.</p>
<p>Creates an instance of the experience Disk usage metric.</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ExperienceDiskUsage.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/disk_usage/#ExperienceDiskUsage.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ExperienceDiskUsage.__str__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.StreamDiskUsage">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">StreamDiskUsage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">paths_to_monitor</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/disk_usage/#StreamDiskUsage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.StreamDiskUsage" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.disk_usage.DiskPluginMetric</span></code></p>
<p>The average stream Disk usage metric.
This plugin metric works only at eval time.</p>
<p>At the end of the eval stream, this metric logs the total
size (in KB) of all the monitored paths.</p>
<p>Creates an instance of the stream Disk usage metric.</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.StreamDiskUsage.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/disk_usage/#StreamDiskUsage.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.StreamDiskUsage.__str__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.disk_usage_metrics">
<span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">disk_usage_metrics</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">paths_to_monitor</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">minibatch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">experience</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stream</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">PluginMetric</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/disk_usage/#disk_usage_metrics"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.disk_usage_metrics" title="Permalink to this definition"></a></dt>
<dd><p>Helper method that can be used to obtain the desired set of
standalone metrics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>minibatch</strong> – If True, will return a metric able to log the minibatch
Disk usage</p></li>
<li><p><strong>epoch</strong> – If True, will return a metric able to log the epoch
Disk usage</p></li>
<li><p><strong>experience</strong> – If True, will return a metric able to log the experience
Disk usage.</p></li>
<li><p><strong>stream</strong> – If True, will return a metric able to log the evaluation
stream Disk usage.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of plugin metrics.</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.Forgetting">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">Forgetting</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#Forgetting"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.Forgetting" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Metric[Union[float,</span> <span class="pre">None,</span> <span class="pre">Dict[int,</span> <span class="pre">float]]]</span></code></p>
<p>The standalone Forgetting metric.
This metric returns the forgetting relative to a specific key.
Alternatively, this metric returns a dict in which each key is associated
to the forgetting.
Forgetting is computed as the difference between the first value recorded
for a specific key and the last value recorded for that key.
The value associated to a key can be update with the <cite>update</cite> method.</p>
<p>At initialization, this metric returns an empty dictionary.</p>
<p>Creates an instance of the standalone Forgetting metric</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.Forgetting.initial">
<span class="sig-name descname"><span class="pre">initial</span></span><em class="property"> <span class="pre">:Dict[int,</span> <span class="pre">float]</span></em><a class="headerlink" href="#avalanche.evaluation.metrics.Forgetting.initial" title="Permalink to this definition"></a></dt>
<dd><p>The initial value for each key.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.Forgetting.last">
<span class="sig-name descname"><span class="pre">last</span></span><em class="property"> <span class="pre">:Dict[int,</span> <span class="pre">float]</span></em><a class="headerlink" href="#avalanche.evaluation.metrics.Forgetting.last" title="Permalink to this definition"></a></dt>
<dd><p>The last value detected for each key</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.Forgetting.update_initial">
<span class="sig-name descname"><span class="pre">update_initial</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#Forgetting.update_initial"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.Forgetting.update_initial" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.Forgetting.update_last">
<span class="sig-name descname"><span class="pre">update_last</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#Forgetting.update_last"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.Forgetting.update_last" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.Forgetting.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#Forgetting.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.Forgetting.update" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.Forgetting.result">
<span class="sig-name descname"><span class="pre">result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="pre">None</span><span class="p"><span class="pre">,</span> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#Forgetting.result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.Forgetting.result" title="Permalink to this definition"></a></dt>
<dd><p>Forgetting is returned only for keys encountered twice.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>k</strong> – the key for which returning forgetting. If k has not
updated at least twice it returns None. If k is None,
forgetting will be returned for all keys encountered at least
twice.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the difference between the first and last value encountered
for k, if k is not None. It returns None if k has not been updated
at least twice. If k is None, returns a dictionary
containing keys whose value has been updated at least twice. The
associated value is the difference between the first and last
value recorded for that key.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.Forgetting.reset_last">
<span class="sig-name descname"><span class="pre">reset_last</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#Forgetting.reset_last"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.Forgetting.reset_last" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.Forgetting.reset">
<span class="sig-name descname"><span class="pre">reset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#Forgetting.reset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.Forgetting.reset" title="Permalink to this definition"></a></dt>
<dd><p>Resets the metric internal state.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericExperienceForgetting">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">GenericExperienceForgetting</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#GenericExperienceForgetting"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericExperienceForgetting" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">PluginMetric[Dict[int,</span> <span class="pre">float]]</span></code></p>
<p>The GenericExperienceForgetting metric, describing the change in
a metric detected for a certain experience. The user should
subclass this and provide the desired metric.</p>
<p>In particular, the user should override:
* __init__ by calling <cite>super</cite> and instantiating the <cite>self.current_metric</cite>
property as a valid avalanche metric
* <cite>metric_update</cite>, to update <cite>current_metric</cite>
* <cite>metric_result</cite> to get the result from <cite>current_metric</cite>.
* <cite>__str__</cite> to define the experience forgetting  name.</p>
<p>This plugin metric, computed separately for each experience,
is the difference between the metric result obtained after
first training on a experience and the metric result obtained
on the same experience at the end of successive experiences.</p>
<p>This metric is computed during the eval phase only.</p>
<p>Creates an instance of the GenericExperienceForgetting metric.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericExperienceForgetting.forgetting">
<span class="sig-name descname"><span class="pre">forgetting</span></span><a class="headerlink" href="#avalanche.evaluation.metrics.GenericExperienceForgetting.forgetting" title="Permalink to this definition"></a></dt>
<dd><p>The general metric to compute forgetting</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericExperienceForgetting.eval_exp_id">
<span class="sig-name descname"><span class="pre">eval_exp_id</span></span><a class="headerlink" href="#avalanche.evaluation.metrics.GenericExperienceForgetting.eval_exp_id" title="Permalink to this definition"></a></dt>
<dd><p>The current evaluation experience id</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericExperienceForgetting.train_exp_id">
<span class="sig-name descname"><span class="pre">train_exp_id</span></span><a class="headerlink" href="#avalanche.evaluation.metrics.GenericExperienceForgetting.train_exp_id" title="Permalink to this definition"></a></dt>
<dd><p>The last encountered training experience id</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericExperienceForgetting.reset">
<span class="sig-name descname"><span class="pre">reset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#GenericExperienceForgetting.reset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericExperienceForgetting.reset" title="Permalink to this definition"></a></dt>
<dd><p>Resets the metric.</p>
<p>Beware that this will also reset the initial metric of each
experience!</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericExperienceForgetting.reset_last">
<span class="sig-name descname"><span class="pre">reset_last</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#GenericExperienceForgetting.reset_last"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericExperienceForgetting.reset_last" title="Permalink to this definition"></a></dt>
<dd><p>Resets the last metric value.</p>
<p>This will preserve the initial metric value of each experience.
To be used at the beginning of each eval experience.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericExperienceForgetting.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#GenericExperienceForgetting.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericExperienceForgetting.update" title="Permalink to this definition"></a></dt>
<dd><p>Update forgetting metric.
See <cite>Forgetting</cite> for more detailed information.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>k</strong> – key to update</p></li>
<li><p><strong>v</strong> – value associated to k</p></li>
<li><p><strong>initial</strong> – update initial value. If False, update
last value.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericExperienceForgetting.result">
<span class="sig-name descname"><span class="pre">result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="pre">None</span><span class="p"><span class="pre">,</span> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#GenericExperienceForgetting.result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericExperienceForgetting.result" title="Permalink to this definition"></a></dt>
<dd><p>See <cite>Forgetting</cite> documentation for more detailed information.</p>
<p>k: optional key from which compute forgetting.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericExperienceForgetting.before_training_exp">
<span class="sig-name descname"><span class="pre">before_training_exp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">BaseStrategy</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#GenericExperienceForgetting.before_training_exp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericExperienceForgetting.before_training_exp" title="Permalink to this definition"></a></dt>
<dd><p>Called before <cite>train_exp</cite> by the <cite>BaseStrategy</cite>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericExperienceForgetting.before_eval">
<span class="sig-name descname"><span class="pre">before_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#GenericExperienceForgetting.before_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericExperienceForgetting.before_eval" title="Permalink to this definition"></a></dt>
<dd><p>Called before <cite>eval</cite> by the <cite>BaseStrategy</cite>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericExperienceForgetting.before_eval_exp">
<span class="sig-name descname"><span class="pre">before_eval_exp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">BaseStrategy</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#GenericExperienceForgetting.before_eval_exp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericExperienceForgetting.before_eval_exp" title="Permalink to this definition"></a></dt>
<dd><p>Called before <cite>eval_exp</cite> by the <cite>BaseStrategy</cite>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericExperienceForgetting.after_eval_iteration">
<span class="sig-name descname"><span class="pre">after_eval_iteration</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">BaseStrategy</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#GenericExperienceForgetting.after_eval_iteration"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericExperienceForgetting.after_eval_iteration" title="Permalink to this definition"></a></dt>
<dd><p>Called after the end of an iteration by the
<cite>BaseStrategy</cite>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericExperienceForgetting.after_eval_exp">
<span class="sig-name descname"><span class="pre">after_eval_exp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">BaseStrategy</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">MetricResult</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#GenericExperienceForgetting.after_eval_exp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericExperienceForgetting.after_eval_exp" title="Permalink to this definition"></a></dt>
<dd><p>Called after <cite>eval_exp</cite> by the <cite>BaseStrategy</cite>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericExperienceForgetting.metric_update">
<em class="property"><span class="pre">abstract</span> </em><span class="sig-name descname"><span class="pre">metric_update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#GenericExperienceForgetting.metric_update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericExperienceForgetting.metric_update" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericExperienceForgetting.metric_result">
<em class="property"><span class="pre">abstract</span> </em><span class="sig-name descname"><span class="pre">metric_result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#GenericExperienceForgetting.metric_result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericExperienceForgetting.metric_result" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericExperienceForgetting.__str__">
<em class="property"><span class="pre">abstract</span> </em><span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#GenericExperienceForgetting.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericExperienceForgetting.__str__" title="Permalink to this definition"></a></dt>
<dd><p>Return str(self).</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericStreamForgetting">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">GenericStreamForgetting</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#GenericStreamForgetting"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericStreamForgetting" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="forgetting_bwt/#avalanche.evaluation.metrics.forgetting_bwt.GenericExperienceForgetting" title="avalanche.evaluation.metrics.forgetting_bwt.GenericExperienceForgetting"><code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.forgetting_bwt.GenericExperienceForgetting</span></code></a></p>
<p>The GenericStreamForgetting metric, describing the average evaluation
change in the desired metric detected over all experiences observed
during training.</p>
<p>In particular, the user should override:
* __init__ by calling <cite>super</cite> and instantiating the <cite>self.current_metric</cite>
property as a valid avalanche metric
* <cite>metric_update</cite>, to update <cite>current_metric</cite>
* <cite>metric_result</cite> to get the result from <cite>current_metric</cite>.
* <cite>__str__</cite> to define the experience forgetting  name.</p>
<p>This plugin metric, computed over all observed experiences during training,
is the average over the difference between the metric result obtained
after first training on a experience and the metric result obtained
on the same experience at the end of successive experiences.</p>
<p>This metric is computed during the eval phase only.</p>
<p>Creates an instance of the GenericStreamForgetting metric.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericStreamForgetting.stream_forgetting">
<span class="sig-name descname"><span class="pre">stream_forgetting</span></span><a class="headerlink" href="#avalanche.evaluation.metrics.GenericStreamForgetting.stream_forgetting" title="Permalink to this definition"></a></dt>
<dd><p>The average forgetting over all experiences</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericStreamForgetting.reset">
<span class="sig-name descname"><span class="pre">reset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#GenericStreamForgetting.reset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericStreamForgetting.reset" title="Permalink to this definition"></a></dt>
<dd><p>Resets the forgetting metrics.</p>
<p>Beware that this will also reset the initial metric value of each
experience!</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericStreamForgetting.exp_update">
<span class="sig-name descname"><span class="pre">exp_update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#GenericStreamForgetting.exp_update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericStreamForgetting.exp_update" title="Permalink to this definition"></a></dt>
<dd><p>Update forgetting metric.
See <cite>Forgetting</cite> for more detailed information.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>k</strong> – key to update</p></li>
<li><p><strong>v</strong> – value associated to k</p></li>
<li><p><strong>initial</strong> – update initial value. If False, update
last value.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericStreamForgetting.exp_result">
<span class="sig-name descname"><span class="pre">exp_result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="pre">None</span><span class="p"><span class="pre">,</span> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#GenericStreamForgetting.exp_result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericStreamForgetting.exp_result" title="Permalink to this definition"></a></dt>
<dd><p>Result for experience defined by a key.
See <cite>Forgetting</cite> documentation for more detailed information.</p>
<p>k: optional key from which compute forgetting.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericStreamForgetting.result">
<span class="sig-name descname"><span class="pre">result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="pre">None</span><span class="p"><span class="pre">,</span> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#GenericStreamForgetting.result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericStreamForgetting.result" title="Permalink to this definition"></a></dt>
<dd><p>The average forgetting over all experience.</p>
<p>k: optional key from which compute forgetting.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericStreamForgetting.before_eval">
<span class="sig-name descname"><span class="pre">before_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#GenericStreamForgetting.before_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericStreamForgetting.before_eval" title="Permalink to this definition"></a></dt>
<dd><p>Called before <cite>eval</cite> by the <cite>BaseStrategy</cite>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericStreamForgetting.after_eval_exp">
<span class="sig-name descname"><span class="pre">after_eval_exp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">BaseStrategy</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#GenericStreamForgetting.after_eval_exp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericStreamForgetting.after_eval_exp" title="Permalink to this definition"></a></dt>
<dd><p>Called after <cite>eval_exp</cite> by the <cite>BaseStrategy</cite>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericStreamForgetting.after_eval">
<span class="sig-name descname"><span class="pre">after_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">BaseStrategy</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">'MetricResult'</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#GenericStreamForgetting.after_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericStreamForgetting.after_eval" title="Permalink to this definition"></a></dt>
<dd><p>Called after <cite>eval</cite> by the <cite>BaseStrategy</cite>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericStreamForgetting.metric_update">
<em class="property"><span class="pre">abstract</span> </em><span class="sig-name descname"><span class="pre">metric_update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#GenericStreamForgetting.metric_update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericStreamForgetting.metric_update" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericStreamForgetting.metric_result">
<em class="property"><span class="pre">abstract</span> </em><span class="sig-name descname"><span class="pre">metric_result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#GenericStreamForgetting.metric_result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericStreamForgetting.metric_result" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericStreamForgetting.__str__">
<em class="property"><span class="pre">abstract</span> </em><span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#GenericStreamForgetting.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericStreamForgetting.__str__" title="Permalink to this definition"></a></dt>
<dd><p>Return str(self).</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ExperienceForgetting">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">ExperienceForgetting</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#ExperienceForgetting"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ExperienceForgetting" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="forgetting_bwt/#avalanche.evaluation.metrics.forgetting_bwt.GenericExperienceForgetting" title="avalanche.evaluation.metrics.forgetting_bwt.GenericExperienceForgetting"><code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.forgetting_bwt.GenericExperienceForgetting</span></code></a></p>
<p>The ExperienceForgetting metric, describing the accuracy loss
detected for a certain experience.</p>
<p>This plugin metric, computed separately for each experience,
is the difference between the accuracy result obtained after
first training on a experience and the accuracy result obtained
on the same experience at the end of successive experiences.</p>
<p>This metric is computed during the eval phase only.</p>
<p>Creates an instance of the ExperienceForgetting metric.</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ExperienceForgetting.metric_update">
<span class="sig-name descname"><span class="pre">metric_update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#ExperienceForgetting.metric_update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ExperienceForgetting.metric_update" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ExperienceForgetting.metric_result">
<span class="sig-name descname"><span class="pre">metric_result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#ExperienceForgetting.metric_result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ExperienceForgetting.metric_result" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ExperienceForgetting.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#ExperienceForgetting.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ExperienceForgetting.__str__" title="Permalink to this definition"></a></dt>
<dd><p>Return str(self).</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.StreamForgetting">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">StreamForgetting</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#StreamForgetting"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.StreamForgetting" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="forgetting_bwt/#avalanche.evaluation.metrics.forgetting_bwt.GenericStreamForgetting" title="avalanche.evaluation.metrics.forgetting_bwt.GenericStreamForgetting"><code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.forgetting_bwt.GenericStreamForgetting</span></code></a></p>
<p>The StreamForgetting metric, describing the average evaluation accuracy loss
detected over all experiences observed during training.</p>
<p>This plugin metric, computed over all observed experiences during training,
is the average over the difference between the accuracy result obtained
after first training on a experience and the accuracy result obtained
on the same experience at the end of successive experiences.</p>
<p>This metric is computed during the eval phase only.</p>
<p>Creates an instance of the StreamForgetting metric.</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.StreamForgetting.metric_update">
<span class="sig-name descname"><span class="pre">metric_update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#StreamForgetting.metric_update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.StreamForgetting.metric_update" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.StreamForgetting.metric_result">
<span class="sig-name descname"><span class="pre">metric_result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#StreamForgetting.metric_result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.StreamForgetting.metric_result" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.StreamForgetting.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#StreamForgetting.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.StreamForgetting.__str__" title="Permalink to this definition"></a></dt>
<dd><p>Return str(self).</p>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.forgetting_metrics">
<span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">forgetting_metrics</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">experience</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stream</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">PluginMetric</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#forgetting_metrics"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.forgetting_metrics" title="Permalink to this definition"></a></dt>
<dd><p>Helper method that can be used to obtain the desired set of
plugin metrics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>experience</strong> – If True, will return a metric able to log
the forgetting on each evaluation experience.</p></li>
<li><p><strong>stream</strong> – If True, will return a metric able to log
the forgetting averaged over the evaluation stream experiences,
which have been observed during training.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of plugin metrics.</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.BWT">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">BWT</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#BWT"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.BWT" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="forgetting_bwt/#avalanche.evaluation.metrics.forgetting_bwt.Forgetting" title="avalanche.evaluation.metrics.forgetting_bwt.Forgetting"><code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.forgetting_bwt.Forgetting</span></code></a></p>
<p>The standalone Backward Transfer metric.
This metric returns the backward transfer relative to a specific key.
Alternatively, this metric returns a dict in which each key is associated
to the backward transfer.
Backward transfer is computed as the difference between the last value
recorded for a specific key and the first value recorded for that key.
The value associated to a key can be update with the <cite>update</cite> method.</p>
<p>At initialization, this metric returns an empty dictionary.</p>
<p>Creates an instance of the standalone Forgetting metric</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.BWT.result">
<span class="sig-name descname"><span class="pre">result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="pre">None</span><span class="p"><span class="pre">,</span> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#BWT.result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.BWT.result" title="Permalink to this definition"></a></dt>
<dd><p>Backward Transfer is returned only for keys encountered twice.
Backward Transfer is the negative forgetting.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>k</strong> – the key for which returning backward transfer. If k has not
updated at least twice it returns None. If k is None,
backward transfer will be returned for all keys encountered at
least twice.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the difference between the last value encountered for k
and its first value, if k is not None.
It returns None if k has not been updated
at least twice. If k is None, returns a dictionary
containing keys whose value has been updated at least twice. The
associated value is the difference between the last and first
value recorded for that key.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ExperienceBWT">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">ExperienceBWT</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#ExperienceBWT"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ExperienceBWT" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="forgetting_bwt/#avalanche.evaluation.metrics.forgetting_bwt.ExperienceForgetting" title="avalanche.evaluation.metrics.forgetting_bwt.ExperienceForgetting"><code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.forgetting_bwt.ExperienceForgetting</span></code></a></p>
<p>The Experience Backward Transfer metric.</p>
<p>This plugin metric, computed separately for each experience,
is the difference between the last accuracy result obtained on a certain
experience and the accuracy result obtained when first training on that
experience.</p>
<p>This metric is computed during the eval phase only.</p>
<p>Creates an instance of the ExperienceForgetting metric.</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ExperienceBWT.result">
<span class="sig-name descname"><span class="pre">result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="pre">None</span><span class="p"><span class="pre">,</span> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#ExperienceBWT.result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ExperienceBWT.result" title="Permalink to this definition"></a></dt>
<dd><p>See <cite>Forgetting</cite> documentation for more detailed information.</p>
<p>k: optional key from which compute forgetting.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ExperienceBWT.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#ExperienceBWT.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ExperienceBWT.__str__" title="Permalink to this definition"></a></dt>
<dd><p>Return str(self).</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.StreamBWT">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">StreamBWT</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#StreamBWT"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.StreamBWT" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="forgetting_bwt/#avalanche.evaluation.metrics.forgetting_bwt.StreamForgetting" title="avalanche.evaluation.metrics.forgetting_bwt.StreamForgetting"><code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.forgetting_bwt.StreamForgetting</span></code></a></p>
<p>The StreamBWT metric, emitting the average BWT across all experiences
encountered during training.</p>
<p>This plugin metric, computed over all observed experiences during training,
is the average over the difference between the last accuracy result
obtained on an experience and the accuracy result obtained when first
training on that experience.</p>
<p>This metric is computed during the eval phase only.</p>
<p>Creates an instance of the StreamForgetting metric.</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.StreamBWT.exp_result">
<span class="sig-name descname"><span class="pre">exp_result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="pre">None</span><span class="p"><span class="pre">,</span> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#StreamBWT.exp_result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.StreamBWT.exp_result" title="Permalink to this definition"></a></dt>
<dd><p>Result for experience defined by a key.
See <cite>BWT</cite> documentation for more detailed information.</p>
<p>k: optional key from which compute backward transfer.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.StreamBWT.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#StreamBWT.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.StreamBWT.__str__" title="Permalink to this definition"></a></dt>
<dd><p>Return str(self).</p>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.bwt_metrics">
<span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">bwt_metrics</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">experience</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stream</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">PluginMetric</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forgetting_bwt/#bwt_metrics"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.bwt_metrics" title="Permalink to this definition"></a></dt>
<dd><p>Helper method that can be used to obtain the desired set of
plugin metrics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>experience</strong> – If True, will return a metric able to log
the backward transfer on each evaluation experience.</p></li>
<li><p><strong>stream</strong> – If True, will return a metric able to log
the backward transfer averaged over the evaluation stream experiences
which have been observed during training.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of plugin metrics.</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ForwardTransfer">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">ForwardTransfer</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forward_transfer/#ForwardTransfer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ForwardTransfer" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Metric[Union[float,</span> <span class="pre">None,</span> <span class="pre">Dict[int,</span> <span class="pre">float]]]</span></code></p>
<p>The standalone Forward Transfer metric.
This metric returns the forward transfer relative to a specific key.
Alternatively, this metric returns a dict in which each key is
associated to the forward transfer.
Forward transfer is computed as the difference between the value
recorded for a specific key after the previous experience has
been trained on, and random initialization before training.
The value associated to a key can be updated with the <cite>update</cite> method.</p>
<p>At initialization, this metric returns an empty dictionary.</p>
<p>Creates an instance of the standalone Forward Transfer metric</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ForwardTransfer.initial">
<span class="sig-name descname"><span class="pre">initial</span></span><em class="property"> <span class="pre">:Dict[int,</span> <span class="pre">float]</span></em><a class="headerlink" href="#avalanche.evaluation.metrics.ForwardTransfer.initial" title="Permalink to this definition"></a></dt>
<dd><p>The initial value for each key. This is the accuracy at
random initialization.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ForwardTransfer.previous">
<span class="sig-name descname"><span class="pre">previous</span></span><em class="property"> <span class="pre">:Dict[int,</span> <span class="pre">float]</span></em><a class="headerlink" href="#avalanche.evaluation.metrics.ForwardTransfer.previous" title="Permalink to this definition"></a></dt>
<dd><p>The previous experience value detected for each key</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ForwardTransfer.update_initial">
<span class="sig-name descname"><span class="pre">update_initial</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forward_transfer/#ForwardTransfer.update_initial"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ForwardTransfer.update_initial" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ForwardTransfer.update_previous">
<span class="sig-name descname"><span class="pre">update_previous</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forward_transfer/#ForwardTransfer.update_previous"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ForwardTransfer.update_previous" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ForwardTransfer.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forward_transfer/#ForwardTransfer.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ForwardTransfer.update" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ForwardTransfer.result">
<span class="sig-name descname"><span class="pre">result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="pre">None</span><span class="p"><span class="pre">,</span> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forward_transfer/#ForwardTransfer.result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ForwardTransfer.result" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>k</strong> – the key for which returning forward transfer. If k is None,
forward transfer will be returned for all keys
where the previous experience has been trained on.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the difference between the key value after training on the
previous experience, and the key at random initialization.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ForwardTransfer.reset">
<span class="sig-name descname"><span class="pre">reset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forward_transfer/#ForwardTransfer.reset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ForwardTransfer.reset" title="Permalink to this definition"></a></dt>
<dd><p>Resets the metric internal state.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericExperienceForwardTransfer">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">GenericExperienceForwardTransfer</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forward_transfer/#GenericExperienceForwardTransfer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericExperienceForwardTransfer" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">PluginMetric[Dict[int,</span> <span class="pre">float]]</span></code></p>
<p>The GenericExperienceForwardMetric metric, describing the forward transfer
detected after a certain experience. The user should
subclass this and provide the desired metric.</p>
<p>In particular, the user should override:
* __init__ by calling <cite>super</cite> and instantiating the <cite>self.current_metric</cite>
property as a valid avalanche metric
* <cite>metric_update</cite>, to update <cite>current_metric</cite>
* <cite>metric_result</cite> to get the result from <cite>current_metric</cite>.
* <cite>__str__</cite> to define the experience forward transfer  name.</p>
<p>This metric is computed during the eval phase only.</p>
<p>Creates an instance of the GenericExperienceForwardTransfer metric.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericExperienceForwardTransfer.forward_transfer">
<span class="sig-name descname"><span class="pre">forward_transfer</span></span><a class="headerlink" href="#avalanche.evaluation.metrics.GenericExperienceForwardTransfer.forward_transfer" title="Permalink to this definition"></a></dt>
<dd><p>The general metric to compute forward transfer</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericExperienceForwardTransfer.eval_exp_id">
<span class="sig-name descname"><span class="pre">eval_exp_id</span></span><a class="headerlink" href="#avalanche.evaluation.metrics.GenericExperienceForwardTransfer.eval_exp_id" title="Permalink to this definition"></a></dt>
<dd><p>The current evaluation experience id</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericExperienceForwardTransfer.train_exp_id">
<span class="sig-name descname"><span class="pre">train_exp_id</span></span><a class="headerlink" href="#avalanche.evaluation.metrics.GenericExperienceForwardTransfer.train_exp_id" title="Permalink to this definition"></a></dt>
<dd><p>The last encountered training experience id</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericExperienceForwardTransfer.reset">
<span class="sig-name descname"><span class="pre">reset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forward_transfer/#GenericExperienceForwardTransfer.reset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericExperienceForwardTransfer.reset" title="Permalink to this definition"></a></dt>
<dd><p>Resets the metric.</p>
<p>Note that this will reset the previous and initial accuracy of each
experience.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericExperienceForwardTransfer.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forward_transfer/#GenericExperienceForwardTransfer.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericExperienceForwardTransfer.update" title="Permalink to this definition"></a></dt>
<dd><p>Update forward transfer metric.
See <cite>ForwardTransfer</cite> for more detailed information.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>k</strong> – key to update</p></li>
<li><p><strong>v</strong> – value associated to k</p></li>
<li><p><strong>initial</strong> – update initial value. If False, update
previous value.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericExperienceForwardTransfer.result">
<span class="sig-name descname"><span class="pre">result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="pre">None</span><span class="p"><span class="pre">,</span> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forward_transfer/#GenericExperienceForwardTransfer.result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericExperienceForwardTransfer.result" title="Permalink to this definition"></a></dt>
<dd><p>Result for experience defined by a key.
See <cite>ForwardTransfer</cite> documentation for more detailed information.</p>
<p>k: optional key from which to compute forward transfer.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericExperienceForwardTransfer.before_training_exp">
<span class="sig-name descname"><span class="pre">before_training_exp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">BaseStrategy</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forward_transfer/#GenericExperienceForwardTransfer.before_training_exp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericExperienceForwardTransfer.before_training_exp" title="Permalink to this definition"></a></dt>
<dd><p>Called before <cite>train_exp</cite> by the <cite>BaseStrategy</cite>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericExperienceForwardTransfer.after_eval">
<span class="sig-name descname"><span class="pre">after_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forward_transfer/#GenericExperienceForwardTransfer.after_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericExperienceForwardTransfer.after_eval" title="Permalink to this definition"></a></dt>
<dd><p>Called after <cite>eval</cite> by the <cite>BaseStrategy</cite>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericExperienceForwardTransfer.before_eval_exp">
<span class="sig-name descname"><span class="pre">before_eval_exp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">BaseStrategy</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forward_transfer/#GenericExperienceForwardTransfer.before_eval_exp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericExperienceForwardTransfer.before_eval_exp" title="Permalink to this definition"></a></dt>
<dd><p>Called before <cite>eval_exp</cite> by the <cite>BaseStrategy</cite>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericExperienceForwardTransfer.after_eval_iteration">
<span class="sig-name descname"><span class="pre">after_eval_iteration</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">BaseStrategy</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forward_transfer/#GenericExperienceForwardTransfer.after_eval_iteration"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericExperienceForwardTransfer.after_eval_iteration" title="Permalink to this definition"></a></dt>
<dd><p>Called after the end of an iteration by the
<cite>BaseStrategy</cite>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericExperienceForwardTransfer.after_eval_exp">
<span class="sig-name descname"><span class="pre">after_eval_exp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">BaseStrategy</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">MetricResult</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forward_transfer/#GenericExperienceForwardTransfer.after_eval_exp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericExperienceForwardTransfer.after_eval_exp" title="Permalink to this definition"></a></dt>
<dd><p>Called after <cite>eval_exp</cite> by the <cite>BaseStrategy</cite>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericExperienceForwardTransfer.metric_update">
<em class="property"><span class="pre">abstract</span> </em><span class="sig-name descname"><span class="pre">metric_update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forward_transfer/#GenericExperienceForwardTransfer.metric_update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericExperienceForwardTransfer.metric_update" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericExperienceForwardTransfer.metric_result">
<em class="property"><span class="pre">abstract</span> </em><span class="sig-name descname"><span class="pre">metric_result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forward_transfer/#GenericExperienceForwardTransfer.metric_result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericExperienceForwardTransfer.metric_result" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericExperienceForwardTransfer.__str__">
<em class="property"><span class="pre">abstract</span> </em><span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forward_transfer/#GenericExperienceForwardTransfer.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericExperienceForwardTransfer.__str__" title="Permalink to this definition"></a></dt>
<dd><p>Return str(self).</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ExperienceForwardTransfer">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">ExperienceForwardTransfer</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forward_transfer/#ExperienceForwardTransfer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ExperienceForwardTransfer" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="forward_transfer/#avalanche.evaluation.metrics.forward_transfer.GenericExperienceForwardTransfer" title="avalanche.evaluation.metrics.forward_transfer.GenericExperienceForwardTransfer"><code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.forward_transfer.GenericExperienceForwardTransfer</span></code></a></p>
<p>The Forward Transfer computed on each experience separately.
The transfer is computed based on the accuracy metric.</p>
<p>Creates an instance of the GenericExperienceForwardTransfer metric.</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ExperienceForwardTransfer.metric_update">
<span class="sig-name descname"><span class="pre">metric_update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forward_transfer/#ExperienceForwardTransfer.metric_update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ExperienceForwardTransfer.metric_update" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ExperienceForwardTransfer.metric_result">
<span class="sig-name descname"><span class="pre">metric_result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forward_transfer/#ExperienceForwardTransfer.metric_result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ExperienceForwardTransfer.metric_result" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ExperienceForwardTransfer.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forward_transfer/#ExperienceForwardTransfer.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ExperienceForwardTransfer.__str__" title="Permalink to this definition"></a></dt>
<dd><p>Return str(self).</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericStreamForwardTransfer">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">GenericStreamForwardTransfer</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forward_transfer/#GenericStreamForwardTransfer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericStreamForwardTransfer" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="forward_transfer/#avalanche.evaluation.metrics.forward_transfer.GenericExperienceForwardTransfer" title="avalanche.evaluation.metrics.forward_transfer.GenericExperienceForwardTransfer"><code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.forward_transfer.GenericExperienceForwardTransfer</span></code></a></p>
<p>The GenericStreamForwardTransfer metric, describing the average evaluation
forward transfer detected over all experiences observed during training.</p>
<p>In particular, the user should override:
* __init__ by calling <cite>super</cite> and instantiating the <cite>self.current_metric</cite>
property as a valid avalanche metric
* <cite>metric_update</cite>, to update <cite>current_metric</cite>
* <cite>metric_result</cite> to get the result from <cite>current_metric</cite>.
* <cite>__str__</cite> to define the experience forgetting  name.</p>
<p>This metric is computed during the eval phase only.</p>
<p>Creates an instance of the GenericStreamForwardTransfer metric.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericStreamForwardTransfer.stream_forward_transfer">
<span class="sig-name descname"><span class="pre">stream_forward_transfer</span></span><a class="headerlink" href="#avalanche.evaluation.metrics.GenericStreamForwardTransfer.stream_forward_transfer" title="Permalink to this definition"></a></dt>
<dd><p>The average forward transfer over all experiences</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericStreamForwardTransfer.reset">
<span class="sig-name descname"><span class="pre">reset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forward_transfer/#GenericStreamForwardTransfer.reset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericStreamForwardTransfer.reset" title="Permalink to this definition"></a></dt>
<dd><p>Resets the forward transfer metrics.</p>
<p>Note that this will reset the previous and initial accuracy of each
experience.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericStreamForwardTransfer.exp_update">
<span class="sig-name descname"><span class="pre">exp_update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forward_transfer/#GenericStreamForwardTransfer.exp_update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericStreamForwardTransfer.exp_update" title="Permalink to this definition"></a></dt>
<dd><p>Update forward transfer metric.
See <cite>Forward Transfer</cite> for more detailed information.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>k</strong> – key to update</p></li>
<li><p><strong>v</strong> – value associated to k</p></li>
<li><p><strong>initial</strong> – update initial value. If False, update
previous value.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericStreamForwardTransfer.exp_result">
<span class="sig-name descname"><span class="pre">exp_result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="pre">None</span><span class="p"><span class="pre">,</span> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forward_transfer/#GenericStreamForwardTransfer.exp_result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericStreamForwardTransfer.exp_result" title="Permalink to this definition"></a></dt>
<dd><p>Result for experience defined by a key.
See <cite>ForwardTransfer</cite> documentation for more detailed information.</p>
<p>k: optional key from which to compute forward transfer.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericStreamForwardTransfer.result">
<span class="sig-name descname"><span class="pre">result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="pre">None</span><span class="p"><span class="pre">,</span> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forward_transfer/#GenericStreamForwardTransfer.result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericStreamForwardTransfer.result" title="Permalink to this definition"></a></dt>
<dd><p>The average forward transfer over all experiences.</p>
<p>k: optional key from which to compute forward transfer.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericStreamForwardTransfer.before_eval">
<span class="sig-name descname"><span class="pre">before_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forward_transfer/#GenericStreamForwardTransfer.before_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericStreamForwardTransfer.before_eval" title="Permalink to this definition"></a></dt>
<dd><p>Called before <cite>eval</cite> by the <cite>BaseStrategy</cite>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericStreamForwardTransfer.after_eval_exp">
<span class="sig-name descname"><span class="pre">after_eval_exp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">BaseStrategy</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forward_transfer/#GenericStreamForwardTransfer.after_eval_exp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericStreamForwardTransfer.after_eval_exp" title="Permalink to this definition"></a></dt>
<dd><p>Called after <cite>eval_exp</cite> by the <cite>BaseStrategy</cite>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericStreamForwardTransfer.after_eval">
<span class="sig-name descname"><span class="pre">after_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">BaseStrategy</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">'MetricResult'</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forward_transfer/#GenericStreamForwardTransfer.after_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericStreamForwardTransfer.after_eval" title="Permalink to this definition"></a></dt>
<dd><p>Called after <cite>eval</cite> by the <cite>BaseStrategy</cite>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericStreamForwardTransfer.metric_update">
<em class="property"><span class="pre">abstract</span> </em><span class="sig-name descname"><span class="pre">metric_update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forward_transfer/#GenericStreamForwardTransfer.metric_update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericStreamForwardTransfer.metric_update" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericStreamForwardTransfer.metric_result">
<em class="property"><span class="pre">abstract</span> </em><span class="sig-name descname"><span class="pre">metric_result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forward_transfer/#GenericStreamForwardTransfer.metric_result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericStreamForwardTransfer.metric_result" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.GenericStreamForwardTransfer.__str__">
<em class="property"><span class="pre">abstract</span> </em><span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forward_transfer/#GenericStreamForwardTransfer.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.GenericStreamForwardTransfer.__str__" title="Permalink to this definition"></a></dt>
<dd><p>Return str(self).</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.StreamForwardTransfer">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">StreamForwardTransfer</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forward_transfer/#StreamForwardTransfer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.StreamForwardTransfer" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="forward_transfer/#avalanche.evaluation.metrics.forward_transfer.GenericStreamForwardTransfer" title="avalanche.evaluation.metrics.forward_transfer.GenericStreamForwardTransfer"><code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.forward_transfer.GenericStreamForwardTransfer</span></code></a></p>
<p>The Forward Transfer averaged over all the evaluation experiences.</p>
<p>This plugin metric, computed over all observed experiences during training,
is the average over the difference between the accuracy result obtained
after the previous experience and the accuracy result obtained
on random initialization.</p>
<p>Creates an instance of the GenericStreamForwardTransfer metric.</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.StreamForwardTransfer.metric_update">
<span class="sig-name descname"><span class="pre">metric_update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forward_transfer/#StreamForwardTransfer.metric_update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.StreamForwardTransfer.metric_update" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.StreamForwardTransfer.metric_result">
<span class="sig-name descname"><span class="pre">metric_result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forward_transfer/#StreamForwardTransfer.metric_result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.StreamForwardTransfer.metric_result" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.StreamForwardTransfer.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forward_transfer/#StreamForwardTransfer.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.StreamForwardTransfer.__str__" title="Permalink to this definition"></a></dt>
<dd><p>Return str(self).</p>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.forward_transfer_metrics">
<span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">forward_transfer_metrics</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">experience</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stream</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/forward_transfer/#forward_transfer_metrics"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.forward_transfer_metrics" title="Permalink to this definition"></a></dt>
<dd><p>Helper method that can be used to obtain the desired set of
plugin metrics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>experience</strong> – If True, will return a metric able to log
the forward transfer on each evaluation experience.</p></li>
<li><p><strong>stream</strong> – If True, will return a metric able to log
the forward transfer averaged over the evaluation stream experiences,
which have been observed during training.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of plugin metrics.</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MaxGPU">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">MaxGPU</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">gpu_id</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">every</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/gpu_usage/#MaxGPU"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MaxGPU" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Metric[float]</span></code></p>
<p>The standalone GPU usage metric.
Important: this metric approximates the real maximum GPU percentage</p>
<blockquote>
<div><p>usage since it sample at discrete amount of time the GPU values.</p>
</div></blockquote>
<p>Instances of this metric keeps the maximum GPU usage percentage detected.
The <cite>start_thread</cite> method starts the usage tracking.
The <cite>stop_thread</cite> method stops the tracking.</p>
<p>The result, obtained using the <cite>result</cite> method, is the usage in mega-bytes.</p>
<p>The reset method will bring the metric to its initial state. By default
this metric in its initial state will return an usage value of 0.</p>
<p>Creates an instance of the GPU usage metric.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>gpu_id</strong> – GPU device ID.</p></li>
<li><p><strong>every</strong> – seconds after which update the maximum GPU
usage</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MaxGPU.thread">
<span class="sig-name descname"><span class="pre">thread</span></span><a class="headerlink" href="#avalanche.evaluation.metrics.MaxGPU.thread" title="Permalink to this definition"></a></dt>
<dd><p>Thread executing GPU monitoring code</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MaxGPU.stop_f">
<span class="sig-name descname"><span class="pre">stop_f</span></span><em class="property"> <span class="pre">=</span> <span class="pre">False</span></em><a class="headerlink" href="#avalanche.evaluation.metrics.MaxGPU.stop_f" title="Permalink to this definition"></a></dt>
<dd><p>Flag to stop the thread</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MaxGPU.max_usage">
<span class="sig-name descname"><span class="pre">max_usage</span></span><em class="property"> <span class="pre">=</span> <span class="pre">0</span></em><a class="headerlink" href="#avalanche.evaluation.metrics.MaxGPU.max_usage" title="Permalink to this definition"></a></dt>
<dd><p>Main metric result. Max GPU usage.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MaxGPU.start_thread">
<span class="sig-name descname"><span class="pre">start_thread</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/gpu_usage/#MaxGPU.start_thread"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MaxGPU.start_thread" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MaxGPU.stop_thread">
<span class="sig-name descname"><span class="pre">stop_thread</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/gpu_usage/#MaxGPU.stop_thread"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MaxGPU.stop_thread" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MaxGPU.reset">
<span class="sig-name descname"><span class="pre">reset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/gpu_usage/#MaxGPU.reset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MaxGPU.reset" title="Permalink to this definition"></a></dt>
<dd><p>Resets the metric.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MaxGPU.result">
<span class="sig-name descname"><span class="pre">result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/gpu_usage/#MaxGPU.result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MaxGPU.result" title="Permalink to this definition"></a></dt>
<dd><p>Returns the max GPU percentage value.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The percentage GPU usage as a float value in range [0, 1].</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MaxGPU.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/gpu_usage/#MaxGPU.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MaxGPU.update" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MinibatchMaxGPU">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">MinibatchMaxGPU</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">gpu_id</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">every</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/gpu_usage/#MinibatchMaxGPU"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MinibatchMaxGPU" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.gpu_usage.GPUPluginMetric</span></code></p>
<p>The Minibatch Max GPU metric.
This plugin metric only works at training time.</p>
<p>Creates an instance of the Minibatch Max GPU metric</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>gpu_id</strong> – GPU device ID.</p></li>
<li><p><strong>every</strong> – seconds after which update the maximum GPU
usage</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MinibatchMaxGPU.before_training">
<span class="sig-name descname"><span class="pre">before_training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">BaseStrategy</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/gpu_usage/#MinibatchMaxGPU.before_training"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MinibatchMaxGPU.before_training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MinibatchMaxGPU.after_training">
<span class="sig-name descname"><span class="pre">after_training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">BaseStrategy</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/gpu_usage/#MinibatchMaxGPU.after_training"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MinibatchMaxGPU.after_training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MinibatchMaxGPU.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/gpu_usage/#MinibatchMaxGPU.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MinibatchMaxGPU.__str__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.EpochMaxGPU">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">EpochMaxGPU</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">gpu_id</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">every</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/gpu_usage/#EpochMaxGPU"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.EpochMaxGPU" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.gpu_usage.GPUPluginMetric</span></code></p>
<p>The Epoch Max GPU metric.
This plugin metric only works at training time.</p>
<p>Creates an instance of the epoch Max GPU metric.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>gpu_id</strong> – GPU device ID.</p></li>
<li><p><strong>every</strong> – seconds after which update the maximum GPU
usage</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.EpochMaxGPU.before_training">
<span class="sig-name descname"><span class="pre">before_training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">BaseStrategy</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/gpu_usage/#EpochMaxGPU.before_training"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.EpochMaxGPU.before_training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.EpochMaxGPU.after_training">
<span class="sig-name descname"><span class="pre">after_training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">BaseStrategy</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/gpu_usage/#EpochMaxGPU.after_training"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.EpochMaxGPU.after_training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.EpochMaxGPU.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/gpu_usage/#EpochMaxGPU.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.EpochMaxGPU.__str__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ExperienceMaxGPU">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">ExperienceMaxGPU</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">gpu_id</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">every</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/gpu_usage/#ExperienceMaxGPU"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ExperienceMaxGPU" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.gpu_usage.GPUPluginMetric</span></code></p>
<p>The Experience Max GPU metric.
This plugin metric only works at eval time.</p>
<p>Creates an instance of the Experience CPU usage metric.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>gpu_id</strong> – GPU device ID.</p></li>
<li><p><strong>every</strong> – seconds after which update the maximum GPU
usage</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ExperienceMaxGPU.before_eval">
<span class="sig-name descname"><span class="pre">before_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">BaseStrategy</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/gpu_usage/#ExperienceMaxGPU.before_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ExperienceMaxGPU.before_eval" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ExperienceMaxGPU.after_eval">
<span class="sig-name descname"><span class="pre">after_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">BaseStrategy</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/gpu_usage/#ExperienceMaxGPU.after_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ExperienceMaxGPU.after_eval" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ExperienceMaxGPU.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/gpu_usage/#ExperienceMaxGPU.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ExperienceMaxGPU.__str__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.StreamMaxGPU">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">StreamMaxGPU</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">gpu_id</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">every</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/gpu_usage/#StreamMaxGPU"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.StreamMaxGPU" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.gpu_usage.GPUPluginMetric</span></code></p>
<p>The Stream Max GPU metric.
This plugin metric only works at eval time.</p>
<p>Creates an instance of the Experience CPU usage metric.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>gpu_id</strong> – GPU device ID.</p></li>
<li><p><strong>every</strong> – seconds after which update the maximum GPU
usage</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.StreamMaxGPU.before_eval">
<span class="sig-name descname"><span class="pre">before_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/gpu_usage/#StreamMaxGPU.before_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.StreamMaxGPU.before_eval" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.StreamMaxGPU.after_eval">
<span class="sig-name descname"><span class="pre">after_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">BaseStrategy</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">MetricResult</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/gpu_usage/#StreamMaxGPU.after_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.StreamMaxGPU.after_eval" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.StreamMaxGPU.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/gpu_usage/#StreamMaxGPU.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.StreamMaxGPU.__str__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.gpu_usage_metrics">
<span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">gpu_usage_metrics</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">gpu_id</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">every</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">minibatch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">experience</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stream</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">PluginMetric</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/gpu_usage/#gpu_usage_metrics"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.gpu_usage_metrics" title="Permalink to this definition"></a></dt>
<dd><p>Helper method that can be used to obtain the desired set of
plugin metrics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>gpu_id</strong> – GPU device ID.</p></li>
<li><p><strong>every</strong> – seconds after which update the maximum GPU
usage</p></li>
<li><p><strong>minibatch</strong> – If True, will return a metric able to log the minibatch
max GPU usage.</p></li>
<li><p><strong>epoch</strong> – If True, will return a metric able to log the epoch
max GPU usage.</p></li>
<li><p><strong>experience</strong> – If True, will return a metric able to log the experience
max GPU usage.</p></li>
<li><p><strong>stream</strong> – If True, will return a metric able to log the evaluation
max stream GPU usage.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of plugin metrics.</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.Loss">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">Loss</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/loss/#Loss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.Loss" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Metric[float]</span></code></p>
<p>The standalone Loss metric. This is a general metric
used to compute more specific ones.</p>
<p>Instances of this metric keeps the running average loss
over multiple &lt;prediction, target&gt; pairs of Tensors,
provided incrementally.
The “prediction” and “target” tensors may contain plain labels or
one-hot/logit vectors.</p>
<p>Each time <cite>result</cite> is called, this metric emits the average loss
across all predictions made since the last <cite>reset</cite>.</p>
<p>The reset method will bring the metric to its initial state. By default
this metric in its initial state will return a loss value of 0.</p>
<p>Creates an instance of the loss metric.</p>
<p>By default this metric in its initial state will return a loss
value of 0. The metric can be updated by using the <cite>update</cite> method
while the running loss can be retrieved using the <cite>result</cite> method.</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.Loss.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">patterns</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">task_label</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/loss/#Loss.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.Loss.update" title="Permalink to this definition"></a></dt>
<dd><p>Update the running loss given the loss Tensor and the minibatch size.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> – The loss Tensor. Different reduction types don’t affect
the result.</p></li>
<li><p><strong>patterns</strong> – The number of patterns in the minibatch.</p></li>
<li><p><strong>task_label</strong> – the task label associated to the current experience</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>None.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.Loss.result">
<span class="sig-name descname"><span class="pre">result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">task_label</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/loss/#Loss.result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.Loss.result" title="Permalink to this definition"></a></dt>
<dd><p>Retrieves the running average loss per pattern.</p>
<p>Calling this method will not change the internal state of the metric.
:param task_label: None to return metric values for all the task labels.</p>
<blockquote>
<div><p>If an int, return value only for that task label</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The running loss, as a float.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.Loss.reset">
<span class="sig-name descname"><span class="pre">reset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">task_label</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/loss/#Loss.reset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.Loss.reset" title="Permalink to this definition"></a></dt>
<dd><p>Resets the metric.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>task_label</strong> – None to reset all metric values. If an int,
reset metric value corresponding to that task label.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>None.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MinibatchLoss">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">MinibatchLoss</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/loss/#MinibatchLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MinibatchLoss" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.loss.LossPluginMetric</span></code></p>
<p>The minibatch loss metric.
This plugin metric only works at training time.</p>
<p>This metric computes the average loss over patterns
from a single minibatch.
It reports the result after each iteration.</p>
<p>If a more coarse-grained logging is needed, consider using
<a class="reference internal" href="#avalanche.evaluation.metrics.EpochLoss" title="avalanche.evaluation.metrics.EpochLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">EpochLoss</span></code></a> instead.</p>
<p>Creates an instance of the MinibatchLoss metric.</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MinibatchLoss.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/loss/#MinibatchLoss.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MinibatchLoss.__str__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.EpochLoss">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">EpochLoss</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/loss/#EpochLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.EpochLoss" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.loss.LossPluginMetric</span></code></p>
<p>The average loss over a single training epoch.
This plugin metric only works at training time.</p>
<p>The loss will be logged after each training epoch by computing
the loss on the predicted patterns during the epoch divided by
the overall number of patterns encountered in that epoch.</p>
<p>Creates an instance of the EpochLoss metric.</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.EpochLoss.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/loss/#EpochLoss.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.EpochLoss.__str__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.RunningEpochLoss">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">RunningEpochLoss</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/loss/#RunningEpochLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.RunningEpochLoss" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.loss.LossPluginMetric</span></code></p>
<p>The average loss across all minibatches up to the current
epoch iteration.
This plugin metric only works at training time.</p>
<p>At each iteration, this metric logs the loss averaged over all patterns
seen so far in the current epoch.
The metric resets its state after each training epoch.</p>
<p>Creates an instance of the RunningEpochLoss metric.</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.RunningEpochLoss.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/loss/#RunningEpochLoss.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.RunningEpochLoss.__str__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ExperienceLoss">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">ExperienceLoss</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/loss/#ExperienceLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ExperienceLoss" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.loss.LossPluginMetric</span></code></p>
<p>At the end of each experience, this metric reports
the average loss over all patterns seen in that experience.
This plugin metric only works at eval time.</p>
<p>Creates an instance of ExperienceLoss metric</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ExperienceLoss.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/loss/#ExperienceLoss.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ExperienceLoss.__str__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.StreamLoss">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">StreamLoss</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/loss/#StreamLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.StreamLoss" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.loss.LossPluginMetric</span></code></p>
<p>At the end of the entire stream of experiences, this metric reports the
average loss over all patterns seen in all experiences.
This plugin metric only works at eval time.</p>
<p>Creates an instance of StreamLoss metric</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.StreamLoss.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/loss/#StreamLoss.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.StreamLoss.__str__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.loss_metrics">
<span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">loss_metrics</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">minibatch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epoch_running</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">experience</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stream</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">PluginMetric</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/loss/#loss_metrics"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.loss_metrics" title="Permalink to this definition"></a></dt>
<dd><p>Helper method that can be used to obtain the desired set of
plugin metrics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>minibatch</strong> – If True, will return a metric able to log
the minibatch loss at training time.</p></li>
<li><p><strong>epoch</strong> – If True, will return a metric able to log
the epoch loss at training time.</p></li>
<li><p><strong>epoch_running</strong> – If True, will return a metric able to log
the running epoch loss at training time.</p></li>
<li><p><strong>experience</strong> – If True, will return a metric able to log
the loss on each evaluation experience.</p></li>
<li><p><strong>stream</strong> – If True, will return a metric able to log
the loss averaged over the entire evaluation stream of experiences.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of plugin metrics.</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MAC">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">MAC</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/mac/#MAC"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MAC" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Metric[int]</span></code></p>
<p>Standalone Multiply-and-accumulate metric. Provides a lower bound of the
computational cost of a model in a hardware-independent way by
computing the number of multiplications. Currently supports only
Linear or Conv2d modules. Other operations are ignored.</p>
<p>Creates an instance of the MAC metric.</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MAC.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dummy_input</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/mac/#MAC.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MAC.update" title="Permalink to this definition"></a></dt>
<dd><p>Computes the MAC metric.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – current model.</p></li>
<li><p><strong>dummy_input</strong> – A tensor of the correct size to feed as input
to model. It includes batch size</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>MAC metric.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MAC.result">
<span class="sig-name descname"><span class="pre">result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/mac/#MAC.result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MAC.result" title="Permalink to this definition"></a></dt>
<dd><p>Return the number of MAC operations as computed in the previous call
to the <cite>update</cite> method.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The number of MAC operations or None if <cite>update</cite> has not been
called yet.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MAC.reset">
<span class="sig-name descname"><span class="pre">reset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/mac/#MAC.reset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MAC.reset" title="Permalink to this definition"></a></dt>
<dd><p>Resets the metric internal state.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MAC.update_compute_cost">
<span class="sig-name descname"><span class="pre">update_compute_cost</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dummy_input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/mac/#MAC.update_compute_cost"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MAC.update_compute_cost" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MAC.is_recognized_module">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">is_recognized_module</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mod</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/mac/#MAC.is_recognized_module"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MAC.is_recognized_module" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MinibatchMAC">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">MinibatchMAC</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/mac/#MinibatchMAC"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MinibatchMAC" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.mac.MACPluginMetric</span></code></p>
<p>The minibatch MAC metric.
This plugin metric only works at training time.</p>
<p>This metric computes the MAC over 1 pattern
from a single minibatch.
It reports the result after each iteration.</p>
<p>If a more coarse-grained logging is needed, consider using
<a class="reference internal" href="#avalanche.evaluation.metrics.EpochMAC" title="avalanche.evaluation.metrics.EpochMAC"><code class="xref py py-class docutils literal notranslate"><span class="pre">EpochMAC</span></code></a> instead.</p>
<p>Creates an instance of the MinibatchMAC metric.</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MinibatchMAC.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/mac/#MinibatchMAC.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MinibatchMAC.__str__" title="Permalink to this definition"></a></dt>
<dd><p>Return str(self).</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.EpochMAC">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">EpochMAC</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/mac/#EpochMAC"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.EpochMAC" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.mac.MACPluginMetric</span></code></p>
<p>The MAC at the end of each epoch computed on a
single pattern.
This plugin metric only works at training time.</p>
<p>The MAC will be logged after each training epoch.</p>
<p>Creates an instance of the EpochMAC metric.</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.EpochMAC.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/mac/#EpochMAC.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.EpochMAC.__str__" title="Permalink to this definition"></a></dt>
<dd><p>Return str(self).</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ExperienceMAC">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">ExperienceMAC</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/mac/#ExperienceMAC"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ExperienceMAC" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.mac.MACPluginMetric</span></code></p>
<p>At the end of each experience, this metric reports the
MAC computed on a single pattern.
This plugin metric only works at eval time.</p>
<p>Creates an instance of ExperienceMAC metric</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ExperienceMAC.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/mac/#ExperienceMAC.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ExperienceMAC.__str__" title="Permalink to this definition"></a></dt>
<dd><p>Return str(self).</p>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MAC_metrics">
<span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">MAC_metrics</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">minibatch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">experience</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">PluginMetric</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/mac/#MAC_metrics"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MAC_metrics" title="Permalink to this definition"></a></dt>
<dd><p>Helper method that can be used to obtain the desired set of
plugin metrics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>minibatch</strong> – If True, will return a metric able to log
the MAC after each iteration at training time.</p></li>
<li><p><strong>epoch</strong> – If True, will return a metric able to log
the MAC after each epoch at training time.</p></li>
<li><p><strong>experience</strong> – If True, will return a metric able to log
the MAC after each eval experience.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of plugin metrics.</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MaxRAM">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">MaxRAM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">every</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/ram_usage/#MaxRAM"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MaxRAM" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Metric[float]</span></code></p>
<p>The standalone RAM usage metric.
Important: this metric approximates the real maximum RAM usage since
it sample at discrete amount of time the RAM values.</p>
<p>Instances of this metric keeps the maximum RAM usage detected.
The <cite>start_thread</cite> method starts the usage tracking.
The <cite>stop_thread</cite> method stops the tracking.</p>
<p>The result, obtained using the <cite>result</cite> method, is the usage in mega-bytes.</p>
<p>The reset method will bring the metric to its initial state. By default
this metric in its initial state will return an usage value of 0.</p>
<p>Creates an instance of the RAM usage metric.
:param every: seconds after which update the maximum RAM</p>
<blockquote>
<div><p>usage</p>
</div></blockquote>
<dl class="py attribute">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MaxRAM.stop_f">
<span class="sig-name descname"><span class="pre">stop_f</span></span><em class="property"> <span class="pre">=</span> <span class="pre">False</span></em><a class="headerlink" href="#avalanche.evaluation.metrics.MaxRAM.stop_f" title="Permalink to this definition"></a></dt>
<dd><p>Flag to stop the thread</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MaxRAM.max_usage">
<span class="sig-name descname"><span class="pre">max_usage</span></span><em class="property"> <span class="pre">=</span> <span class="pre">0</span></em><a class="headerlink" href="#avalanche.evaluation.metrics.MaxRAM.max_usage" title="Permalink to this definition"></a></dt>
<dd><p>Main metric result. Max RAM usage.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MaxRAM.thread">
<span class="sig-name descname"><span class="pre">thread</span></span><a class="headerlink" href="#avalanche.evaluation.metrics.MaxRAM.thread" title="Permalink to this definition"></a></dt>
<dd><p>Thread executing RAM monitoring code</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MaxRAM.result">
<span class="sig-name descname"><span class="pre">result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/ram_usage/#MaxRAM.result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MaxRAM.result" title="Permalink to this definition"></a></dt>
<dd><p>Retrieves the RAM usage.</p>
<p>Calling this method will not change the internal state of the metric.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The average RAM usage in bytes, as a float value.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MaxRAM.start_thread">
<span class="sig-name descname"><span class="pre">start_thread</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/ram_usage/#MaxRAM.start_thread"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MaxRAM.start_thread" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MaxRAM.stop_thread">
<span class="sig-name descname"><span class="pre">stop_thread</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/ram_usage/#MaxRAM.stop_thread"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MaxRAM.stop_thread" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MaxRAM.reset">
<span class="sig-name descname"><span class="pre">reset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/ram_usage/#MaxRAM.reset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MaxRAM.reset" title="Permalink to this definition"></a></dt>
<dd><p>Resets the metric.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MaxRAM.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/ram_usage/#MaxRAM.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MaxRAM.update" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MinibatchMaxRAM">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">MinibatchMaxRAM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">every</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/ram_usage/#MinibatchMaxRAM"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MinibatchMaxRAM" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.ram_usage.RAMPluginMetric</span></code></p>
<p>The Minibatch Max RAM metric.
This plugin metric only works at training time.</p>
<p>Creates an instance of the Minibatch Max RAM metric
:param every: seconds after which update the maximum RAM</p>
<blockquote>
<div><p>usage</p>
</div></blockquote>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MinibatchMaxRAM.before_training">
<span class="sig-name descname"><span class="pre">before_training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">BaseStrategy</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/ram_usage/#MinibatchMaxRAM.before_training"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MinibatchMaxRAM.before_training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MinibatchMaxRAM.after_training">
<span class="sig-name descname"><span class="pre">after_training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">BaseStrategy</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/ram_usage/#MinibatchMaxRAM.after_training"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MinibatchMaxRAM.after_training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MinibatchMaxRAM.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/ram_usage/#MinibatchMaxRAM.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MinibatchMaxRAM.__str__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.EpochMaxRAM">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">EpochMaxRAM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">every</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/ram_usage/#EpochMaxRAM"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.EpochMaxRAM" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.ram_usage.RAMPluginMetric</span></code></p>
<p>The Epoch Max RAM metric.
This plugin metric only works at training time.</p>
<p>Creates an instance of the epoch Max RAM metric.
:param every: seconds after which update the maximum RAM</p>
<blockquote>
<div><p>usage</p>
</div></blockquote>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.EpochMaxRAM.before_training">
<span class="sig-name descname"><span class="pre">before_training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">BaseStrategy</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/ram_usage/#EpochMaxRAM.before_training"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.EpochMaxRAM.before_training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.EpochMaxRAM.after_training">
<span class="sig-name descname"><span class="pre">after_training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">BaseStrategy</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/ram_usage/#EpochMaxRAM.after_training"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.EpochMaxRAM.after_training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.EpochMaxRAM.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/ram_usage/#EpochMaxRAM.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.EpochMaxRAM.__str__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ExperienceMaxRAM">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">ExperienceMaxRAM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">every</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/ram_usage/#ExperienceMaxRAM"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ExperienceMaxRAM" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.ram_usage.RAMPluginMetric</span></code></p>
<p>The Experience Max RAM metric.
This plugin metric only works at eval time.</p>
<p>Creates an instance of the Experience CPU usage metric.
:param every: seconds after which update the maximum RAM</p>
<blockquote>
<div><p>usage</p>
</div></blockquote>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ExperienceMaxRAM.before_eval">
<span class="sig-name descname"><span class="pre">before_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">BaseStrategy</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/ram_usage/#ExperienceMaxRAM.before_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ExperienceMaxRAM.before_eval" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ExperienceMaxRAM.after_eval">
<span class="sig-name descname"><span class="pre">after_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">BaseStrategy</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/ram_usage/#ExperienceMaxRAM.after_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ExperienceMaxRAM.after_eval" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ExperienceMaxRAM.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/ram_usage/#ExperienceMaxRAM.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ExperienceMaxRAM.__str__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.StreamMaxRAM">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">StreamMaxRAM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">every</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/ram_usage/#StreamMaxRAM"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.StreamMaxRAM" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.ram_usage.RAMPluginMetric</span></code></p>
<p>The Stream Max RAM metric.
This plugin metric only works at eval time.</p>
<p>Creates an instance of the Experience CPU usage metric.
:param every: seconds after which update the maximum RAM</p>
<blockquote>
<div><p>usage</p>
</div></blockquote>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.StreamMaxRAM.before_eval">
<span class="sig-name descname"><span class="pre">before_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/ram_usage/#StreamMaxRAM.before_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.StreamMaxRAM.before_eval" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.StreamMaxRAM.after_eval">
<span class="sig-name descname"><span class="pre">after_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">BaseStrategy</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">MetricResult</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/ram_usage/#StreamMaxRAM.after_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.StreamMaxRAM.after_eval" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.StreamMaxRAM.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/ram_usage/#StreamMaxRAM.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.StreamMaxRAM.__str__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ram_usage_metrics">
<span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">ram_usage_metrics</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">every</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">minibatch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">experience</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stream</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">PluginMetric</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/ram_usage/#ram_usage_metrics"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ram_usage_metrics" title="Permalink to this definition"></a></dt>
<dd><p>Helper method that can be used to obtain the desired set of
plugin metrics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>every</strong> – seconds after which update the maximum RAM
usage</p></li>
<li><p><strong>minibatch</strong> – If True, will return a metric able to log the minibatch
max RAM usage.</p></li>
<li><p><strong>epoch</strong> – If True, will return a metric able to log the epoch
max RAM usage.</p></li>
<li><p><strong>experience</strong> – If True, will return a metric able to log the experience
max RAM usage.</p></li>
<li><p><strong>stream</strong> – If True, will return a metric able to log the evaluation
max stream RAM usage.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of plugin metrics.</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ElapsedTime">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">ElapsedTime</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/timing/#ElapsedTime"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ElapsedTime" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Metric[float]</span></code></p>
<p>The standalone Elapsed Time metric.</p>
<p>Instances of this metric keep track of the time elapsed between calls to the
<cite>update</cite> method. The starting time is set when the <cite>update</cite> method is called
for the first time. That is, the starting time is <em>not</em> taken at the time
the constructor is invoked.</p>
<p>Calling the <cite>update</cite> method more than twice will update the metric to the
elapsed time between the first and the last call to <cite>update</cite>.</p>
<p>The result, obtained using the <cite>result</cite> method, is the time, in seconds,
computed as stated above.</p>
<p>The <cite>reset</cite> method will set the metric to its initial state, thus resetting
the initial time. This metric in its initial state (or if the <cite>update</cite>
method was invoked only once) will return an elapsed time of 0.</p>
<p>Creates an instance of the ElapsedTime metric.</p>
<p>This metric in its initial state (or if the <cite>update</cite> method was invoked
only once) will return an elapsed time of 0. The metric can be updated
by using the <cite>update</cite> method while the running accuracy can be retrieved
using the <cite>result</cite> method.</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ElapsedTime.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/timing/#ElapsedTime.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ElapsedTime.update" title="Permalink to this definition"></a></dt>
<dd><p>Update the elapsed time.</p>
<p>For more info on how to set the initial time see the class description.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ElapsedTime.result">
<span class="sig-name descname"><span class="pre">result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/timing/#ElapsedTime.result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ElapsedTime.result" title="Permalink to this definition"></a></dt>
<dd><p>Retrieves the elapsed time.</p>
<p>Calling this method will not change the internal state of the metric.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The elapsed time, in seconds, as a float value.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ElapsedTime.reset">
<span class="sig-name descname"><span class="pre">reset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/timing/#ElapsedTime.reset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ElapsedTime.reset" title="Permalink to this definition"></a></dt>
<dd><p>Resets the metric, including the initial time.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MinibatchTime">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">MinibatchTime</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/timing/#MinibatchTime"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MinibatchTime" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.timing.TimePluginMetric</span></code></p>
<p>The minibatch time metric.
This plugin metric only works at training time.</p>
<p>This metric “logs” the elapsed time for each iteration.</p>
<p>If a more coarse-grained logging is needed, consider using
<a class="reference internal" href="#avalanche.evaluation.metrics.EpochTime" title="avalanche.evaluation.metrics.EpochTime"><code class="xref py py-class docutils literal notranslate"><span class="pre">EpochTime</span></code></a>.</p>
<p>Creates an instance of the minibatch time metric.</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MinibatchTime.before_training_iteration">
<span class="sig-name descname"><span class="pre">before_training_iteration</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">MetricResult</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/timing/#MinibatchTime.before_training_iteration"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MinibatchTime.before_training_iteration" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.MinibatchTime.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/timing/#MinibatchTime.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.MinibatchTime.__str__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.EpochTime">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">EpochTime</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/timing/#EpochTime"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.EpochTime" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.timing.TimePluginMetric</span></code></p>
<p>The epoch elapsed time metric.
This plugin metric only works at training time.</p>
<p>The elapsed time will be logged after each epoch.</p>
<p>Creates an instance of the epoch time metric.</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.EpochTime.before_training_epoch">
<span class="sig-name descname"><span class="pre">before_training_epoch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/timing/#EpochTime.before_training_epoch"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.EpochTime.before_training_epoch" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.EpochTime.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/timing/#EpochTime.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.EpochTime.__str__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.RunningEpochTime">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">RunningEpochTime</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/timing/#RunningEpochTime"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.RunningEpochTime" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.timing.TimePluginMetric</span></code></p>
<p>The running epoch time metric.
This plugin metric only works at training time.</p>
<p>For each iteration, this metric logs the average time
between the start of the
epoch and the current iteration.</p>
<p>Creates an instance of the running epoch time metric..</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.RunningEpochTime.before_training_epoch">
<span class="sig-name descname"><span class="pre">before_training_epoch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/timing/#RunningEpochTime.before_training_epoch"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.RunningEpochTime.before_training_epoch" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.RunningEpochTime.after_training_iteration">
<span class="sig-name descname"><span class="pre">after_training_iteration</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">BaseStrategy</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">MetricResult</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/timing/#RunningEpochTime.after_training_iteration"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.RunningEpochTime.after_training_iteration" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.RunningEpochTime.result">
<span class="sig-name descname"><span class="pre">result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/timing/#RunningEpochTime.result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.RunningEpochTime.result" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.RunningEpochTime.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/timing/#RunningEpochTime.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.RunningEpochTime.__str__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ExperienceTime">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">ExperienceTime</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/timing/#ExperienceTime"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ExperienceTime" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.timing.TimePluginMetric</span></code></p>
<p>The experience time metric.
This plugin metric only works at eval time.</p>
<p>After each experience, this metric emits the average time of that
experience.</p>
<p>Creates an instance of the experience time metric.</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ExperienceTime.before_eval_exp">
<span class="sig-name descname"><span class="pre">before_eval_exp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">BaseStrategy</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/timing/#ExperienceTime.before_eval_exp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ExperienceTime.before_eval_exp" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.ExperienceTime.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/timing/#ExperienceTime.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.ExperienceTime.__str__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.StreamTime">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">StreamTime</span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/timing/#StreamTime"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.StreamTime" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.timing.TimePluginMetric</span></code></p>
<p>The stream time metric.
This metric only works at eval time.</p>
<p>After the entire evaluation stream,
this plugin metric emits the average time of that stream.</p>
<p>Creates an instance of the stream time metric.</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.StreamTime.before_eval">
<span class="sig-name descname"><span class="pre">before_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">BaseStrategy</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/timing/#StreamTime.before_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.StreamTime.before_eval" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.StreamTime.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/timing/#StreamTime.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.StreamTime.__str__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.timing_metrics">
<span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">timing_metrics</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">minibatch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epoch_running</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">experience</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stream</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">PluginMetric</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../../../_modules/avalanche/evaluation/metrics/timing/#timing_metrics"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.timing_metrics" title="Permalink to this definition"></a></dt>
<dd><p>Helper method that can be used to obtain the desired set of
plugin metrics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>minibatch</strong> – If True, will return a metric able to log the train
minibatch elapsed time.</p></li>
<li><p><strong>epoch</strong> – If True, will return a metric able to log the train epoch
elapsed time.</p></li>
<li><p><strong>epoch_running</strong> – If True, will return a metric able to log the running
train epoch elapsed time.</p></li>
<li><p><strong>experience</strong> – If True, will return a metric able to log the eval
experience elapsed time.</p></li>
<li><p><strong>stream</strong> – If True, will return a metric able to log the eval stream
elapsed time.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of plugin metrics.</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, ContinualAI.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>