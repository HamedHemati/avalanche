:mod:`avalanche.logging.text_logging`
=====================================

.. py:module:: avalanche.logging.text_logging


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   avalanche.logging.text_logging.TextLogger



.. data:: UNSUPPORTED_TYPES
   :annotation: :Tuple[Type]

   

.. py:class:: TextLogger(file=sys.stdout)

   Bases: :class:`avalanche.logging.StrategyLogger`

   The `TextLogger` class provides logging facilities
   printed to a user specified file. The logger writes
   metric results after each training epoch, evaluation
   experience and at the end of the entire evaluation stream.

   .. note::
       To avoid an excessive amount of printed lines,
       this logger will **not** print results after
       each iteration. If the user is monitoring
       metrics which emit results after each minibatch
       (e.g., `MinibatchAccuracy`), only the last recorded
       value of such metrics will be reported at the end
       of the epoch.

   .. note::
       Since this logger works on the standard output,
       metrics producing images or more complex visualizations
       will be converted to a textual format suitable for
       console printing. You may want to add more loggers
       to your `EvaluationPlugin` to better support
       different formats.

   Creates an instance of `TextLogger` class.

   :param file: destination file to which print metrics
       (default=sys.stdout).

   .. method:: log_single_metric(self, name, value, x_plot) -> None

      This abstract method will have to be implemented by each subclass.
      This method takes a metric name, a metric value and a x value and
      decides how to show the metric value.

      :param name: str, metric name
      :param value: the metric value, will be ignored if
          not supported by the logger
      :param x_plot: an integer representing the x value
          associated to the metric value


   .. method:: print_current_metrics(self)


   .. method:: before_training_exp(self, strategy: BaseStrategy, metric_values: List['MetricValue'], **kwargs)

      Called before `train_exp` by the `BaseStrategy`. 


   .. method:: before_eval_exp(self, strategy: BaseStrategy, metric_values: List['MetricValue'], **kwargs)

      Called before `eval_exp` by the `BaseStrategy`. 


   .. method:: after_training_epoch(self, strategy: BaseStrategy, metric_values: List['MetricValue'], **kwargs)

      Called after `train_epoch` by the `BaseStrategy`. 


   .. method:: after_eval_exp(self, strategy: BaseStrategy, metric_values: List['MetricValue'], **kwargs)

      Called after `eval_exp` by the `BaseStrategy`. 


   .. method:: before_training(self, strategy: BaseStrategy, metric_values: List['MetricValue'], **kwargs)

      Called before `train` by the `BaseStrategy`. 


   .. method:: before_eval(self, strategy: BaseStrategy, metric_values: List['MetricValue'], **kwargs)

      Called before `eval` by the `BaseStrategy`. 


   .. method:: after_training(self, strategy: BaseStrategy, metric_values: List['MetricValue'], **kwargs)

      Called after `train` by the `BaseStrategy`. 


   .. method:: after_eval(self, strategy: BaseStrategy, metric_values: List['MetricValue'], **kwargs)

      Called after `eval` by the `BaseStrategy`. 



