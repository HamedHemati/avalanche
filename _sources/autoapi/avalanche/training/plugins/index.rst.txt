:mod:`avalanche.training.plugins`
=================================

.. py:module:: avalanche.training.plugins


Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   agem/index.rst
   clock/index.rst
   cope/index.rst
   cwr_star/index.rst
   early_stopping/index.rst
   evaluation/index.rst
   ewc/index.rst
   gdumb/index.rst
   gem/index.rst
   gss_greedy/index.rst
   lfl/index.rst
   lr_scheduling/index.rst
   lwf/index.rst
   replay/index.rst
   strategy_plugin/index.rst
   synaptic_intelligence/index.rst


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   avalanche.training.plugins.AGEMPlugin
   avalanche.training.plugins.CWRStarPlugin
   avalanche.training.plugins.EvaluationPlugin
   avalanche.training.plugins.EWCPlugin
   avalanche.training.plugins.GDumbPlugin
   avalanche.training.plugins.GEMPlugin
   avalanche.training.plugins.LwFPlugin
   avalanche.training.plugins.ReplayPlugin
   avalanche.training.plugins.StrategyPlugin
   avalanche.training.plugins.SynapticIntelligencePlugin
   avalanche.training.plugins.GSS_greedyPlugin
   avalanche.training.plugins.CoPEPlugin
   avalanche.training.plugins.PPPloss
   avalanche.training.plugins.LFLPlugin
   avalanche.training.plugins.EarlyStoppingPlugin



.. py:class:: AGEMPlugin(patterns_per_experience: int, sample_size: int)

   Bases: :class:`avalanche.training.plugins.strategy_plugin.StrategyPlugin`

   Average Gradient Episodic Memory Plugin.
   AGEM projects the gradient on the current minibatch by using an external
   episodic memory of patterns from previous experiences. If the dot product
   between the current gradient and the (average) gradient of a randomly
   sampled set of memory examples is negative, the gradient is projected.
   This plugin does not use task identities.

   :param patterns_per_experience: number of patterns per experience in the
       memory.
   :param sample_size: number of patterns in memory sample when computing
       reference gradient.

   .. method:: before_training_iteration(self, strategy, **kwargs)

      Compute reference gradient on memory sample.


   .. method:: after_backward(self, strategy, **kwargs)

      Project gradient based on reference gradients


   .. method:: after_training_exp(self, strategy, **kwargs)

      Update replay memory with patterns from current experience. 


   .. method:: sample_from_memory(self)

      Sample a minibatch from memory.
      Return a tuple of patterns (tensor), targets (tensor).


   .. method:: update_memory(self, dataset)

      Update replay memory with patterns from current experience.



.. py:class:: CWRStarPlugin(model, cwr_layer_name=None, freeze_remaining_model=True)

   Bases: :class:`avalanche.training.plugins.strategy_plugin.StrategyPlugin`

   Base class for strategy plugins. Implements all the callbacks required
   by the BaseStrategy with an empty function. Subclasses must override
   the callbacks.

   CWR* Strategy.
   This plugin does not use task identities.

   :param model: the model.
   :param cwr_layer_name: name of the last fully connected layer. Defaults
       to None, which means that the plugin will attempt an automatic
       detection.
   :param freeze_remaining_model: If True, the plugin will freeze (set
       layers in eval mode and disable autograd for parameters) all the
       model except the cwr layer. Defaults to True.

   .. method:: after_training_exp(self, strategy, **kwargs)


   .. method:: before_training_exp(self, strategy, **kwargs)


   .. method:: consolidate_weights(self)

      Mean-shift for the target layer weights


   .. method:: set_consolidate_weights(self)

      set trained weights 


   .. method:: reset_weights(self, cur_clas)

      reset weights


   .. method:: get_cwr_layer(self) -> Optional[Linear]


   .. method:: freeze_other_layers(self)



.. py:class:: EvaluationPlugin(*metrics: Union['PluginMetric', Sequence['PluginMetric']], loggers: Union['StrategyLogger', Sequence['StrategyLogger']] = None, collect_all=True, benchmark=None, strict_checks=False, suppress_warnings=False)

   Bases: :class:`avalanche.training.plugins.strategy_plugin.StrategyPlugin`

   An evaluation plugin that obtains relevant data from the
   training and eval loops of the strategy through callbacks.
   The plugin keeps a dictionary with the last recorded value for each metric.
   The dictionary will be returned by the `train` and `eval` methods of the
   strategies.
   It is also possible to keep a dictionary with all recorded metrics by
   specifying `collect_all=True`. The dictionary can be retrieved via
   the `get_all_metrics` method.

   This plugin also logs metrics using the provided loggers.

   Creates an instance of the evaluation plugin.

   :param metrics: The metrics to compute.
   :param loggers: The loggers to be used to log the metric values.
   :param collect_all: if True, collect in a separate dictionary all
       metric curves values. This dictionary is accessible with
       `get_all_metrics` method.
   :param benchmark: continual learning benchmark needed to check stream
       completeness during evaluation or other kind of properties. If
       None, no check will be conducted and the plugin will emit a
       warning to signal this fact.
   :param strict_checks: if True, `benchmark` has to be provided.
       In this case, only full evaluation streams are admitted when
       calling `eval`. An error will be raised otherwise. When False,
       `benchmark` can be `None` and only warnings will be raised.
   :param suppress_warnings: if True, warnings and errors will never be
       raised from the plugin.
       If False, warnings and errors will be raised following
       `benchmark` and `strict_checks` behavior.

   .. method:: active(self)
      :property:


   .. method:: get_last_metrics(self)

      Return a shallow copy of dictionary with metric names
      as keys and last metrics value as values.

      :return: a dictionary with full metric
          names as keys and last metric value as value.


   .. method:: get_all_metrics(self)

      Return the dictionary of all collected metrics.
      This method should be called only when `collect_all` is set to True.

      :return: if `collect_all` is True, returns a dictionary
          with full metric names as keys and a tuple of two lists
          as value. The first list gathers x values (indices
          representing time steps at which the corresponding
          metric value has been emitted). The second list
          gathers metric values. a dictionary. If `collect_all`
          is False return an empty dictionary


   .. method:: reset_last_metrics(self)

      Set the dictionary storing last value for each metric to be
      empty dict.


   .. method:: before_training(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_training_exp(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_train_dataset_adaptation(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_train_dataset_adaptation(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_training_epoch(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_training_iteration(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_forward(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_forward(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_backward(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_backward(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_training_iteration(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_update(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_update(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_training_epoch(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_training_exp(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_training(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_eval(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_eval_dataset_adaptation(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_eval_dataset_adaptation(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_eval_exp(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_eval_exp(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_eval(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_eval_iteration(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_eval_forward(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_eval_forward(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_eval_iteration(self, strategy: BaseStrategy, **kwargs)



.. py:class:: EWCPlugin(ewc_lambda, mode='separate', decay_factor=None, keep_importance_data=False)

   Bases: :class:`avalanche.training.plugins.strategy_plugin.StrategyPlugin`

   Elastic Weight Consolidation (EWC) plugin.
   EWC computes importance of each weight at the end of training on current
   experience. During training on each minibatch, the loss is augmented
   with a penalty which keeps the value of the current weights close to the
   value they had on previous experiences in proportion to their importance
   on that experience. Importances are computed with an additional pass on the
   training set. This plugin does not use task identities.

   :param ewc_lambda: hyperparameter to weigh the penalty inside the total
          loss. The larger the lambda, the larger the regularization.
   :param mode: `separate` to keep a separate penalty for each previous
          experience.
          `online` to keep a single penalty summed with a decay factor
          over all previous tasks.
   :param decay_factor: used only if mode is `online`.
          It specifies the decay term of the importance matrix.
   :param keep_importance_data: if True, keep in memory both parameter
           values and importances for all previous task, for all modes.
           If False, keep only last parameter values and importances.
           If mode is `separate`, the value of `keep_importance_data` is
           set to be True.

   .. method:: before_backward(self, strategy, **kwargs)

      Compute EWC penalty and add it to the loss.


   .. method:: after_training_exp(self, strategy, **kwargs)

      Compute importances of parameters after each experience.


   .. method:: compute_importances(self, model, criterion, optimizer, dataset, device, batch_size)

      Compute EWC importance matrix for each parameter


   .. method:: update_importances(self, importances, t)

      Update importance for each parameter based on the currently computed
      importances.



.. py:class:: GDumbPlugin(mem_size: int = 200)

   Bases: :class:`avalanche.training.plugins.strategy_plugin.StrategyPlugin`

   GDumb plugin.

   At each experience the model is trained  from scratch using a buffer of
   samples collected from all the previous learning experiences.
   The buffer is updated at the start of each experience to add new classes or
   new examples of already encountered classes.
   In multitask scenarios, mem_size is the memory size for each task.
   This plugin can be combined with a Naive strategy to obtain the
   standard GDumb strategy.
   https://www.robots.ox.ac.uk/~tvg/publications/2020/gdumb.pdf

   .. method:: before_train_dataset_adaptation(self, strategy: BaseStrategy, **kwargs)

      Reset model. 


   .. method:: after_train_dataset_adaptation(self, strategy: BaseStrategy, **kwargs)



.. py:class:: GEMPlugin(patterns_per_experience: int, memory_strength: float)

   Bases: :class:`avalanche.training.plugins.strategy_plugin.StrategyPlugin`

   Gradient Episodic Memory Plugin.
   GEM projects the gradient on the current minibatch by using an external
   episodic memory of patterns from previous experiences. The gradient on
   the current minibatch is projected so that the dot product with all the
   reference gradients of previous tasks remains positive.
   This plugin does not use task identities.

   :param patterns_per_experience: number of patterns per experience in the
       memory.
   :param memory_strength: offset to add to the projection direction
       in order to favour backward transfer (gamma in original paper).

   .. method:: before_training_iteration(self, strategy, **kwargs)

      Compute gradient constraints on previous memory samples from all
      experiences.


   .. method:: after_backward(self, strategy, **kwargs)

      Project gradient based on reference gradients


   .. method:: after_training_exp(self, strategy, **kwargs)

      Save a copy of the model after each experience


   .. method:: update_memory(self, dataset, t, batch_size)

      Update replay memory with patterns from current experience.


   .. method:: solve_quadprog(self, g)

      Solve quadratic programming with current gradient g and
      gradients matrix on previous tasks G.
      Taken from original code:
      https://github.com/facebookresearch/GradientEpisodicMemory/blob/master/model/gem.py



.. py:class:: LwFPlugin(alpha=1, temperature=2)

   Bases: :class:`avalanche.training.plugins.strategy_plugin.StrategyPlugin`

   A Learning without Forgetting plugin.
   LwF uses distillation to regularize the current loss with soft targets
   taken from a previous version of the model.
   This plugin does not use task identities.

   :param alpha: distillation hyperparameter. It can be either a float
           number or a list containing alpha for each experience.
   :param temperature: softmax temperature for distillation

   .. attribute:: prev_classes
      

      In Avalanche, targets of different experiences are not ordered. 
      As a result, some units may be allocated even though their 
      corresponding class has never been seen by the model.
      Knowledge distillation uses only units corresponding to old classes. 


   .. method:: penalty(self, out, x, alpha, curr_model)

      Compute weighted distillation loss.


   .. method:: before_backward(self, strategy, **kwargs)

      Add distillation loss


   .. method:: after_training_exp(self, strategy, **kwargs)

      Save a copy of the model after each experience and
      update self.prev_classes to include the newly learned classes.



.. py:class:: ReplayPlugin(mem_size: int = 200, storage_policy: Optional['ExemplarsBuffer'] = None)

   Bases: :class:`avalanche.training.plugins.strategy_plugin.StrategyPlugin`

   Experience replay plugin.

   Handles an external memory filled with randomly selected
   patterns and implementing `before_training_exp` and `after_training_exp`
   callbacks. 
   The `before_training_exp` callback is implemented in order to use the
   dataloader that creates mini-batches with examples from both training
   data and external memory. The examples in the mini-batch is balanced 
   such that there are the same number of examples for each experience.    

   The `after_training_exp` callback is implemented in order to add new 
   patterns to the external memory.

   The :mem_size: attribute controls the total number of patterns to be stored 
   in the external memory.
   :param storage_policy: The policy that controls how to add new exemplars
                          in memory

   .. method:: ext_mem(self)
      :property:


   .. method:: before_training_exp(self, strategy: BaseStrategy, num_workers: int = 0, shuffle: bool = True, **kwargs)

      Dataloader to build batches containing examples from both memories and
      the training dataset


   .. method:: after_training_exp(self, strategy: BaseStrategy, **kwargs)



.. py:class:: StrategyPlugin

   Bases: :class:`StrategyCallbacks[Any]`

   Base class for strategy plugins. Implements all the callbacks required
   by the BaseStrategy with an empty function. Subclasses must override
   the callbacks.

   .. method:: before_training(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_training_exp(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_train_dataset_adaptation(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_train_dataset_adaptation(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_training_epoch(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_training_iteration(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_forward(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_forward(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_backward(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_backward(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_training_iteration(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_update(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_update(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_training_epoch(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_training_exp(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_training(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_eval(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_eval_dataset_adaptation(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_eval_dataset_adaptation(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_eval_exp(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_eval_exp(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_eval(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_eval_iteration(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_eval_forward(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_eval_forward(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_eval_iteration(self, strategy: BaseStrategy, **kwargs)



.. py:class:: SynapticIntelligencePlugin(si_lambda: Union[float, Sequence[float]], eps: float = 1e-07, excluded_parameters: Sequence['str'] = None, device: Any = 'as_strategy')

   Bases: :class:`avalanche.training.plugins.strategy_plugin.StrategyPlugin`

   The Synaptic Intelligence plugin.

   This is the Synaptic Intelligence PyTorch implementation of the
   algorithm described in the paper
   "Continuous Learning in Single-Incremental-Task Scenarios"
   (https://arxiv.org/abs/1806.08568)

   The original implementation has been proposed in the paper
   "Continual Learning Through Synaptic Intelligence"
   (https://arxiv.org/abs/1703.04200).

   This plugin can be attached to existing strategies to achieve a
   regularization effect.

   This plugin will require the strategy `loss` field to be set before the
   `before_backward` callback is invoked. The loss Tensor will be updated to
   achieve the S.I. regularization effect.

   Creates an instance of the Synaptic Intelligence plugin.

   :param si_lambda: Synaptic Intelligence lambda term.
       If list, one lambda for each experience. If the list has less
       elements than the number of experiences, last lambda will be
       used for the remaining experiences.
   :param eps: Synaptic Intelligence damping parameter.
   :param device: The device to use to run the S.I. experiences.
       Defaults to "as_strategy", which means that the `device` field of
       the strategy will be used. Using a different device may lead to a
       performance drop due to the required data transfer.

   .. attribute:: ewc_data
      :annotation: :EwcDataType

      The first dictionary contains the params at loss minimum while the 
      second one contains the parameter importance.


   .. method:: before_training_exp(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_backward(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_training_iteration(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_training_iteration(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_training_exp(self, strategy: BaseStrategy, **kwargs)


   .. method:: device(self, strategy: BaseStrategy)


   .. method:: create_syn_data(model: Module, ewc_data: EwcDataType, syn_data: SynDataType, excluded_parameters: Set[str])
      :staticmethod:


   .. method:: extract_weights(model: Module, target: ParamDict, excluded_parameters: Set[str])
      :staticmethod:


   .. method:: extract_grad(model, target: ParamDict, excluded_parameters: Set[str])
      :staticmethod:


   .. method:: init_batch(model, ewc_data: EwcDataType, syn_data: SynDataType, excluded_parameters: Set[str])
      :staticmethod:


   .. method:: pre_update(model, syn_data: SynDataType, excluded_parameters: Set[str])
      :staticmethod:


   .. method:: post_update(model, syn_data: SynDataType, excluded_parameters: Set[str])
      :staticmethod:


   .. method:: compute_ewc_loss(model, ewc_data: EwcDataType, excluded_parameters: Set[str], device, lambd=0.0)
      :staticmethod:


   .. method:: update_ewc_data(net, ewc_data: EwcDataType, syn_data: SynDataType, clip_to: float, excluded_parameters: Set[str], c=0.0015, eps: float = 1e-07)
      :staticmethod:


   .. method:: explode_excluded_parameters(excluded: Set[str]) -> Set[str]
      :staticmethod:

      Explodes a list of excluded parameters by adding a generic final ".*"
      wildcard at its end.

      :param excluded: The original set of excluded parameters.

      :return: The set of excluded parameters in which ".*" patterns have been
          added.


   .. method:: not_excluded_parameters(model: Module, excluded_parameters: Set[str]) -> List[Tuple[str, Tensor]]
      :staticmethod:


   .. method:: allowed_parameters(model: Module, excluded_parameters: Set[str]) -> List[Tuple[str, Tensor]]
      :staticmethod:



.. py:class:: GSS_greedyPlugin(mem_size=200, mem_strength=5, input_size=[])

   Bases: :class:`avalanche.training.plugins.strategy_plugin.StrategyPlugin`

   GSSPlugin replay plugin.

   Code adapted from the repository: 
   https://github.com/RaptorMai/online-continual-learning
   Handles an external memory fulled with samples selected 
   using the Greedy approach of GSS algorithm. 
   `before_forward` callback is used to process the current 
   sample and estimate a score.    

   The :mem_size: attribute controls the total number of patterns to be stored 
   in the external memory.

   .. method:: cosine_similarity(self, x1, x2=None, eps=1e-08)


   .. method:: get_grad_vector(self, pp, grad_dims)

      gather the gradients in one vector


   .. method:: get_batch_sim(self, strategy, grad_dims, batch_x, batch_y)

      Args:
          buffer: memory buffer
          grad_dims: gradient dimensions
          batch_x: current batch x
          batch_y: current batch y
      Returns: score of current batch, gradient from memory subsets


   .. method:: get_rand_mem_grads(self, strategy, grad_dims, gss_batch_size)

      Args:
          buffer: memory buffer
          grad_dims: gradient dimensions
      Returns: gradient from memory subsets


   .. method:: get_each_batch_sample_sim(self, strategy, grad_dims, mem_grads, batch_x, batch_y)

      Args:
          buffer: memory buffer
          grad_dims: gradient dimensions
          mem_grads: gradient from memory subsets
          batch_x: batch images
          batch_y: batch labels
      Returns: score of each sample from current batch


   .. method:: before_training_exp(self, strategy, num_workers=0, shuffle=True, **kwargs)

      Dataloader to build batches containing examples from both memories and
      the training dataset


   .. method:: after_forward(self, strategy, num_workers=0, shuffle=True, **kwargs)

      After every forward this function select sample to fill 
      the memory buffer based on cosine similarity



.. py:class:: CoPEPlugin(mem_size=200, n_classes=10, p_size=100, alpha=0.99, T=0.1, max_it_cnt=1)

   Bases: :class:`avalanche.training.plugins.strategy_plugin.StrategyPlugin`

   Continual Prototype Evolution plugin.
   Each class has a prototype for nearest-neighbor classification.
   The prototypes are updated continually with an exponentially moving average,
   using class-balanced replay to keep the prototypes up-to-date.
   The embedding space is optimized using the PseudoPrototypicalProxy-loss,
   exploiting both prototypes and batch information.

   This plugin doesn't use task identities in training or eval
   (data incremental) and is designed for online learning (1 epoch per task).

   :param mem_size: max number of input samples in the replay memory.
   :param n_classes: total number of classes that will be encountered. This
   is used to output predictions for all classes, with zero probability
   for unseen classes.
   :param p_size: The prototype size, which equals the feature size of the
   last layer.
   :param alpha: The momentum for the exponentially moving average of the
   prototypes.
   :param T: The softmax temperature, used as a concentration parameter.
   :param max_it_cnt: How many processing iterations per batch (experience)

   .. method:: before_training(self, strategy, **kwargs)

      Enforce using the PPP-loss and add a NN-classifier.


   .. method:: before_training_exp(self, strategy, num_workers=0, shuffle=True, **kwargs)

      Random retrieval from a class-balanced memory.
      Dataloader builds batches containing examples from both memories and
      the training dataset.
      This implementation requires the use of early stopping, otherwise the
      entire memory will be iterated.


   .. method:: after_training_iteration(self, strategy, **kwargs)

      Implements early stopping, determining how many subsequent times a
      batch can be used for updates. The dataloader contains only data for
      the current experience (batch) and the entire memory.
      Multiple iterations will hence result in the original batch with new
      exemplars sampled from the memory for each iteration.


   .. method:: after_forward(self, strategy, **kwargs)

      After the forward we can use the representations to update our running
      avg of the prototypes. This is in case we do multiple iterations of
      processing on the same batch.

      New prototypes are initialized for previously unseen classes.


   .. method:: after_training_exp(self, strategy, **kwargs)

      After the current experience (batch), update prototypes and
      store observed samples for replay.


   .. method:: after_eval_iteration(self, strategy, **kwargs)

      Convert output scores to probabilities for other metrics like
      accuracy and forgetting. We only do it at this point because before
      this,we still need the embedding outputs to obtain the PPP-loss.



.. py:class:: PPPloss(p_mem: Dict, T=0.1)

   Bases: :class:`object`

   Pseudo-Prototypical Proxy loss (PPP-loss).
   This is a contrastive loss using prototypes and representations of the
   samples in the batch to optimize the embedding space.

   :param p_mem: dictionary with keys the prototype identifier and
                 values the prototype tensors.
   :param T: temperature of the softmax, serving as concentration
             density parameter.

   .. method:: __call__(self, x, y)

      The loss is calculated with one-vs-rest batches Bc and Bk,
      split into the attractor and repellor loss terms.
      We iterate over the possible batches while accumulating the losses per
      class c vs other-classes k.


   .. method:: attractor(self, pc, pk, Bc)

      Get the attractor loss terms for all instances in xc.
      :param pc: Prototype of the same class c.
      :param pk: Prototoypes of the other classes.
      :param Bc: Batch of instances of the same class c.
      :return: Sum_{i, the part of same class c} log P(c|x_i^c)


   .. method:: repellor(self, pc, pk, Bc, Bk)

      Get the repellor loss terms for all pseudo-prototype instances in Bc.
      :param pc: Actual prototype of the same class c.
      :param pk: Prototoypes of the other classes (k).
      :param Bc: Batch of instances of the same class c. Acting as
      pseudo-prototypes.
      :param Bk: Batch of instances of other-than-c classes (k).
      :return: Sum_{i, part of same class c} Sum_{x_j^k} log 1 - P(c|x_j^k)



.. py:class:: LFLPlugin(lambda_e)

   Bases: :class:`avalanche.training.plugins.strategy_plugin.StrategyPlugin`

   Less-Forgetful Learning (LFL) Plugin
   LFL satisfies two properties to mitigate catastrophic forgetting.
   1) To keep the decision boundaries unchanged
   2) The feature space should not change much on target(new) data
   LFL uses euclidean loss between features from current and previous version
   of model as regularization to maintain the feature space and avoid
   catastrophic forgetting.
   Refer paper https://arxiv.org/pdf/1607.00122.pdf for more details
   This plugin does not use task identities.

   :param lambda_e: Euclidean loss hyper parameter

   .. method:: penalty(self, x, model, lambda_e)

      Compute weighted euclidean loss


   .. method:: compute_features(self, model, x)

      Compute features from prev model and current model


   .. method:: before_backward(self, strategy, **kwargs)

      Add euclidean loss between prev and current features as penalty


   .. method:: after_training_exp(self, strategy, **kwargs)

      Save a copy of the model after each experience
      and freeze the prev model and freeze the last layer of current model


   .. method:: before_training(self, strategy, **kwargs)

      Check if the model is an instance of base class to ensure get_features()
      is implemented



.. py:class:: EarlyStoppingPlugin(patience: int, val_stream_name: str, metric_name: str = 'Top1_Acc_Stream', mode: str = 'max')

   Bases: :class:`avalanche.training.plugins.StrategyPlugin`

   Base class for strategy plugins. Implements all the callbacks required
   by the BaseStrategy with an empty function. Subclasses must override
   the callbacks.

   Simple plugin stopping the training when the accuracy on the
   corresponding validation metric stopped progressing for a few epochs.
   The state of the best model is saved after each improvement on the
   given metric and is loaded back into the model before stopping the
   training procedure.

   :param patience: Number of epochs to wait before stopping the training.
   :param val_stream_name: Name of the validation stream to search in the
   metrics. The corresponding stream will be used to keep track of the
   evolution of the performance of a model.
   :param metric_name: The name of the metric to watch as it will be
   reported in the evaluator.
   :param mode: Must be "max" or "min". max (resp. min) means that the
   given metric should me maximized (resp. minimized).

   .. method:: before_training(self, strategy, **kwargs)


   .. method:: before_training_epoch(self, strategy, **kwargs)



