:mod:`avalanche.evaluation.metrics`
===================================

.. py:module:: avalanche.evaluation.metrics

.. autoapi-nested-parse::

   The :py:mod:`metrics` module provides a set of already
   implemented metrics, ready to be used both standalone
   and together with the `EvaluationPlugin`.
   To use a standalone metric, please use the class which
   inherits from `Metric` and manually call the appropriate
   `update`, `reset` and 'result` method.
   To automatically monitor metrics during training and evaluation
   flows, specific classes which inherit from `PluginMetric` or
   `GenericPluginMetric` are provided. Most of these metrics should
   be instantiated by the user by leveraging
   the related helper function. Such functions create an instance of
   a specific metric (e.g. accuracy) and monitors it on multiple callbacks
   (after each epoch, minibatch experience or stream).
   For example, to print accuracy metrics at the
   end of each training epoch and at the end of each evaluation experience,
   it is only required to call `accuracy_metrics(epoch=True, experience=True)`
   when creating the `EvaluationPlugin`.

   When available, please always use helper functions to specify
   the metrics to be monitored.

   The following table describes all the metrics available in Avalanche.

   =========================  ===================================================
   Metric Name                Description
   =========================  ===================================================
   Top1_Acc                   The accuracy metric for single-label classification
   Loss                       The specific loss is provided by
                              the user when creating the strategy.
   Forgetting                 The difference between
                              the training performance and the evaluation
                              performance after training on future experiences.
   Backward Transfer          The negative forgetting. That is, the difference
                              between the last evaluation performance and the
                              first training performance.
   Confusion Matrix           A representation of
                              false/true positive/negatives for classification
   Multiply and Accumulate    a.k.a. MAC. Estimates the computational cost
                              of the model forward pass on a single pattern.
                              Cost is estimated in terms of multiplications
                              operations.
   Timing                     Time elapsed between different moments of the
                              execution
   CPU Usage                  The average CPU consumption between different
                              moments of the execution.
   RAM Usage                  The maximum RAM occupancy, as retrieved by
                              sampling its value at fixed intervals during
                              execution.
   GPU Usage                  The maximum GPU occuapncy, as retrieved by
                              sampling its value at fixed intervals during
                              execution
   Disk Usage                 The size in KB of the disk occupancy for a set
                              of file system paths.
   =========================  ===================================================

   The following table provides a brief description of when each metric can be
   computed.
   `Stream` specifies on which stream (training or evaluation) that metric
   is computed.
   Please, refer to the helper function of each metric to check which levels
   are available for that metric.

   =================  =================================================== ========
   Level              Description                                         Stream
   =================  =================================================== ========
   MB (minibatch)     Metric emitted at the end of each training          Train
                      iteration. Its value is averaged across all
                      patterns in that minibatch. Metric is reset at the
                      beginning of each training iteration.
   Epoch              Metric emitted at the end of each training epoch.   Train
                      Its value is averaged across all patterns in
                      that epoch. Metric is reset at the beginning of
                      each training epoch.
   RunningEpoch       Metric emitted at the end of each training          Train
                      iteration. Its value is the average across all
                      patterns seen since the beginning of the epoch.
                      Metric is reset at the beginning of each
                      training epoch.
   Experience         Metric emitted at the end of each evaluation        Eval
                      experience. Its value is averaged across all
                      patterns in that experience.
                      Metric is reset at the beginning of each
                      evaluation experience.
   Stream             Metric emitted at the end of each evaluation        Eval
                      stream. Its value is averaged across all patterns
                      in that stream. Metric is reset at the beginning
                      of each evaluation stream.
   =================  =================================================== ========



Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   accuracy/index.rst
   checkpoint/index.rst
   confusion_matrix/index.rst
   cpu_usage/index.rst
   disk_usage/index.rst
   forgetting_bwt/index.rst
   forward_transfer/index.rst
   gpu_usage/index.rst
   images_samples/index.rst
   labels_repartition/index.rst
   loss/index.rst
   mac/index.rst
   mean/index.rst
   mean_scores/index.rst
   ram_usage/index.rst
   timing/index.rst


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   avalanche.evaluation.metrics.Mean
   avalanche.evaluation.metrics.Sum
   avalanche.evaluation.metrics.Accuracy
   avalanche.evaluation.metrics.MinibatchAccuracy
   avalanche.evaluation.metrics.EpochAccuracy
   avalanche.evaluation.metrics.RunningEpochAccuracy
   avalanche.evaluation.metrics.ExperienceAccuracy
   avalanche.evaluation.metrics.StreamAccuracy
   avalanche.evaluation.metrics.TrainedExperienceAccuracy
   avalanche.evaluation.metrics.WeightCheckpoint
   avalanche.evaluation.metrics.ConfusionMatrix
   avalanche.evaluation.metrics.StreamConfusionMatrix
   avalanche.evaluation.metrics.WandBStreamConfusionMatrix
   avalanche.evaluation.metrics.CPUUsage
   avalanche.evaluation.metrics.MinibatchCPUUsage
   avalanche.evaluation.metrics.EpochCPUUsage
   avalanche.evaluation.metrics.RunningEpochCPUUsage
   avalanche.evaluation.metrics.ExperienceCPUUsage
   avalanche.evaluation.metrics.StreamCPUUsage
   avalanche.evaluation.metrics.DiskUsage
   avalanche.evaluation.metrics.MinibatchDiskUsage
   avalanche.evaluation.metrics.EpochDiskUsage
   avalanche.evaluation.metrics.ExperienceDiskUsage
   avalanche.evaluation.metrics.StreamDiskUsage
   avalanche.evaluation.metrics.Forgetting
   avalanche.evaluation.metrics.GenericExperienceForgetting
   avalanche.evaluation.metrics.GenericStreamForgetting
   avalanche.evaluation.metrics.ExperienceForgetting
   avalanche.evaluation.metrics.StreamForgetting
   avalanche.evaluation.metrics.BWT
   avalanche.evaluation.metrics.ExperienceBWT
   avalanche.evaluation.metrics.StreamBWT
   avalanche.evaluation.metrics.ForwardTransfer
   avalanche.evaluation.metrics.GenericExperienceForwardTransfer
   avalanche.evaluation.metrics.ExperienceForwardTransfer
   avalanche.evaluation.metrics.GenericStreamForwardTransfer
   avalanche.evaluation.metrics.StreamForwardTransfer
   avalanche.evaluation.metrics.MaxGPU
   avalanche.evaluation.metrics.MinibatchMaxGPU
   avalanche.evaluation.metrics.EpochMaxGPU
   avalanche.evaluation.metrics.ExperienceMaxGPU
   avalanche.evaluation.metrics.StreamMaxGPU
   avalanche.evaluation.metrics.Loss
   avalanche.evaluation.metrics.MinibatchLoss
   avalanche.evaluation.metrics.EpochLoss
   avalanche.evaluation.metrics.RunningEpochLoss
   avalanche.evaluation.metrics.ExperienceLoss
   avalanche.evaluation.metrics.StreamLoss
   avalanche.evaluation.metrics.MAC
   avalanche.evaluation.metrics.MinibatchMAC
   avalanche.evaluation.metrics.EpochMAC
   avalanche.evaluation.metrics.ExperienceMAC
   avalanche.evaluation.metrics.MaxRAM
   avalanche.evaluation.metrics.MinibatchMaxRAM
   avalanche.evaluation.metrics.EpochMaxRAM
   avalanche.evaluation.metrics.ExperienceMaxRAM
   avalanche.evaluation.metrics.StreamMaxRAM
   avalanche.evaluation.metrics.ElapsedTime
   avalanche.evaluation.metrics.MinibatchTime
   avalanche.evaluation.metrics.EpochTime
   avalanche.evaluation.metrics.RunningEpochTime
   avalanche.evaluation.metrics.ExperienceTime
   avalanche.evaluation.metrics.StreamTime



Functions
~~~~~~~~~

.. autoapisummary::

   avalanche.evaluation.metrics.accuracy_metrics
   avalanche.evaluation.metrics.confusion_matrix_metrics
   avalanche.evaluation.metrics.cpu_usage_metrics
   avalanche.evaluation.metrics.disk_usage_metrics
   avalanche.evaluation.metrics.forgetting_metrics
   avalanche.evaluation.metrics.bwt_metrics
   avalanche.evaluation.metrics.forward_transfer_metrics
   avalanche.evaluation.metrics.gpu_usage_metrics
   avalanche.evaluation.metrics.loss_metrics
   avalanche.evaluation.metrics.MAC_metrics
   avalanche.evaluation.metrics.ram_usage_metrics
   avalanche.evaluation.metrics.timing_metrics


.. py:class:: Mean

   Bases: :class:`Metric[float]`

   The standalone mean metric.

   This utility metric is a general purpose metric that can be used to keep
   track of the mean of a sequence of values.

   Creates an instance of the mean metric.

   This metric in its initial state will return a mean value of 0.
   The metric can be updated by using the `update` method while the mean
   can be retrieved using the `result` method.

   .. method:: update(self, value: SupportsFloat, weight: SupportsFloat = 1.0) -> None

      Update the running mean given the value.

      The value can be weighted with a custom value, defined by the `weight`
      parameter.

      :param value: The value to be used to update the mean.
      :param weight: The weight of the value. Defaults to 1.
      :return: None.


   .. method:: result(self) -> float

      Retrieves the mean.

      Calling this method will not change the internal state of the metric.

      :return: The mean, as a float.


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.


   .. method:: __add__(self, other: Mean) -> 'Mean'

      Return a metric representing the weighted mean of the 2 means.

      :param other: the other mean
      :return: The weighted mean



.. py:class:: Sum

   Bases: :class:`Metric[float]`

   The standalone sum metric.

   This utility metric is a general purpose metric that can be used to keep
   track of the sum of a sequence of values.

   Beware that this metric only supports summing numbers and the result is
   always a float value, even when `update` is called by passing `int`s only.

   Creates an instance of the sum metric.

   This metric in its initial state will return a sum value of 0.
   The metric can be updated by using the `update` method while the sum
   can be retrieved using the `result` method.

   .. method:: update(self, value: SupportsFloat) -> None

      Update the running sum given the value.

      :param value: The value to be used to update the sum.
      :return: None.


   .. method:: result(self) -> float

      Retrieves the sum.

      Calling this method will not change the internal state of the metric.

      :return: The sum, as a float.


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.



.. py:class:: Accuracy

   Bases: :class:`Metric[float]`

   The Accuracy metric. This is a standalone metric.

   The metric keeps a dictionary of <task_label, accuracy value> pairs.
   and update the values through a running average over multiple
   <prediction, target> pairs of Tensors, provided incrementally.
   The "prediction" and "target" tensors may contain plain labels or
   one-hot/logit vectors.

   Each time `result` is called, this metric emits the average accuracy
   across all predictions made since the last `reset`.

   The reset method will bring the metric to its initial state. By default
   this metric in its initial state will return an accuracy value of 0.

   Creates an instance of the standalone Accuracy metric.

   By default this metric in its initial state will return an accuracy
   value of 0. The metric can be updated by using the `update` method
   while the running accuracy can be retrieved using the `result` method.

   .. method:: update(self, predicted_y: Tensor, true_y: Tensor, task_labels: Union[float, Tensor]) -> None

      Update the running accuracy given the true and predicted labels.
      Parameter `task_labels` is used to decide how to update the inner
      dictionary: if Float, only the dictionary value related to that task
      is updated. If Tensor, all the dictionary elements belonging to the
      task labels will be updated.

      :param predicted_y: The model prediction. Both labels and logit vectors
          are supported.
      :param true_y: The ground truth. Both labels and one-hot vectors
          are supported.
      :param task_labels: the int task label associated to the current
          experience or the task labels vector showing the task label
          for each pattern.

      :return: None.


   .. method:: result(self, task_label=None) -> Dict[int, float]

      Retrieves the running accuracy.

      Calling this method will not change the internal state of the metric.

      :param task_label: if None, return the entire dictionary of accuracies
          for each task. Otherwise return the dictionary
          `{task_label: accuracy}`.
      :return: A dict of running accuracies for each task label,
          where each value is a float value between 0 and 1.


   .. method:: reset(self, task_label=None) -> None

      Resets the metric.
      :param task_label: if None, reset the entire dictionary.
          Otherwise, reset the value associated to `task_label`.

      :return: None.



.. py:class:: MinibatchAccuracy

   Bases: :class:`avalanche.evaluation.metrics.accuracy.AccuracyPluginMetric`

   The minibatch plugin accuracy metric.
   This metric only works at training time.

   This metric computes the average accuracy over patterns
   from a single minibatch.
   It reports the result after each iteration.

   If a more coarse-grained logging is needed, consider using
   :class:`EpochAccuracy` instead.

   Creates an instance of the MinibatchAccuracy metric.

   .. method:: __str__(self)



.. py:class:: EpochAccuracy

   Bases: :class:`avalanche.evaluation.metrics.accuracy.AccuracyPluginMetric`

   The average accuracy over a single training epoch.
   This plugin metric only works at training time.

   The accuracy will be logged after each training epoch by computing
   the number of correctly predicted patterns during the epoch divided by
   the overall number of patterns encountered in that epoch.

   Creates an instance of the EpochAccuracy metric.

   .. method:: __str__(self)



.. py:class:: RunningEpochAccuracy

   Bases: :class:`avalanche.evaluation.metrics.accuracy.AccuracyPluginMetric`

   The average accuracy across all minibatches up to the current
   epoch iteration.
   This plugin metric only works at training time.

   At each iteration, this metric logs the accuracy averaged over all patterns
   seen so far in the current epoch.
   The metric resets its state after each training epoch.

   Creates an instance of the RunningEpochAccuracy metric.

   .. method:: __str__(self)



.. py:class:: ExperienceAccuracy

   Bases: :class:`avalanche.evaluation.metrics.accuracy.AccuracyPluginMetric`

   At the end of each experience, this plugin metric reports
   the average accuracy over all patterns seen in that experience.
   This metric only works at eval time.

   Creates an instance of ExperienceAccuracy metric

   .. method:: __str__(self)



.. py:class:: StreamAccuracy

   Bases: :class:`avalanche.evaluation.metrics.accuracy.AccuracyPluginMetric`

   At the end of the entire stream of experiences, this plugin metric
   reports the average accuracy over all patterns seen in all experiences.
   This metric only works at eval time.

   Creates an instance of StreamAccuracy metric

   .. method:: __str__(self)



.. py:class:: TrainedExperienceAccuracy

   Bases: :class:`avalanche.evaluation.metrics.accuracy.AccuracyPluginMetric`

   At the end of each experience, this plugin metric reports the average
   accuracy for only the experiences that the model has been trained on so far.

   This metric only works at eval time.

   Creates an instance of TrainedExperienceAccuracy metric by first 
   constructing AccuracyPluginMetric

   .. method:: after_training_exp(self, strategy) -> None


   .. method:: update(self, strategy)

      Only update the accuracy with results from experiences that have been 
      trained on


   .. method:: __str__(self)



.. function:: accuracy_metrics(*, minibatch=False, epoch=False, epoch_running=False, experience=False, stream=False, trained_experience=False) -> List[PluginMetric]

   Helper method that can be used to obtain the desired set of
   plugin metrics.

   :param minibatch: If True, will return a metric able to log
       the minibatch accuracy at training time.
   :param epoch: If True, will return a metric able to log
       the epoch accuracy at training time.
   :param epoch_running: If True, will return a metric able to log
       the running epoch accuracy at training time.
   :param experience: If True, will return a metric able to log
       the accuracy on each evaluation experience.
   :param stream: If True, will return a metric able to log
       the accuracy averaged over the entire evaluation stream of experiences.
   :param trained_experience: If True, will return a metric able to log
       the average evaluation accuracy only for experiences that the
       model has been trained on         

   :return: A list of plugin metrics.


.. py:class:: WeightCheckpoint

   Bases: :class:`PluginMetric[Tensor]`

   The WeightCheckpoint Metric. This is a standalone metric.

   Instances of this metric keeps the weight checkpoint tensor of the
   model at each experience. 

   Each time `result` is called, this metric emits the latest experience's
   weight checkpoint tensor since the last `reset`.

   The reset method will bring the metric to its initial state. By default
   this metric in its initial state will return None.

   Creates an instance of the WeightCheckpoint Metric.

   By default this metric in its initial state will return None.
   The metric can be updated by using the `update` method
   while the current experience's weight checkpoint tensor can be 
   retrieved using the `result` method.

   .. method:: update(self, weights) -> Tensor

      Update the weight checkpoint at the current experience.

      :param weights: the weight tensor at current experience
      :return: None.


   .. method:: result(self) -> Tensor

      Retrieves the weight checkpoint at the current experience.

      :return: The weight checkpoint as a tensor.


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.


   .. method:: after_eval_exp(self, strategy: BaseStrategy) -> 'MetricResult'

      Called after `eval_exp` by the `BaseStrategy`. 


   .. method:: __str__(self)

      Return str(self).



.. py:class:: ConfusionMatrix(num_classes: int = None, normalize: Literal['true', 'pred', 'all'] = None)

   Bases: :class:`Metric[Tensor]`

   The standalone confusion matrix metric.

   Instances of this metric keep track of the confusion matrix by receiving a
   pair of "ground truth" and "prediction" Tensors describing the labels of a
   minibatch. Those two tensors can both contain plain labels or
   one-hot/logit vectors.

   The result is the unnormalized running confusion matrix.

   Beware that by default the confusion matrix size will depend on the value of
   the maximum label as detected by looking at both the ground truth and
   predictions Tensors. When passing one-hot/logit vectors, this
   metric will try to infer the number of classes from the vector sizes.
   Otherwise, the maximum label value encountered in the truth/prediction
   Tensors will be used.

   If the user sets the `num_classes`, then the confusion matrix will always be
   of size `num_classes, num_classes`. Whenever a prediction or label tensor is
   provided as logits, only the first `num_classes` units will be considered in
   the confusion matrix computation. If they are provided as numerical labels,
   each of them has to be smaller than `num_classes`.

   The reset method will bring the metric to its initial state. By default
   this metric in its initial state will return an empty Tensor.

   Creates an instance of the standalone confusion matrix metric.

   By default this metric in its initial state will return an empty Tensor.
   The metric can be updated by using the `update` method while the running
   confusion matrix can be retrieved using the `result` method.

   :param num_classes: The number of classes. Defaults to None,
       which means that the number of classes will be inferred from
       ground truth and prediction Tensors (see class description for more
       details). If not None, the confusion matrix will always be of size
       `num_classes, num_classes` and only the first `num_classes` values
       of output logits or target logits will be considered in the update.
       If the output or targets are provided as numerical labels,
       there can be no label greater than `num_classes`.
   :param normalize: how to normalize confusion matrix.
       None to not normalize

   .. method:: update(self, true_y: Tensor, predicted_y: Tensor) -> None

      Update the running confusion matrix given the true and predicted labels.

      :param true_y: The ground truth. Both labels and one-hot vectors
          are supported.
      :param predicted_y: The ground truth. Both labels and logit vectors
          are supported.
      :return: None.


   .. method:: result(self) -> Tensor

      Retrieves the unnormalized confusion matrix.

      Calling this method will not change the internal state of the metric.

      :return: The running confusion matrix, as a Tensor.


   .. method:: reset(self) -> None

      Resets the metric.

      Calling this method will *not* reset the default number of classes
      optionally defined in the constructor optional parameter.

      :return: None.


   .. method:: nan_to_num(matrix: Tensor) -> Tensor
      :staticmethod:



.. py:class:: StreamConfusionMatrix(num_classes: Union[int, Mapping[int, int]] = None, normalize: Literal['true', 'pred', 'all'] = None, save_image: bool = True, image_creator: Callable[[Tensor, Sequence], Image] = default_cm_image_creator, absolute_class_order: bool = False)

   Bases: :class:`PluginMetric[Tensor]`

   The Stream Confusion Matrix metric.
   This plugin metric only works on the eval phase.

   Confusion Matrix computation can be slow if you compute it for a large
   number of classes. We recommend to set `save_image=False` if the runtime
   is too large.

   At the end of the eval phase, this metric logs the confusion matrix
   relative to all the patterns seen during eval.

   The metric can log either a Tensor or a PIL Image representing the
   confusion matrix.

   Creates an instance of the Stream Confusion Matrix metric.

   We recommend to set `save_image=False` if the runtime is too large.
   In fact, a large number of classes may increase the computation time
   of this metric.

   :param num_classes: The number of classes. Defaults to None,
       which means that the number of classes will be inferred from
       ground truth and prediction Tensors (see class description for more
       details). If not None, the confusion matrix will always be of size
       `num_classes, num_classes` and only the first `num_classes` values
       of output logits or target logits will be considered in the update.
       If the output or targets are provided as numerical labels,
       there can be no label greater than `num_classes`.
   :param normalize: Normalizes confusion matrix over the true (rows),
       predicted (columns) conditions or all the population. If None,
       confusion matrix will not be normalized. Valid values are: 'true',
       'pred' and 'all' or None.
   :param save_image: If True, a graphical representation of the confusion
       matrix will be logged, too. If False, only the Tensor representation
       will be logged. Defaults to True.
   :param image_creator: A callable that, given the tensor representation
       of the confusion matrix and the corresponding labels, returns a
       graphical representation of the matrix as a PIL Image. Defaults to
       `default_cm_image_creator`.
   :param absolute_class_order: If true, the labels in the created image
       will be sorted by id, otherwise they will be sorted by order of
       encounter at training time. This parameter is ignored if
       `save_image` is False, or the scenario is not a NCScenario.

   .. method:: reset(self) -> None


   .. method:: result(self) -> Tensor


   .. method:: update(self, true_y: Tensor, predicted_y: Tensor) -> None


   .. method:: before_eval(self, strategy) -> None

      Called before `eval` by the `BaseStrategy`. 


   .. method:: after_eval_iteration(self, strategy: BaseStrategy) -> None

      Called after the end of an iteration by the
      `BaseStrategy`. 


   .. method:: after_eval(self, strategy: BaseStrategy) -> MetricResult

      Called after `eval` by the `BaseStrategy`. 


   .. method:: __str__(self)

      Return str(self).



.. py:class:: WandBStreamConfusionMatrix(class_names=None)

   Bases: :class:`avalanche.evaluation.PluginMetric`

   Confusion Matrix metric compatible with Weights and Biases logger.
   Differently from the `StreamConfusionMatrix`, this metric will use W&B
   built-in functionalities to log the Confusion Matrix.

   This metric may not produce meaningful outputs with other loggers.

   https://docs.wandb.ai/guides/track/log#custom-charts

   :param class_names: list of names for the classes.
       E.g. ["cat", "dog"] if class 0 == "cat" and class 1 == "dog"
       If None, no class names will be used. Default None.

   .. method:: reset(self) -> None

      Resets the metric internal state.

      :return: None.


   .. method:: before_eval(self, strategy) -> None

      Called before `eval` by the `BaseStrategy`. 


   .. method:: result(self)

      Obtains the value of the metric.

      :return: The value of the metric.


   .. method:: update(self, output, target)


   .. method:: after_eval_iteration(self, strategy: BaseStrategy)

      Called after the end of an iteration by the
      `BaseStrategy`. 


   .. method:: after_eval(self, strategy: BaseStrategy) -> MetricResult

      Called after `eval` by the `BaseStrategy`. 


   .. method:: __str__(self)

      Return str(self).



.. function:: confusion_matrix_metrics(num_classes=None, normalize=None, save_image=True, image_creator=default_cm_image_creator, class_names=None, stream=False, wandb=False, absolute_class_order: bool = False) -> List[PluginMetric]

   Helper method that can be used to obtain the desired set of
   plugin metrics.

   :param num_classes: The number of classes. Defaults to None,
       which means that the number of classes will be inferred from
       ground truth and prediction Tensors (see class description for more
       details). If not None, the confusion matrix will always be of size
       `num_classes, num_classes` and only the first `num_classes` values
       of output logits or target logits will be considered in the update.
       If the output or targets are provided as numerical labels,
       there can be no label greater than `num_classes`.
   :param normalize: Normalizes confusion matrix over the true (rows),
       predicted (columns) conditions or all the population. If None,
       confusion matrix will not be normalized. Valid values are: 'true',
       'pred' and 'all' or None.
   :param save_image: If True, a graphical representation of the confusion
       matrix will be logged, too. If False, only the Tensor representation
       will be logged. Defaults to True.
   :param image_creator: A callable that, given the tensor representation
       of the confusion matrix, returns a graphical representation of the
       matrix as a PIL Image. Defaults to `default_cm_image_creator`.
   :param class_names: W&B only. List of names for the classes.
       E.g. ["cat", "dog"] if class 0 == "cat" and class 1 == "dog"
       If None, no class names will be used. Default None.
   :param stream: If True, will return a metric able to log
       the confusion matrix averaged over the entire evaluation stream
       of experiences.
   :param wandb: if True, will return a Weights and Biases confusion matrix
       together with all the other confusion matrixes requested.
   :param absolute_class_order: Not W&B. If true, the labels in the created
       image will be sorted by id, otherwise they will be sorted by order of
       encounter at training time. This parameter is ignored if `save_image` is
        False, or the scenario is not a NCScenario.

   :return: A list of plugin metrics.


.. py:class:: CPUUsage

   Bases: :class:`Metric[float]`

   The standalone CPU usage metric.

   Instances of this metric compute the average CPU usage as a float value.
   The metric starts tracking the CPU usage when the `update` method is called
   for the first time. That is, the tracking does not start at the time the
   constructor is invoked.

   Calling the `update` method more than twice will update the metric to the
   average usage between the first and the last call to `update`.

   The result, obtained using the `result` method, is the usage computed
   as stated above.

   The reset method will bring the metric to its initial state. By default
   this metric in its initial state will return an usage value of 0.

   Creates an instance of the standalone CPU usage metric.

   By default this metric in its initial state will return a CPU usage
   value of 0. The metric can be updated by using the `update` method
   while the average CPU usage can be retrieved using the `result` method.

   .. method:: update(self) -> None

      Update the running CPU usage.

      For more info on how to set the starting moment see the class
      description.

      :return: None.


   .. method:: result(self) -> float

      Retrieves the average CPU usage.

      Calling this method will not change the internal state of the metric.

      :return: The average CPU usage, as a float value.


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.



.. py:class:: MinibatchCPUUsage

   Bases: :class:`avalanche.evaluation.metrics.cpu_usage.CPUPluginMetric`

   The minibatch CPU usage metric.
   This plugin metric only works at training time.

   This metric "logs" the CPU usage for each iteration.

   If a more coarse-grained logging is needed, consider using
   :class:`EpochCPUUsage`.

   Creates an instance of the minibatch CPU usage metric.

   .. method:: before_training_iteration(self, strategy)


   .. method:: __str__(self)



.. py:class:: EpochCPUUsage

   Bases: :class:`avalanche.evaluation.metrics.cpu_usage.CPUPluginMetric`

   The Epoch CPU usage metric.
   This plugin metric only works at training time.

   The average usage will be logged after each epoch.

   Creates an instance of the epoch CPU usage metric.

   .. method:: before_training_epoch(self, strategy)


   .. method:: __str__(self)



.. py:class:: RunningEpochCPUUsage

   Bases: :class:`avalanche.evaluation.metrics.cpu_usage.CPUPluginMetric`

   The running epoch CPU usage metric.
   This plugin metric only works at training time

   After each iteration, the metric logs the average CPU usage up
   to the current epoch iteration.

   Creates an instance of the average epoch cpu usage metric.

   .. method:: result(self, strategy) -> float


   .. method:: before_training_epoch(self, strategy)


   .. method:: before_training_iteration(self, strategy)


   .. method:: after_training_iteration(self, strategy)


   .. method:: __str__(self)



.. py:class:: ExperienceCPUUsage

   Bases: :class:`avalanche.evaluation.metrics.cpu_usage.CPUPluginMetric`

   The average experience CPU usage metric.
   This plugin metric works only at eval time.

   After each experience, this metric emits the average CPU usage on that
   experience.

   Creates an instance of the experience CPU usage metric.

   .. method:: before_eval_exp(self, strategy)


   .. method:: __str__(self)



.. py:class:: StreamCPUUsage

   Bases: :class:`avalanche.evaluation.metrics.cpu_usage.CPUPluginMetric`

   The average stream CPU usage metric.
   This plugin metric works only at eval time.

   After the entire evaluation stream, this metric emits
   the average CPU usage on all experiences.

   Creates an instance of the stream CPU usage metric.

   .. method:: before_eval(self, strategy)


   .. method:: __str__(self)



.. function:: cpu_usage_metrics(*, minibatch=False, epoch=False, epoch_running=False, experience=False, stream=False) -> List[PluginMetric]

   Helper method that can be used to obtain the desired set of
   plugin metrics.

   :param minibatch: If True, will return a metric able to log the minibatch
       CPU usage
   :param epoch: If True, will return a metric able to log the epoch
       CPU usage
   :param epoch_running: If True, will return a metric able to log the running
       epoch CPU usage.
   :param experience: If True, will return a metric able to log the experience
       CPU usage.
   :param stream: If True, will return a metric able to log the evaluation
       stream CPU usage.

   :return: A list of plugin metrics.


.. py:class:: DiskUsage(paths_to_monitor: Union[PathAlike, Sequence[PathAlike]] = None)

   Bases: :class:`Metric[float]`

   The standalone disk usage metric.

   This metric can be used to monitor the size of a set of directories.
   e.g. This can be useful to monitor the size of a replay buffer,

   Creates an instance of the standalone disk usage metric.

   The `result` method will return the sum of the size
   of the directories specified as the first parameter in KiloBytes.

   :param paths_to_monitor: a path or a list of paths to monitor. If None,
       the current working directory is used. Defaults to None.

   .. method:: update(self)

      Updates the disk usage statistics.

      :return None.


   .. method:: result(self) -> Optional[float]

      Retrieves the disk usage as computed during the last call to the
      `update` method.

      Calling this method will not change the internal state of the metric.

      :return: The disk usage or None if `update` was not invoked yet.


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.


   .. method:: get_dir_size(path: str)
      :staticmethod:



.. py:class:: MinibatchDiskUsage(paths_to_monitor)

   Bases: :class:`avalanche.evaluation.metrics.disk_usage.DiskPluginMetric`

   The minibatch Disk usage metric.
   This plugin metric only works at training time.

   At the end of each iteration, this metric logs the total
   size (in KB) of all the monitored paths.

   If a more coarse-grained logging is needed, consider using
   :class:`EpochDiskUsage`.

   Creates an instance of the minibatch Disk usage metric.

   .. method:: __str__(self)



.. py:class:: EpochDiskUsage(paths_to_monitor)

   Bases: :class:`avalanche.evaluation.metrics.disk_usage.DiskPluginMetric`

   The Epoch Disk usage metric.
   This plugin metric only works at training time.

   At the end of each epoch, this metric logs the total
   size (in KB) of all the monitored paths.

   Creates an instance of the epoch Disk usage metric.

   .. method:: __str__(self)



.. py:class:: ExperienceDiskUsage(paths_to_monitor)

   Bases: :class:`avalanche.evaluation.metrics.disk_usage.DiskPluginMetric`

   The average experience Disk usage metric.
   This plugin metric works only at eval time.

   At the end of each experience, this metric logs the total
   size (in KB) of all the monitored paths.

   Creates an instance of the experience Disk usage metric.

   .. method:: __str__(self)



.. py:class:: StreamDiskUsage(paths_to_monitor)

   Bases: :class:`avalanche.evaluation.metrics.disk_usage.DiskPluginMetric`

   The average stream Disk usage metric.
   This plugin metric works only at eval time.

   At the end of the eval stream, this metric logs the total
   size (in KB) of all the monitored paths.

   Creates an instance of the stream Disk usage metric.

   .. method:: __str__(self)



.. function:: disk_usage_metrics(*, paths_to_monitor=None, minibatch=False, epoch=False, experience=False, stream=False) -> List[PluginMetric]

   Helper method that can be used to obtain the desired set of
   standalone metrics.

   :param minibatch: If True, will return a metric able to log the minibatch
       Disk usage
   :param epoch: If True, will return a metric able to log the epoch
       Disk usage
   :param experience: If True, will return a metric able to log the experience
       Disk usage.
   :param stream: If True, will return a metric able to log the evaluation
       stream Disk usage.

   :return: A list of plugin metrics.


.. py:class:: Forgetting

   Bases: :class:`Metric[Union[float, None, Dict[int, float]]]`

   The standalone Forgetting metric.
   This metric returns the forgetting relative to a specific key.
   Alternatively, this metric returns a dict in which each key is associated
   to the forgetting.
   Forgetting is computed as the difference between the first value recorded
   for a specific key and the last value recorded for that key.
   The value associated to a key can be update with the `update` method.

   At initialization, this metric returns an empty dictionary.

   Creates an instance of the standalone Forgetting metric

   .. attribute:: initial
      :annotation: :Dict[int, float]

      The initial value for each key.


   .. attribute:: last
      :annotation: :Dict[int, float]

      The last value detected for each key


   .. method:: update_initial(self, k, v)


   .. method:: update_last(self, k, v)


   .. method:: update(self, k, v, initial=False)


   .. method:: result(self, k=None) -> Union[float, None, Dict[int, float]]

      Forgetting is returned only for keys encountered twice.

      :param k: the key for which returning forgetting. If k has not
          updated at least twice it returns None. If k is None,
          forgetting will be returned for all keys encountered at least
          twice.

      :return: the difference between the first and last value encountered
          for k, if k is not None. It returns None if k has not been updated
          at least twice. If k is None, returns a dictionary
          containing keys whose value has been updated at least twice. The
          associated value is the difference between the first and last
          value recorded for that key.


   .. method:: reset_last(self) -> None


   .. method:: reset(self) -> None

      Resets the metric internal state.

      :return: None.



.. py:class:: GenericExperienceForgetting

   Bases: :class:`PluginMetric[Dict[int, float]]`

   The GenericExperienceForgetting metric, describing the change in
   a metric detected for a certain experience. The user should
   subclass this and provide the desired metric.

   In particular, the user should override:
   * __init__ by calling `super` and instantiating the `self.current_metric`
   property as a valid avalanche metric
   * `metric_update`, to update `current_metric`
   * `metric_result` to get the result from `current_metric`.
   * `__str__` to define the experience forgetting  name.

   This plugin metric, computed separately for each experience,
   is the difference between the metric result obtained after
   first training on a experience and the metric result obtained
   on the same experience at the end of successive experiences.

   This metric is computed during the eval phase only.

   Creates an instance of the GenericExperienceForgetting metric.

   .. attribute:: forgetting
      

      The general metric to compute forgetting


   .. attribute:: eval_exp_id
      

      The current evaluation experience id


   .. attribute:: train_exp_id
      

      The last encountered training experience id


   .. method:: reset(self) -> None

      Resets the metric.

      Beware that this will also reset the initial metric of each
      experience!

      :return: None.


   .. method:: reset_last(self) -> None

      Resets the last metric value.

      This will preserve the initial metric value of each experience.
      To be used at the beginning of each eval experience.

      :return: None.


   .. method:: update(self, k, v, initial=False)

      Update forgetting metric.
      See `Forgetting` for more detailed information.

      :param k: key to update
      :param v: value associated to k
      :param initial: update initial value. If False, update
          last value.


   .. method:: result(self, k=None) -> Union[float, None, Dict[int, float]]

      See `Forgetting` documentation for more detailed information.

      k: optional key from which compute forgetting.


   .. method:: before_training_exp(self, strategy: BaseStrategy) -> None

      Called before `train_exp` by the `BaseStrategy`. 


   .. method:: before_eval(self, strategy) -> None

      Called before `eval` by the `BaseStrategy`. 


   .. method:: before_eval_exp(self, strategy: BaseStrategy) -> None

      Called before `eval_exp` by the `BaseStrategy`. 


   .. method:: after_eval_iteration(self, strategy: BaseStrategy) -> None

      Called after the end of an iteration by the
      `BaseStrategy`. 


   .. method:: after_eval_exp(self, strategy: BaseStrategy) -> MetricResult

      Called after `eval_exp` by the `BaseStrategy`. 


   .. method:: metric_update(self, strategy)
      :abstractmethod:


   .. method:: metric_result(self, strategy)
      :abstractmethod:


   .. method:: __str__(self)
      :abstractmethod:

      Return str(self).



.. py:class:: GenericStreamForgetting

   Bases: :class:`avalanche.evaluation.metrics.forgetting_bwt.GenericExperienceForgetting`

   The GenericStreamForgetting metric, describing the average evaluation
   change in the desired metric detected over all experiences observed
   during training.

   In particular, the user should override:
   * __init__ by calling `super` and instantiating the `self.current_metric`
   property as a valid avalanche metric
   * `metric_update`, to update `current_metric`
   * `metric_result` to get the result from `current_metric`.
   * `__str__` to define the experience forgetting  name.

   This plugin metric, computed over all observed experiences during training,
   is the average over the difference between the metric result obtained
   after first training on a experience and the metric result obtained
   on the same experience at the end of successive experiences.

   This metric is computed during the eval phase only.

   Creates an instance of the GenericStreamForgetting metric.

   .. attribute:: stream_forgetting
      

      The average forgetting over all experiences


   .. method:: reset(self) -> None

      Resets the forgetting metrics.

      Beware that this will also reset the initial metric value of each
      experience!

      :return: None.


   .. method:: exp_update(self, k, v, initial=False)

      Update forgetting metric.
      See `Forgetting` for more detailed information.

      :param k: key to update
      :param v: value associated to k
      :param initial: update initial value. If False, update
          last value.


   .. method:: exp_result(self, k=None) -> Union[float, None, Dict[int, float]]

      Result for experience defined by a key.
      See `Forgetting` documentation for more detailed information.

      k: optional key from which compute forgetting.


   .. method:: result(self, k=None) -> Union[float, None, Dict[int, float]]

      The average forgetting over all experience.

      k: optional key from which compute forgetting.


   .. method:: before_eval(self, strategy) -> None

      Called before `eval` by the `BaseStrategy`. 


   .. method:: after_eval_exp(self, strategy: BaseStrategy) -> None

      Called after `eval_exp` by the `BaseStrategy`. 


   .. method:: after_eval(self, strategy: BaseStrategy) -> 'MetricResult'

      Called after `eval` by the `BaseStrategy`. 


   .. method:: metric_update(self, strategy)
      :abstractmethod:


   .. method:: metric_result(self, strategy)
      :abstractmethod:


   .. method:: __str__(self)
      :abstractmethod:

      Return str(self).



.. py:class:: ExperienceForgetting

   Bases: :class:`avalanche.evaluation.metrics.forgetting_bwt.GenericExperienceForgetting`

   The ExperienceForgetting metric, describing the accuracy loss
   detected for a certain experience.

   This plugin metric, computed separately for each experience,
   is the difference between the accuracy result obtained after
   first training on a experience and the accuracy result obtained
   on the same experience at the end of successive experiences.

   This metric is computed during the eval phase only.

   Creates an instance of the ExperienceForgetting metric.

   .. method:: metric_update(self, strategy)


   .. method:: metric_result(self, strategy)


   .. method:: __str__(self)

      Return str(self).



.. py:class:: StreamForgetting

   Bases: :class:`avalanche.evaluation.metrics.forgetting_bwt.GenericStreamForgetting`

   The StreamForgetting metric, describing the average evaluation accuracy loss
   detected over all experiences observed during training.

   This plugin metric, computed over all observed experiences during training,
   is the average over the difference between the accuracy result obtained
   after first training on a experience and the accuracy result obtained
   on the same experience at the end of successive experiences.

   This metric is computed during the eval phase only.

   Creates an instance of the StreamForgetting metric.

   .. method:: metric_update(self, strategy)


   .. method:: metric_result(self, strategy)


   .. method:: __str__(self)

      Return str(self).



.. function:: forgetting_metrics(*, experience=False, stream=False) -> List[PluginMetric]

   Helper method that can be used to obtain the desired set of
   plugin metrics.

   :param experience: If True, will return a metric able to log
       the forgetting on each evaluation experience.
   :param stream: If True, will return a metric able to log
       the forgetting averaged over the evaluation stream experiences,
       which have been observed during training.

   :return: A list of plugin metrics.


.. py:class:: BWT

   Bases: :class:`avalanche.evaluation.metrics.forgetting_bwt.Forgetting`

   The standalone Backward Transfer metric.
   This metric returns the backward transfer relative to a specific key.
   Alternatively, this metric returns a dict in which each key is associated
   to the backward transfer.
   Backward transfer is computed as the difference between the last value
   recorded for a specific key and the first value recorded for that key.
   The value associated to a key can be update with the `update` method.

   At initialization, this metric returns an empty dictionary.

   Creates an instance of the standalone Forgetting metric

   .. method:: result(self, k=None) -> Union[float, None, Dict[int, float]]

      Backward Transfer is returned only for keys encountered twice.
      Backward Transfer is the negative forgetting.

      :param k: the key for which returning backward transfer. If k has not
          updated at least twice it returns None. If k is None,
          backward transfer will be returned for all keys encountered at
          least twice.

      :return: the difference between the last value encountered for k
          and its first value, if k is not None.
          It returns None if k has not been updated
          at least twice. If k is None, returns a dictionary
          containing keys whose value has been updated at least twice. The
          associated value is the difference between the last and first
          value recorded for that key.



.. py:class:: ExperienceBWT

   Bases: :class:`avalanche.evaluation.metrics.forgetting_bwt.ExperienceForgetting`

   The Experience Backward Transfer metric.

   This plugin metric, computed separately for each experience,
   is the difference between the last accuracy result obtained on a certain
   experience and the accuracy result obtained when first training on that
   experience.

   This metric is computed during the eval phase only.

   Creates an instance of the ExperienceForgetting metric.

   .. method:: result(self, k=None) -> Union[float, None, Dict[int, float]]

      See `Forgetting` documentation for more detailed information.

      k: optional key from which compute forgetting.


   .. method:: __str__(self)

      Return str(self).



.. py:class:: StreamBWT

   Bases: :class:`avalanche.evaluation.metrics.forgetting_bwt.StreamForgetting`

   The StreamBWT metric, emitting the average BWT across all experiences
   encountered during training.

   This plugin metric, computed over all observed experiences during training,
   is the average over the difference between the last accuracy result
   obtained on an experience and the accuracy result obtained when first
   training on that experience.

   This metric is computed during the eval phase only.

   Creates an instance of the StreamForgetting metric.

   .. method:: exp_result(self, k=None) -> Union[float, None, Dict[int, float]]

      Result for experience defined by a key.
      See `BWT` documentation for more detailed information.

      k: optional key from which compute backward transfer.


   .. method:: __str__(self)

      Return str(self).



.. function:: bwt_metrics(*, experience=False, stream=False) -> List[PluginMetric]

   Helper method that can be used to obtain the desired set of
   plugin metrics.

   :param experience: If True, will return a metric able to log
       the backward transfer on each evaluation experience.
   :param stream: If True, will return a metric able to log
       the backward transfer averaged over the evaluation stream experiences
       which have been observed during training.
   :return: A list of plugin metrics.


.. py:class:: ForwardTransfer

   Bases: :class:`Metric[Union[float, None, Dict[int, float]]]`

   The standalone Forward Transfer metric.
   This metric returns the forward transfer relative to a specific key.
   Alternatively, this metric returns a dict in which each key is
   associated to the forward transfer.
   Forward transfer is computed as the difference between the value
   recorded for a specific key after the previous experience has
   been trained on, and random initialization before training.
   The value associated to a key can be updated with the `update` method.

   At initialization, this metric returns an empty dictionary.

   Creates an instance of the standalone Forward Transfer metric

   .. attribute:: initial
      :annotation: :Dict[int, float]

      The initial value for each key. This is the accuracy at 
      random initialization.


   .. attribute:: previous
      :annotation: :Dict[int, float]

      The previous experience value detected for each key


   .. method:: update_initial(self, k, v)


   .. method:: update_previous(self, k, v)


   .. method:: update(self, k, v, initial=False)


   .. method:: result(self, k=None) -> Union[float, None, Dict[int, float]]

      :param k: the key for which returning forward transfer. If k is None,
          forward transfer will be returned for all keys
          where the previous experience has been trained on.

      :return: the difference between the key value after training on the
          previous experience, and the key at random initialization.


   .. method:: reset(self) -> None

      Resets the metric internal state.

      :return: None.



.. py:class:: GenericExperienceForwardTransfer

   Bases: :class:`PluginMetric[Dict[int, float]]`

   The GenericExperienceForwardMetric metric, describing the forward transfer
   detected after a certain experience. The user should
   subclass this and provide the desired metric.

   In particular, the user should override:
   * __init__ by calling `super` and instantiating the `self.current_metric`
   property as a valid avalanche metric
   * `metric_update`, to update `current_metric`
   * `metric_result` to get the result from `current_metric`.
   * `__str__` to define the experience forward transfer  name.

   This metric is computed during the eval phase only.

   Creates an instance of the GenericExperienceForwardTransfer metric.

   .. attribute:: forward_transfer
      

      The general metric to compute forward transfer


   .. attribute:: eval_exp_id
      

      The current evaluation experience id


   .. attribute:: train_exp_id
      

      The last encountered training experience id


   .. method:: reset(self) -> None

      Resets the metric.

      Note that this will reset the previous and initial accuracy of each
      experience.

      :return: None.


   .. method:: update(self, k, v, initial=False)

      Update forward transfer metric.
      See `ForwardTransfer` for more detailed information.

      :param k: key to update
      :param v: value associated to k
      :param initial: update initial value. If False, update
          previous value.


   .. method:: result(self, k=None) -> Union[float, None, Dict[int, float]]

      Result for experience defined by a key.
      See `ForwardTransfer` documentation for more detailed information.

      k: optional key from which to compute forward transfer.


   .. method:: before_training_exp(self, strategy: BaseStrategy) -> None

      Called before `train_exp` by the `BaseStrategy`. 


   .. method:: after_eval(self, strategy)

      Called after `eval` by the `BaseStrategy`. 


   .. method:: before_eval_exp(self, strategy: BaseStrategy) -> None

      Called before `eval_exp` by the `BaseStrategy`. 


   .. method:: after_eval_iteration(self, strategy: BaseStrategy) -> None

      Called after the end of an iteration by the
      `BaseStrategy`. 


   .. method:: after_eval_exp(self, strategy: BaseStrategy) -> MetricResult

      Called after `eval_exp` by the `BaseStrategy`. 


   .. method:: metric_update(self, strategy)
      :abstractmethod:


   .. method:: metric_result(self, strategy)
      :abstractmethod:


   .. method:: __str__(self)
      :abstractmethod:

      Return str(self).



.. py:class:: ExperienceForwardTransfer

   Bases: :class:`avalanche.evaluation.metrics.forward_transfer.GenericExperienceForwardTransfer`

   The Forward Transfer computed on each experience separately.
   The transfer is computed based on the accuracy metric.

   Creates an instance of the GenericExperienceForwardTransfer metric.

   .. method:: metric_update(self, strategy)


   .. method:: metric_result(self, strategy)


   .. method:: __str__(self)

      Return str(self).



.. py:class:: GenericStreamForwardTransfer

   Bases: :class:`avalanche.evaluation.metrics.forward_transfer.GenericExperienceForwardTransfer`

   The GenericStreamForwardTransfer metric, describing the average evaluation
   forward transfer detected over all experiences observed during training.

   In particular, the user should override:
   * __init__ by calling `super` and instantiating the `self.current_metric`
   property as a valid avalanche metric
   * `metric_update`, to update `current_metric`
   * `metric_result` to get the result from `current_metric`.
   * `__str__` to define the experience forgetting  name.

   This metric is computed during the eval phase only.

   Creates an instance of the GenericStreamForwardTransfer metric.

   .. attribute:: stream_forward_transfer
      

      The average forward transfer over all experiences


   .. method:: reset(self) -> None

      Resets the forward transfer metrics.

      Note that this will reset the previous and initial accuracy of each
      experience.

      :return: None.


   .. method:: exp_update(self, k, v, initial=False)

      Update forward transfer metric.
      See `Forward Transfer` for more detailed information.

      :param k: key to update
      :param v: value associated to k
      :param initial: update initial value. If False, update
          previous value.


   .. method:: exp_result(self, k=None) -> Union[float, None, Dict[int, float]]

      Result for experience defined by a key.
      See `ForwardTransfer` documentation for more detailed information.

      k: optional key from which to compute forward transfer.


   .. method:: result(self, k=None) -> Union[float, None, Dict[int, float]]

      The average forward transfer over all experiences.

      k: optional key from which to compute forward transfer.


   .. method:: before_eval(self, strategy) -> None

      Called before `eval` by the `BaseStrategy`. 


   .. method:: after_eval_exp(self, strategy: BaseStrategy) -> None

      Called after `eval_exp` by the `BaseStrategy`. 


   .. method:: after_eval(self, strategy: BaseStrategy) -> 'MetricResult'

      Called after `eval` by the `BaseStrategy`. 


   .. method:: metric_update(self, strategy)
      :abstractmethod:


   .. method:: metric_result(self, strategy)
      :abstractmethod:


   .. method:: __str__(self)
      :abstractmethod:

      Return str(self).



.. py:class:: StreamForwardTransfer

   Bases: :class:`avalanche.evaluation.metrics.forward_transfer.GenericStreamForwardTransfer`

   The Forward Transfer averaged over all the evaluation experiences.

   This plugin metric, computed over all observed experiences during training,
   is the average over the difference between the accuracy result obtained
   after the previous experience and the accuracy result obtained
   on random initialization.

   Creates an instance of the GenericStreamForwardTransfer metric.

   .. method:: metric_update(self, strategy)


   .. method:: metric_result(self, strategy)


   .. method:: __str__(self)

      Return str(self).



.. function:: forward_transfer_metrics(*, experience=False, stream=False)

   Helper method that can be used to obtain the desired set of
   plugin metrics.

   :param experience: If True, will return a metric able to log
       the forward transfer on each evaluation experience.
   :param stream: If True, will return a metric able to log
       the forward transfer averaged over the evaluation stream experiences,
       which have been observed during training.

   :return: A list of plugin metrics.


.. py:class:: MaxGPU(gpu_id, every=0.5)

   Bases: :class:`Metric[float]`

   The standalone GPU usage metric.
   Important: this metric approximates the real maximum GPU percentage
    usage since it sample at discrete amount of time the GPU values.

   Instances of this metric keeps the maximum GPU usage percentage detected.
   The `start_thread` method starts the usage tracking.
   The `stop_thread` method stops the tracking.

   The result, obtained using the `result` method, is the usage in mega-bytes.

   The reset method will bring the metric to its initial state. By default
   this metric in its initial state will return an usage value of 0.

   Creates an instance of the GPU usage metric.

   :param gpu_id: GPU device ID.
   :param every: seconds after which update the maximum GPU
       usage

   .. attribute:: thread
      

      Thread executing GPU monitoring code


   .. attribute:: stop_f
      :annotation: = False

      Flag to stop the thread


   .. attribute:: max_usage
      :annotation: = 0

      Main metric result. Max GPU usage.


   .. method:: start_thread(self)


   .. method:: stop_thread(self)


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.


   .. method:: result(self) -> Optional[float]

      Returns the max GPU percentage value.

      :return: The percentage GPU usage as a float value in range [0, 1].


   .. method:: update(self)



.. py:class:: MinibatchMaxGPU(gpu_id, every=0.5)

   Bases: :class:`avalanche.evaluation.metrics.gpu_usage.GPUPluginMetric`

   The Minibatch Max GPU metric.
   This plugin metric only works at training time.

   Creates an instance of the Minibatch Max GPU metric

   :param gpu_id: GPU device ID.
   :param every: seconds after which update the maximum GPU
       usage

   .. method:: before_training(self, strategy: BaseStrategy) -> None


   .. method:: after_training(self, strategy: BaseStrategy) -> None


   .. method:: __str__(self)



.. py:class:: EpochMaxGPU(gpu_id, every=0.5)

   Bases: :class:`avalanche.evaluation.metrics.gpu_usage.GPUPluginMetric`

   The Epoch Max GPU metric.
   This plugin metric only works at training time.

   Creates an instance of the epoch Max GPU metric.

   :param gpu_id: GPU device ID.
   :param every: seconds after which update the maximum GPU
       usage

   .. method:: before_training(self, strategy: BaseStrategy)


   .. method:: after_training(self, strategy: BaseStrategy) -> None


   .. method:: __str__(self)



.. py:class:: ExperienceMaxGPU(gpu_id, every=0.5)

   Bases: :class:`avalanche.evaluation.metrics.gpu_usage.GPUPluginMetric`

   The Experience Max GPU metric.
   This plugin metric only works at eval time.

   Creates an instance of the Experience CPU usage metric.

   :param gpu_id: GPU device ID.
   :param every: seconds after which update the maximum GPU
       usage

   .. method:: before_eval(self, strategy: BaseStrategy)


   .. method:: after_eval(self, strategy: BaseStrategy)


   .. method:: __str__(self)



.. py:class:: StreamMaxGPU(gpu_id, every=0.5)

   Bases: :class:`avalanche.evaluation.metrics.gpu_usage.GPUPluginMetric`

   The Stream Max GPU metric.
   This plugin metric only works at eval time.

   Creates an instance of the Experience CPU usage metric.

   :param gpu_id: GPU device ID.
   :param every: seconds after which update the maximum GPU
       usage

   .. method:: before_eval(self, strategy)


   .. method:: after_eval(self, strategy: BaseStrategy) -> MetricResult


   .. method:: __str__(self)



.. function:: gpu_usage_metrics(gpu_id, every=0.5, minibatch=False, epoch=False, experience=False, stream=False) -> List[PluginMetric]

   Helper method that can be used to obtain the desired set of
   plugin metrics.

   :param gpu_id: GPU device ID.
   :param every: seconds after which update the maximum GPU
       usage
   :param minibatch: If True, will return a metric able to log the minibatch
       max GPU usage.
   :param epoch: If True, will return a metric able to log the epoch
       max GPU usage.
   :param experience: If True, will return a metric able to log the experience
       max GPU usage.
   :param stream: If True, will return a metric able to log the evaluation
       max stream GPU usage.

   :return: A list of plugin metrics.


.. py:class:: Loss

   Bases: :class:`Metric[float]`

   The standalone Loss metric. This is a general metric
   used to compute more specific ones.

   Instances of this metric keeps the running average loss
   over multiple <prediction, target> pairs of Tensors,
   provided incrementally.
   The "prediction" and "target" tensors may contain plain labels or
   one-hot/logit vectors.

   Each time `result` is called, this metric emits the average loss
   across all predictions made since the last `reset`.

   The reset method will bring the metric to its initial state. By default
   this metric in its initial state will return a loss value of 0.

   Creates an instance of the loss metric.

   By default this metric in its initial state will return a loss
   value of 0. The metric can be updated by using the `update` method
   while the running loss can be retrieved using the `result` method.

   .. method:: update(self, loss: Tensor, patterns: int, task_label: int) -> None

      Update the running loss given the loss Tensor and the minibatch size.

      :param loss: The loss Tensor. Different reduction types don't affect
          the result.
      :param patterns: The number of patterns in the minibatch.
      :param task_label: the task label associated to the current experience
      :return: None.


   .. method:: result(self, task_label=None) -> Dict[int, float]

      Retrieves the running average loss per pattern.

      Calling this method will not change the internal state of the metric.
      :param task_label: None to return metric values for all the task labels.
          If an int, return value only for that task label
      :return: The running loss, as a float.


   .. method:: reset(self, task_label=None) -> None

      Resets the metric.

      :param task_label: None to reset all metric values. If an int,
          reset metric value corresponding to that task label.
      :return: None.



.. py:class:: MinibatchLoss

   Bases: :class:`avalanche.evaluation.metrics.loss.LossPluginMetric`

   The minibatch loss metric.
   This plugin metric only works at training time.

   This metric computes the average loss over patterns
   from a single minibatch.
   It reports the result after each iteration.

   If a more coarse-grained logging is needed, consider using
   :class:`EpochLoss` instead.

   Creates an instance of the MinibatchLoss metric.

   .. method:: __str__(self)



.. py:class:: EpochLoss

   Bases: :class:`avalanche.evaluation.metrics.loss.LossPluginMetric`

   The average loss over a single training epoch.
   This plugin metric only works at training time.

   The loss will be logged after each training epoch by computing
   the loss on the predicted patterns during the epoch divided by
   the overall number of patterns encountered in that epoch.

   Creates an instance of the EpochLoss metric.

   .. method:: __str__(self)



.. py:class:: RunningEpochLoss

   Bases: :class:`avalanche.evaluation.metrics.loss.LossPluginMetric`

   The average loss across all minibatches up to the current
   epoch iteration.
   This plugin metric only works at training time.

   At each iteration, this metric logs the loss averaged over all patterns
   seen so far in the current epoch.
   The metric resets its state after each training epoch.

   Creates an instance of the RunningEpochLoss metric.

   .. method:: __str__(self)



.. py:class:: ExperienceLoss

   Bases: :class:`avalanche.evaluation.metrics.loss.LossPluginMetric`

   At the end of each experience, this metric reports
   the average loss over all patterns seen in that experience.
   This plugin metric only works at eval time.

   Creates an instance of ExperienceLoss metric

   .. method:: __str__(self)



.. py:class:: StreamLoss

   Bases: :class:`avalanche.evaluation.metrics.loss.LossPluginMetric`

   At the end of the entire stream of experiences, this metric reports the
   average loss over all patterns seen in all experiences.
   This plugin metric only works at eval time.

   Creates an instance of StreamLoss metric

   .. method:: __str__(self)



.. function:: loss_metrics(*, minibatch=False, epoch=False, epoch_running=False, experience=False, stream=False) -> List[PluginMetric]

   Helper method that can be used to obtain the desired set of
   plugin metrics.

   :param minibatch: If True, will return a metric able to log
       the minibatch loss at training time.
   :param epoch: If True, will return a metric able to log
       the epoch loss at training time.
   :param epoch_running: If True, will return a metric able to log
       the running epoch loss at training time.
   :param experience: If True, will return a metric able to log
       the loss on each evaluation experience.
   :param stream: If True, will return a metric able to log
       the loss averaged over the entire evaluation stream of experiences.

   :return: A list of plugin metrics.


.. py:class:: MAC

   Bases: :class:`Metric[int]`

   Standalone Multiply-and-accumulate metric. Provides a lower bound of the
   computational cost of a model in a hardware-independent way by
   computing the number of multiplications. Currently supports only
   Linear or Conv2d modules. Other operations are ignored.

   Creates an instance of the MAC metric.

   .. method:: update(self, model: Module, dummy_input: Tensor)

      Computes the MAC metric.

      :param model: current model.
      :param dummy_input: A tensor of the correct size to feed as input
          to model. It includes batch size
      :return: MAC metric.


   .. method:: result(self) -> Optional[int]

      Return the number of MAC operations as computed in the previous call
      to the `update` method.

      :return: The number of MAC operations or None if `update` has not been
          called yet.


   .. method:: reset(self)

      Resets the metric internal state.

      :return: None.


   .. method:: update_compute_cost(self, module, dummy_input, output)


   .. method:: is_recognized_module(mod)
      :staticmethod:



.. py:class:: MinibatchMAC

   Bases: :class:`avalanche.evaluation.metrics.mac.MACPluginMetric`

   The minibatch MAC metric.
   This plugin metric only works at training time.

   This metric computes the MAC over 1 pattern
   from a single minibatch.
   It reports the result after each iteration.

   If a more coarse-grained logging is needed, consider using
   :class:`EpochMAC` instead.

   Creates an instance of the MinibatchMAC metric.

   .. method:: __str__(self)

      Return str(self).



.. py:class:: EpochMAC

   Bases: :class:`avalanche.evaluation.metrics.mac.MACPluginMetric`

   The MAC at the end of each epoch computed on a
   single pattern.
   This plugin metric only works at training time.

   The MAC will be logged after each training epoch.

   Creates an instance of the EpochMAC metric.

   .. method:: __str__(self)

      Return str(self).



.. py:class:: ExperienceMAC

   Bases: :class:`avalanche.evaluation.metrics.mac.MACPluginMetric`

   At the end of each experience, this metric reports the
   MAC computed on a single pattern.
   This plugin metric only works at eval time.

   Creates an instance of ExperienceMAC metric

   .. method:: __str__(self)

      Return str(self).



.. function:: MAC_metrics(*, minibatch=False, epoch=False, experience=False) -> List[PluginMetric]

   Helper method that can be used to obtain the desired set of
   plugin metrics.

   :param minibatch: If True, will return a metric able to log
       the MAC after each iteration at training time.
   :param epoch: If True, will return a metric able to log
       the MAC after each epoch at training time.
   :param experience: If True, will return a metric able to log
       the MAC after each eval experience.

   :return: A list of plugin metrics.


.. py:class:: MaxRAM(every=1)

   Bases: :class:`Metric[float]`

   The standalone RAM usage metric.
   Important: this metric approximates the real maximum RAM usage since
   it sample at discrete amount of time the RAM values.

   Instances of this metric keeps the maximum RAM usage detected.
   The `start_thread` method starts the usage tracking.
   The `stop_thread` method stops the tracking.

   The result, obtained using the `result` method, is the usage in mega-bytes.

   The reset method will bring the metric to its initial state. By default
   this metric in its initial state will return an usage value of 0.

   Creates an instance of the RAM usage metric.
   :param every: seconds after which update the maximum RAM
       usage

   .. attribute:: stop_f
      :annotation: = False

      Flag to stop the thread


   .. attribute:: max_usage
      :annotation: = 0

      Main metric result. Max RAM usage.


   .. attribute:: thread
      

      Thread executing RAM monitoring code


   .. method:: result(self) -> Optional[float]

      Retrieves the RAM usage.

      Calling this method will not change the internal state of the metric.

      :return: The average RAM usage in bytes, as a float value.


   .. method:: start_thread(self)


   .. method:: stop_thread(self)


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.


   .. method:: update(self)



.. py:class:: MinibatchMaxRAM(every=1)

   Bases: :class:`avalanche.evaluation.metrics.ram_usage.RAMPluginMetric`

   The Minibatch Max RAM metric.
   This plugin metric only works at training time.

   Creates an instance of the Minibatch Max RAM metric
   :param every: seconds after which update the maximum RAM
       usage

   .. method:: before_training(self, strategy: BaseStrategy) -> None


   .. method:: after_training(self, strategy: BaseStrategy) -> None


   .. method:: __str__(self)



.. py:class:: EpochMaxRAM(every=1)

   Bases: :class:`avalanche.evaluation.metrics.ram_usage.RAMPluginMetric`

   The Epoch Max RAM metric.
   This plugin metric only works at training time.

   Creates an instance of the epoch Max RAM metric.
   :param every: seconds after which update the maximum RAM
       usage

   .. method:: before_training(self, strategy: BaseStrategy) -> None


   .. method:: after_training(self, strategy: BaseStrategy) -> None


   .. method:: __str__(self)



.. py:class:: ExperienceMaxRAM(every=1)

   Bases: :class:`avalanche.evaluation.metrics.ram_usage.RAMPluginMetric`

   The Experience Max RAM metric.
   This plugin metric only works at eval time.

   Creates an instance of the Experience CPU usage metric.
   :param every: seconds after which update the maximum RAM
       usage

   .. method:: before_eval(self, strategy: BaseStrategy) -> None


   .. method:: after_eval(self, strategy: BaseStrategy) -> None


   .. method:: __str__(self)



.. py:class:: StreamMaxRAM(every=1)

   Bases: :class:`avalanche.evaluation.metrics.ram_usage.RAMPluginMetric`

   The Stream Max RAM metric.
   This plugin metric only works at eval time.

   Creates an instance of the Experience CPU usage metric.
   :param every: seconds after which update the maximum RAM
       usage

   .. method:: before_eval(self, strategy)


   .. method:: after_eval(self, strategy: BaseStrategy) -> MetricResult


   .. method:: __str__(self)



.. function:: ram_usage_metrics(*, every=1, minibatch=False, epoch=False, experience=False, stream=False) -> List[PluginMetric]

   Helper method that can be used to obtain the desired set of
   plugin metrics.

   :param every: seconds after which update the maximum RAM
       usage
   :param minibatch: If True, will return a metric able to log the minibatch
       max RAM usage.
   :param epoch: If True, will return a metric able to log the epoch
       max RAM usage.
   :param experience: If True, will return a metric able to log the experience
       max RAM usage.
   :param stream: If True, will return a metric able to log the evaluation
       max stream RAM usage.

   :return: A list of plugin metrics.


.. py:class:: ElapsedTime

   Bases: :class:`Metric[float]`

   The standalone Elapsed Time metric.

   Instances of this metric keep track of the time elapsed between calls to the
   `update` method. The starting time is set when the `update` method is called
   for the first time. That is, the starting time is *not* taken at the time
   the constructor is invoked.

   Calling the `update` method more than twice will update the metric to the
   elapsed time between the first and the last call to `update`.

   The result, obtained using the `result` method, is the time, in seconds,
   computed as stated above.

   The `reset` method will set the metric to its initial state, thus resetting
   the initial time. This metric in its initial state (or if the `update`
   method was invoked only once) will return an elapsed time of 0.

   Creates an instance of the ElapsedTime metric.

   This metric in its initial state (or if the `update` method was invoked
   only once) will return an elapsed time of 0. The metric can be updated
   by using the `update` method while the running accuracy can be retrieved
   using the `result` method.

   .. method:: update(self) -> None

      Update the elapsed time.

      For more info on how to set the initial time see the class description.

      :return: None.


   .. method:: result(self) -> float

      Retrieves the elapsed time.

      Calling this method will not change the internal state of the metric.

      :return: The elapsed time, in seconds, as a float value.


   .. method:: reset(self) -> None

      Resets the metric, including the initial time.

      :return: None.



.. py:class:: MinibatchTime

   Bases: :class:`avalanche.evaluation.metrics.timing.TimePluginMetric`

   The minibatch time metric.
   This plugin metric only works at training time.

   This metric "logs" the elapsed time for each iteration.

   If a more coarse-grained logging is needed, consider using
   :class:`EpochTime`.

   Creates an instance of the minibatch time metric.

   .. method:: before_training_iteration(self, strategy) -> MetricResult


   .. method:: __str__(self)



.. py:class:: EpochTime

   Bases: :class:`avalanche.evaluation.metrics.timing.TimePluginMetric`

   The epoch elapsed time metric.
   This plugin metric only works at training time.

   The elapsed time will be logged after each epoch.

   Creates an instance of the epoch time metric.

   .. method:: before_training_epoch(self, strategy)


   .. method:: __str__(self)



.. py:class:: RunningEpochTime

   Bases: :class:`avalanche.evaluation.metrics.timing.TimePluginMetric`

   The running epoch time metric.
   This plugin metric only works at training time.

   For each iteration, this metric logs the average time
   between the start of the
   epoch and the current iteration.

   Creates an instance of the running epoch time metric..

   .. method:: before_training_epoch(self, strategy)


   .. method:: after_training_iteration(self, strategy: BaseStrategy) -> MetricResult


   .. method:: result(self, strategy) -> float


   .. method:: __str__(self)



.. py:class:: ExperienceTime

   Bases: :class:`avalanche.evaluation.metrics.timing.TimePluginMetric`

   The experience time metric.
   This plugin metric only works at eval time.

   After each experience, this metric emits the average time of that
   experience.

   Creates an instance of the experience time metric.

   .. method:: before_eval_exp(self, strategy: BaseStrategy)


   .. method:: __str__(self)



.. py:class:: StreamTime

   Bases: :class:`avalanche.evaluation.metrics.timing.TimePluginMetric`

   The stream time metric.
   This metric only works at eval time.

   After the entire evaluation stream,
   this plugin metric emits the average time of that stream.

   Creates an instance of the stream time metric.

   .. method:: before_eval(self, strategy: BaseStrategy)


   .. method:: __str__(self)



.. function:: timing_metrics(*, minibatch=False, epoch=False, epoch_running=False, experience=False, stream=False) -> List[PluginMetric]

   Helper method that can be used to obtain the desired set of
   plugin metrics.

   :param minibatch: If True, will return a metric able to log the train
       minibatch elapsed time.
   :param epoch: If True, will return a metric able to log the train epoch
       elapsed time.
   :param epoch_running: If True, will return a metric able to log the running
       train epoch elapsed time.
   :param experience: If True, will return a metric able to log the eval
       experience elapsed time.
   :param stream: If True, will return a metric able to log the eval stream
       elapsed time.

   :return: A list of plugin metrics.


