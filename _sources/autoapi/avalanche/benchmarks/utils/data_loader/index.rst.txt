:mod:`avalanche.benchmarks.utils.data_loader`
=============================================

.. py:module:: avalanche.benchmarks.utils.data_loader

.. autoapi-nested-parse::

   Avalanche supports data loading using pytorch's dataloaders.
   This module provides custom dataloaders for continual learning such as
   support for balanced dataloading between different tasks or balancing
   between the current data and the replay memory.



Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   avalanche.benchmarks.utils.data_loader.TaskBalancedDataLoader
   avalanche.benchmarks.utils.data_loader.GroupBalancedDataLoader
   avalanche.benchmarks.utils.data_loader.GroupBalancedInfiniteDataLoader
   avalanche.benchmarks.utils.data_loader.ReplayDataLoader



.. py:class:: TaskBalancedDataLoader(data: AvalancheDataset, oversample_small_tasks: bool = False, collate_mbatches=_default_collate_mbatches_fn, **kwargs)

   Task-balanced data loader for Avalanche's datasets.

   The iterator returns a mini-batch balanced across each task, which
   makes it useful when training in multi-task scenarios whenever data is
   highly unbalanced.

   If `oversample_small_tasks == True` smaller tasks are
   oversampled to match the largest task. Otherwise, once the data for a
   specific task is terminated, that task will not be present in the
   subsequent mini-batches.

   :param data: an instance of `AvalancheDataset`.
   :param oversample_small_tasks: whether smaller tasks should be
       oversampled to match the largest one.
   :param collate_mbatches: function that given a sequence of mini-batches
       (one for each task) combines them into a single mini-batch. Used to
       combine the mini-batches obtained separately from each task.
   :param kwargs: data loader arguments used to instantiate the loader for
       each task separately. See pytorch :class:`DataLoader`.

   .. method:: __iter__(self)


   .. method:: __len__(self)



.. py:class:: GroupBalancedDataLoader(datasets: Sequence[AvalancheDataset], oversample_small_groups: bool = False, collate_mbatches=_default_collate_mbatches_fn, **kwargs)

   Data loader that balances data from multiple datasets.

   Mini-batches emitted by this dataloader are created by collating
   together mini-batches from each group. It may be used to balance data
   among classes, experiences, tasks, and so on.

   If `oversample_small_groups == True` smaller groups are oversampled to
   match the largest group. Otherwise, once data from a group is
   completely iterated, the group will be skipped.

   :param datasets: an instance of `AvalancheDataset`.
   :param oversample_small_groups: whether smaller groups should be
       oversampled to match the largest one.
   :param collate_mbatches: function that given a sequence of mini-batches
       (one for each task) combines them into a single mini-batch. Used to
       combine the mini-batches obtained separately from each task.
   :param kwargs: data loader arguments used to instantiate the loader for
       each group separately. See pytorch :class:`DataLoader`.

   .. method:: __iter__(self)


   .. method:: __len__(self)



.. py:class:: GroupBalancedInfiniteDataLoader(datasets: Sequence[AvalancheDataset], collate_mbatches=_default_collate_mbatches_fn, **kwargs)

   Data loader that balances data from multiple datasets emitting an
   infinite stream.

   Mini-batches emitted by this dataloader are created by collating
   together mini-batches from each group. It may be used to balance data
   among classes, experiences, tasks, and so on.

   :param datasets: an instance of `AvalancheDataset`.
   :param collate_mbatches: function that given a sequence of mini-batches
       (one for each task) combines them into a single mini-batch. Used to
       combine the mini-batches obtained separately from each task.
   :param kwargs: data loader arguments used to instantiate the loader for
       each group separately. See pytorch :class:`DataLoader`.

   .. method:: __iter__(self)


   .. method:: __len__(self)



.. py:class:: ReplayDataLoader(data: AvalancheDataset, memory: AvalancheDataset = None, oversample_small_tasks: bool = False, collate_mbatches=_default_collate_mbatches_fn, batch_size: int = 32, force_data_batch_size: int = None, **kwargs)

   Custom data loader for rehearsal strategies.

   The iterates in parallel two datasets, the current `data` and the
   rehearsal `memory`, which are used to create mini-batches by
   concatenating their data together. Mini-batches from both of them are
   balanced using the task label (i.e. each mini-batch contains a balanced
   number of examples from all the tasks in the `data` and `memory`).

   If `oversample_small_tasks == True` smaller tasks are oversampled to
   match the largest task.

   :param data: AvalancheDataset.
   :param memory: AvalancheDataset.
   :param oversample_small_tasks: whether smaller tasks should be
       oversampled to match the largest one.
   :param collate_mbatches: function that given a sequence of mini-batches
       (one for each task) combines them into a single mini-batch. Used to
       combine the mini-batches obtained separately from each task.
   :param batch_size: the size of the batch. It must be greater than or
       equal to the number of tasks.
   :param ratio_data_mem: How many of the samples should be from
   :param kwargs: data loader arguments used to instantiate the loader for
       each task separately. See pytorch :class:`DataLoader`.

   .. method:: __iter__(self)


   .. method:: __len__(self)



